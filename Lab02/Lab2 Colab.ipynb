{"cells":[{"cell_type":"markdown","id":"19c247ef","metadata":{"id":"19c247ef"},"source":["# Lab 2: Fine-tuning BERT To Perform Common Sense Reasoning\n","\n","## May 13, 2024\n","\n","Welcome to the Lab 2 of our course on Natural Language Processing. As the name suggests in this lab you will learn how to fine-tune a pretrained model like BERT on a downstream task to improve much more superior performance compared to the methods discussed so far. We will be working with the [SocialIQA](https://arxiv.org/abs/1904.09728) dataset this week, which is a multiple choice classification dataset designed to learn and measure social and emotional intelligence in NLP models.\n","\n","\n","This assignment will also make heavy use of the [ðŸ¤— Transformers Library](https://huggingface.co/docs/transformers/index). Don't worry if you are not familiar with the library, we will discuss its usage in detail.\n","\n","Note: Access to a GPU will be crucial for working on this assignment. So do select a GPU runtime in Colab before you start working.\n","\n","Learning Outcomes from this Lab:\n","- Learn how to use ðŸ¤— Transformer library to load and fine-tune pre-trained langauge models\n","- Learn how to solve common sense reasoning problems using Masked Language Models like BERT\n","\n","Suggested Reading:\n","- [Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*](https://arxiv.org/pdf/1810.04805.pdf)\n","- [Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social IQa: Commonsense Reasoning about Social Interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4463â€“4473, Hong Kong, China. Association for Computational Linguistics.] (https://arxiv.org/pdf/1810.04805.pdf)"]},{"cell_type":"code","execution_count":1,"id":"26c077e7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26807,"status":"ok","timestamp":1715707608748,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"26c077e7","outputId":"6d72cca6-ef8f-4e57-d998-bc1815452231"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","siqa_data_dir = \"/content/gdrive/MyDrive/PlakshaTLF24-NLP/Lab02/data/socialiqa-train-dev\"\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":2,"id":"3242992d","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1715707608748,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"3242992d"},"outputs":[],"source":["# If using Colab, NO NEED TO INSTALL ANYTHING\n","# Install required libraries\n","# %pip install numpy\n","# %pip install pandas\n","# %pip install torch\n","# %pip install tqdm\n","# %pip install matplotlib\n","# %pip install transformers\n","# %pip install scikit-learn\n","# %pip install tqdm"]},{"cell_type":"code","execution_count":3,"id":"9e7b0e64","metadata":{"executionInfo":{"elapsed":7570,"status":"ok","timestamp":1715707616314,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"9e7b0e64"},"outputs":[],"source":["# We start by importing libraries that we will be making use of in the assignment.\n","import os\n","from functools import partial\n","import json\n","from pprint import pprint\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","import copy\n","from tqdm.notebook import tqdm\n","\n","from transformers.utils import logging\n","logging.set_verbosity(40) # to avoid warnings from transformers"]},{"cell_type":"markdown","id":"3c04bc11","metadata":{"id":"3c04bc11"},"source":["## SocialIQA Dataset\n","\n","We start by discussing the dataset that we will making use of in today's Lab. As described above SocialIQA was designed to learn and measure social and emotional intelligence in NLP models. It is a multiple choice classification task, where you are given a context of some social situation, a question about the context and then three possible answers to the questions. The task is to predict which of the three options answers the question given the context.\n","\n","![siqa dataset](https://i.ibb.co/s5tMpY8/siqa.png)\n","\n","Below we load the dataset in memory"]},{"cell_type":"code","execution_count":4,"id":"b3f82e9d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4117,"status":"ok","timestamp":1715707620426,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"b3f82e9d","outputId":"278dcb09-709b-4f88-9e2e-665a999860a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of Training Examples: 33410\n","Number of Validation Examples: 1954\n"]}],"source":["def load_siqa_data(split):\n","\n","    # We first load the file containing context, question and answers\n","    with open(f\"{siqa_data_dir}/{split}.jsonl\") as f:\n","        data = [json.loads(jline) for jline in f.read().splitlines()]\n","\n","    # We then load the file containing the correct answer for each question\n","    with open(f\"{siqa_data_dir}/{split}-labels.lst\") as f:\n","        labels = f.read().splitlines()\n","\n","    labels_dict = {\"1\": \"A\", \"2\": \"B\", \"3\": \"C\"}\n","    labels = [labels_dict[label] for label in labels]\n","\n","    return data, labels\n","\n","\n","train_data, train_labels = load_siqa_data(\"train\")\n","dev_data, dev_labels = load_siqa_data(\"dev\")\n","\n","print(f\"Number of Training Examples: {len(train_data)}\")\n","print(f\"Number of Validation Examples: {len(dev_data)}\")"]},{"cell_type":"code","execution_count":5,"id":"3dc04307","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"executionInfo":{"elapsed":552,"status":"ok","timestamp":1715707620975,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"3dc04307","outputId":"ef706af3-6c0b-4d50-c123-db990e1812bf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<Axes: ylabel='count'>"]},"metadata":{},"execution_count":5},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAk0AAAGdCAYAAAAPLEfqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAkqUlEQVR4nO3df1iV9f3H8ddBxoHUc8gfnMP5xhxllz/S5cJmZ5WXGhOVunJjbRZNS6argcvo0qJLKctiYf6WST809Qo3ay1X2pgMFy4lUYo0M+Y2m17TA20KJ0kB5Xz/aNyXJ537hOg5yPNxXee65L4/5+Z9e53NZ/e5OdgCgUBAAAAAOKeIUA8AAADQERBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYiAz1AJeKlpYWHTp0SN27d5fNZgv1OAAAwEAgENBnn30mj8ejiIhzX0simtrJoUOHlJCQEOoxAABAGxw8eFBXXHHFOdcQTe2ke/fukr74S3c4HCGeBgAAmPD7/UpISLD+HT8XoqmdtL4l53A4iCYAADoYk1truBEcAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAYiQz0AgiXNWBPqERBGKudNDPUIAID/4EoTAACAAaIJAADAAG/PATgn3jLG6XjLGJ0ZV5oAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAjxwAAHQofAwGvuxifRQGV5oAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMhDSatmzZottuu00ej0c2m03r168P2h8IBJSbm6v4+HjFxMQoOTlZ+/btC1pz5MgRpaeny+FwKDY2VhkZGTp27FjQml27dunmm29WdHS0EhISlJ+ff8Ysr776qvr376/o6GgNHjxYb731VrufLwAA6LhCGk0NDQ269tprVVBQcNb9+fn5WrJkiQoLC7V9+3Z17dpVKSkpOnHihLUmPT1de/bsUUlJiTZs2KAtW7Zo6tSp1n6/36/Ro0erT58+qqys1Lx58/T444/r+eeft9Zs27ZNd955pzIyMvT+++9r/PjxGj9+vD788MMLd/IAAKBDiQzlNx87dqzGjh171n2BQECLFi3SrFmzdPvtt0uS1qxZI5fLpfXr12vChAnau3eviouLtWPHDg0dOlSStHTpUo0bN07PPvusPB6PioqK1NTUpJUrVyoqKkrXXHONqqqqtGDBAiuuFi9erDFjxmjGjBmSpCeffFIlJSVatmyZCgsLL8LfBAAACHdhe0/T/v375fP5lJycbG1zOp0aNmyYysvLJUnl5eWKjY21gkmSkpOTFRERoe3bt1trhg8frqioKGtNSkqKqqurdfToUWvN6d+ndU3r9zmbxsZG+f3+oAcAALh0hW00+Xw+SZLL5Qra7nK5rH0+n09xcXFB+yMjI9WjR4+gNWc7xunf47+tad1/Nnl5eXI6ndYjISHhq54iAADoQMI2msJdTk6O6uvrrcfBgwdDPRIAALiAwjaa3G63JKmmpiZoe01NjbXP7XartrY2aP/Jkyd15MiRoDVnO8bp3+O/rWndfzZ2u10OhyPoAQAALl1hG02JiYlyu90qLS21tvn9fm3fvl1er1eS5PV6VVdXp8rKSmvN5s2b1dLSomHDhllrtmzZoubmZmtNSUmJ+vXrp8svv9xac/r3aV3T+n0AAABCGk3Hjh1TVVWVqqqqJH1x83dVVZUOHDggm82m6dOna+7cuXrjjTe0e/duTZw4UR6PR+PHj5ckDRgwQGPGjNGUKVNUUVGhrVu3KisrSxMmTJDH45Ek3XXXXYqKilJGRob27NmjdevWafHixcrOzrbmeOCBB1RcXKz58+fr448/1uOPP66dO3cqKyvrYv+VAACAMBXSjxzYuXOnRo4caX3dGjKTJk3SqlWrNHPmTDU0NGjq1Kmqq6vTTTfdpOLiYkVHR1vPKSoqUlZWlm655RZFREQoLS1NS5YssfY7nU5t2rRJmZmZSkpKUq9evZSbmxv0WU7f+c53tHbtWs2aNUuPPvqorr76aq1fv16DBg26CH8LAACgI7AFAoFAqIe4FPj9fjmdTtXX15/X/U1JM9a041To6CrnTQz1CLwmEYTXJMLR+bwuv8q/32F7TxMAAEA4IZoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGwjqaTp06pdmzZysxMVExMTG66qqr9OSTTyoQCFhrAoGAcnNzFR8fr5iYGCUnJ2vfvn1Bxzly5IjS09PlcDgUGxurjIwMHTt2LGjNrl27dPPNNys6OloJCQnKz8+/KOcIAAA6hrCOpmeeeUbLly/XsmXLtHfvXj3zzDPKz8/X0qVLrTX5+flasmSJCgsLtX37dnXt2lUpKSk6ceKEtSY9PV179uxRSUmJNmzYoC1btmjq1KnWfr/fr9GjR6tPnz6qrKzUvHnz9Pjjj+v555+/qOcLAADCV2SoBziXbdu26fbbb1dqaqok6Rvf+IZ+9atfqaKiQtIXV5kWLVqkWbNm6fbbb5ckrVmzRi6XS+vXr9eECRO0d+9eFRcXa8eOHRo6dKgkaenSpRo3bpyeffZZeTweFRUVqampSStXrlRUVJSuueYaVVVVacGCBUFxBQAAOq+wvtL0ne98R6WlpfrLX/4iSfrggw/0zjvvaOzYsZKk/fv3y+fzKTk52XqO0+nUsGHDVF5eLkkqLy9XbGysFUySlJycrIiICG3fvt1aM3z4cEVFRVlrUlJSVF1draNHj551tsbGRvn9/qAHAAC4dIX1laZHHnlEfr9f/fv3V5cuXXTq1Ck99dRTSk9PlyT5fD5JksvlCnqey+Wy9vl8PsXFxQXtj4yMVI8ePYLWJCYmnnGM1n2XX375GbPl5eVpzpw57XCWAACgIwjrK02vvPKKioqKtHbtWr333ntavXq1nn32Wa1evTrUoyknJ0f19fXW4+DBg6EeCQAAXEBhfaVpxowZeuSRRzRhwgRJ0uDBg/WPf/xDeXl5mjRpktxutySppqZG8fHx1vNqamo0ZMgQSZLb7VZtbW3QcU+ePKkjR45Yz3e73aqpqQla0/p165ovs9vtstvt53+SAACgQwjrK02ff/65IiKCR+zSpYtaWlokSYmJiXK73SotLbX2+/1+bd++XV6vV5Lk9XpVV1enyspKa83mzZvV0tKiYcOGWWu2bNmi5uZma01JSYn69et31rfmAABA5xPW0XTbbbfpqaee0saNG/XJJ5/o9ddf14IFC/S9731PkmSz2TR9+nTNnTtXb7zxhnbv3q2JEyfK4/Fo/PjxkqQBAwZozJgxmjJliioqKrR161ZlZWVpwoQJ8ng8kqS77rpLUVFRysjI0J49e7Ru3TotXrxY2dnZoTp1AAAQZsL67bmlS5dq9uzZ+tnPfqba2lp5PB799Kc/VW5urrVm5syZamho0NSpU1VXV6ebbrpJxcXFio6OttYUFRUpKytLt9xyiyIiIpSWlqYlS5ZY+51OpzZt2qTMzEwlJSWpV69eys3N5eMGAACAxRY4/eO10WZ+v19Op1P19fVyOBxtPk7SjDXtOBU6usp5E0M9Aq9JBOE1iXB0Pq/Lr/Lvd1i/PQcAABAuiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABtoUTaNGjVJdXd0Z2/1+v0aNGnW+MwEAAISdNkXT22+/raampjO2nzhxQn/+85/PeygAAIBwE/lVFu/atcv680cffSSfz2d9ferUKRUXF+v//u//2m86AACAMPGVomnIkCGy2Wyy2WxnfRsuJiZGS5cubbfhAAAAwsVXiqb9+/crEAjoyiuvVEVFhXr37m3ti4qKUlxcnLp06dLuQwIAAITaV4qmPn36SJJaWlouyDAAAADh6itF0+n27dunP/3pT6qtrT0jonJzc897MAAAgHDSpmh64YUXdP/996tXr15yu92y2WzWPpvNRjQBAIBLTpuiae7cuXrqqaf08MMPt/c8AAAAYalNn9N09OhR3XHHHe09CwAAQNhqUzTdcccd2rRpU3vPAgAAELbaFE19+/bV7Nmzdc8992j+/PlasmRJ0KM9/fOf/9Tdd9+tnj17KiYmRoMHD9bOnTut/YFAQLm5uYqPj1dMTIySk5O1b9++oGMcOXJE6enpcjgcio2NVUZGho4dOxa0ZteuXbr55psVHR2thIQE5efnt+t5AACAjq1N9zQ9//zz6tatm8rKylRWVha0z2az6ec//3m7DHf06FHdeOONGjlypH7/+9+rd+/e2rdvny6//HJrTX5+vpYsWaLVq1crMTFRs2fPVkpKij766CNFR0dLktLT03X48GGVlJSoublZ9957r6ZOnaq1a9dK+uJ35o0ePVrJyckqLCzU7t27NXnyZMXGxmrq1Kntci4AAKBja1M07d+/v73nOKtnnnlGCQkJeumll6xtiYmJ1p8DgYAWLVqkWbNm6fbbb5ckrVmzRi6XS+vXr9eECRO0d+9eFRcXa8eOHRo6dKgkaenSpRo3bpyeffZZeTweFRUVqampSStXrlRUVJSuueYaVVVVacGCBUQTAACQ1Ma35y6WN954Q0OHDtUdd9yhuLg4fetb39ILL7xg7d+/f798Pp+Sk5OtbU6nU8OGDVN5ebkkqby8XLGxsVYwSVJycrIiIiK0fft2a83w4cMVFRVlrUlJSVF1dbWOHj161tkaGxvl9/uDHgAA4NLVpitNkydPPuf+lStXtmmYL/v73/+u5cuXKzs7W48++qh27Nihn//854qKitKkSZOsXxjscrmCnudyuax9Pp9PcXFxQfsjIyPVo0ePoDWnX8E6/Zg+ny/o7cBWeXl5mjNnTrucJwAACH9tiqYvX31pbm7Whx9+qLq6urP+It+2amlp0dChQ/X0009Lkr71rW/pww8/VGFhoSZNmtRu36ctcnJylJ2dbX3t9/uVkJAQwokAAMCF1KZoev3118/Y1tLSovvvv19XXXXVeQ/VKj4+XgMHDgzaNmDAAL322muSJLfbLUmqqalRfHy8taampkZDhgyx1tTW1gYd4+TJkzpy5Ij1fLfbrZqamqA1rV+3rvkyu90uu93exjMDAAAdTbvd0xQREaHs7GwtXLiwvQ6pG2+8UdXV1UHb/vKXv1i/ODgxMVFut1ulpaXWfr/fr+3bt8vr9UqSvF6v6urqVFlZaa3ZvHmzWlpaNGzYMGvNli1b1NzcbK0pKSlRv379zvrWHAAA6Hza9Ubwv/3tbzp58mS7He/BBx/Uu+++q6efflp//etftXbtWj3//PPKzMyU9MXHG0yfPl1z587VG2+8od27d2vixInyeDwaP368pC+uTI0ZM0ZTpkxRRUWFtm7dqqysLE2YMEEej0eSdNdddykqKkoZGRnas2eP1q1bp8WLFwe9/QYAADq3Nr099+WYCAQCOnz4sDZu3Niu9xpdf/31ev3115WTk6MnnnhCiYmJWrRokdLT0601M2fOVENDg6ZOnaq6ujrddNNNKi4utj6jSZKKioqUlZWlW265RREREUpLSwv6EE6n06lNmzYpMzNTSUlJ6tWrl3Jzc/m4AQAAYLEFAoHAV33SyJEjg76OiIhQ7969NWrUKE2ePFmRkW1qsQ7N7/fL6XSqvr5eDoejzcdJmrGmHadCR1c5b2KoR+A1iSC8JhGOzud1+VX+/W5T3fzpT39q02AAAAAd1XldEvr000+tG7X79eun3r17t8tQAAAA4aZNN4I3NDRo8uTJio+P1/DhwzV8+HB5PB5lZGTo888/b+8ZAQAAQq5N0ZSdna2ysjK9+eabqqurU11dnX73u9+prKxMDz30UHvPCAAAEHJtenvutdde029+8xuNGDHC2jZu3DjFxMTohz/8oZYvX95e8wEAAISFNl1p+vzzz8/4fW+SFBcXx9tzAADgktSmaPJ6vXrsscd04sQJa9vx48c1Z84c65O4AQAALiVtentu0aJFGjNmjK644gpde+21kqQPPvhAdrtdmzZtatcBAQAAwkGbomnw4MHat2+fioqK9PHHH0uS7rzzTqWnpysmJqZdBwQAAAgHbYqmvLw8uVwuTZkyJWj7ypUr9emnn+rhhx9ul+EAAADCRZvuaXruuefUv3//M7Zfc801KiwsPO+hAAAAwk2bosnn8yk+Pv6M7b1799bhw4fPeygAAIBw06ZoSkhI0NatW8/YvnXrVnk8nvMeCgAAINy06Z6mKVOmaPr06WpubtaoUaMkSaWlpZo5cyafCA4AAC5JbYqmGTNm6N///rd+9rOfqampSZIUHR2thx9+WDk5Oe06IAAAQDhoUzTZbDY988wzmj17tvbu3auYmBhdffXVstvt7T0fAABAWGhTNLXq1q2brr/++vaaBQAAIGy16UZwAACAzoZoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAw0KGi6Re/+IVsNpumT59ubTtx4oQyMzPVs2dPdevWTWlpaaqpqQl63oEDB5SamqrLLrtMcXFxmjFjhk6ePBm05u2339Z1110nu92uvn37atWqVRfhjAAAQEfRYaJpx44deu655/TNb34zaPuDDz6oN998U6+++qrKysp06NAhff/737f2nzp1SqmpqWpqatK2bdu0evVqrVq1Srm5udaa/fv3KzU1VSNHjlRVVZWmT5+un/zkJ/rDH/5w0c4PAACEtw4RTceOHVN6erpeeOEFXX755db2+vp6rVixQgsWLNCoUaOUlJSkl156Sdu2bdO7774rSdq0aZM++ugjvfzyyxoyZIjGjh2rJ598UgUFBWpqapIkFRYWKjExUfPnz9eAAQOUlZWlH/zgB1q4cGFIzhcAAISfDhFNmZmZSk1NVXJyctD2yspKNTc3B23v37+/vv71r6u8vFySVF5ersGDB8vlcllrUlJS5Pf7tWfPHmvNl4+dkpJiHeNsGhsb5ff7gx4AAODSFRnqAf6XX//613rvvfe0Y8eOM/b5fD5FRUUpNjY2aLvL5ZLP57PWnB5Mrftb951rjd/v1/HjxxUTE3PG987Ly9OcOXPafF4AAKBjCesrTQcPHtQDDzygoqIiRUdHh3qcIDk5Oaqvr7ceBw8eDPVIAADgAgrraKqsrFRtba2uu+46RUZGKjIyUmVlZVqyZIkiIyPlcrnU1NSkurq6oOfV1NTI7XZLktxu9xk/Tdf69f9a43A4znqVSZLsdrscDkfQAwAAXLrCOppuueUW7d69W1VVVdZj6NChSk9Pt/78ta99TaWlpdZzqqurdeDAAXm9XkmS1+vV7t27VVtba60pKSmRw+HQwIEDrTWnH6N1TesxAAAAwvqepu7du2vQoEFB27p27aqePXta2zMyMpSdna0ePXrI4XBo2rRp8nq9uuGGGyRJo0eP1sCBA/XjH/9Y+fn58vl8mjVrljIzM2W32yVJ9913n5YtW6aZM2dq8uTJ2rx5s1555RVt3Ljx4p4wAAAIW2EdTSYWLlyoiIgIpaWlqbGxUSkpKfrlL39p7e/SpYs2bNig+++/X16vV127dtWkSZP0xBNPWGsSExO1ceNGPfjgg1q8eLGuuOIKvfjii0pJSQnFKQEAgDDU4aLp7bffDvo6OjpaBQUFKigo+K/P6dOnj956661zHnfEiBF6//3322NEAABwCQrre5oAAADCBdEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYCOtoysvL0/XXX6/u3bsrLi5O48ePV3V1ddCaEydOKDMzUz179lS3bt2UlpammpqaoDUHDhxQamqqLrvsMsXFxWnGjBk6efJk0Jq3335b1113nex2u/r27atVq1Zd6NMDAAAdSFhHU1lZmTIzM/Xuu++qpKREzc3NGj16tBoaGqw1Dz74oN588029+uqrKisr06FDh/T973/f2n/q1CmlpqaqqalJ27Zt0+rVq7Vq1Srl5uZaa/bv36/U1FSNHDlSVVVVmj59un7yk5/oD3/4w0U9XwAAEL4iQz3AuRQXFwd9vWrVKsXFxamyslLDhw9XfX29VqxYobVr12rUqFGSpJdeekkDBgzQu+++qxtuuEGbNm3SRx99pD/+8Y9yuVwaMmSInnzyST388MN6/PHHFRUVpcLCQiUmJmr+/PmSpAEDBuidd97RwoULlZKSctHPGwAAhJ+wvtL0ZfX19ZKkHj16SJIqKyvV3Nys5ORka03//v319a9/XeXl5ZKk8vJyDR48WC6Xy1qTkpIiv9+vPXv2WGtOP0brmtZjnE1jY6P8fn/QAwAAXLo6TDS1tLRo+vTpuvHGGzVo0CBJks/nU1RUlGJjY4PWulwu+Xw+a83pwdS6v3Xfudb4/X4dP378rPPk5eXJ6XRaj4SEhPM+RwAAEL46TDRlZmbqww8/1K9//etQjyJJysnJUX19vfU4ePBgqEcCAAAXUFjf09QqKytLGzZs0JYtW3TFFVdY291ut5qamlRXVxd0tammpkZut9taU1FREXS81p+uO33Nl3/irqamRg6HQzExMWedyW63y263n/e5AQCAjiGsrzQFAgFlZWXp9ddf1+bNm5WYmBi0PykpSV/72tdUWlpqbauurtaBAwfk9XolSV6vV7t371Ztba21pqSkRA6HQwMHDrTWnH6M1jWtxwAAAAjrK02ZmZlau3atfve736l79+7WPUhOp1MxMTFyOp3KyMhQdna2evToIYfDoWnTpsnr9eqGG26QJI0ePVoDBw7Uj3/8Y+Xn58vn82nWrFnKzMy0rhTdd999WrZsmWbOnKnJkydr8+bNeuWVV7Rx48aQnTsAAAgvYX2lafny5aqvr9eIESMUHx9vPdatW2etWbhwoW699ValpaVp+PDhcrvd+u1vf2vt79KlizZs2KAuXbrI6/Xq7rvv1sSJE/XEE09YaxITE7Vx40aVlJTo2muv1fz58/Xiiy/ycQMAAMAS1leaAoHA/1wTHR2tgoICFRQU/Nc1ffr00VtvvXXO44wYMULvv//+V54RAAB0DmF9pQkAACBcEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRNOXFBQU6Bvf+Iaio6M1bNgwVVRUhHokAAAQBoim06xbt07Z2dl67LHH9N577+naa69VSkqKamtrQz0aAAAIMaLpNAsWLNCUKVN07733auDAgSosLNRll12mlStXhno0AAAQYpGhHiBcNDU1qbKyUjk5Oda2iIgIJScnq7y8/Iz1jY2NamxstL6ur6+XJPn9/vOa41Tj8fN6Pi4t5/t6ag+8JnE6XpMIR+fzumx9biAQ+J9riab/+Ne//qVTp07J5XIFbXe5XPr444/PWJ+Xl6c5c+acsT0hIeGCzYjOx7n0vlCPAAThNYlw1B6vy88++0xOp/Oca4imNsrJyVF2drb1dUtLi44cOaKePXvKZrOFcLKOz+/3KyEhQQcPHpTD4Qj1OACvSYQdXpPtJxAI6LPPPpPH4/mfa4mm/+jVq5e6dOmimpqaoO01NTVyu91nrLfb7bLb7UHbYmNjL+SInY7D4eD/DBBWeE0i3PCabB//6wpTK24E/4+oqCglJSWptLTU2tbS0qLS0lJ5vd4QTgYAAMIBV5pOk52drUmTJmno0KH69re/rUWLFqmhoUH33ntvqEcDAAAhRjSd5kc/+pE+/fRT5ebmyufzaciQISouLj7j5nBcWHa7XY899tgZb38CocJrEuGG12Ro2AImP2MHAADQyXFPEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0IayUl5erS5cuSk1NDfUogO655x7ZbDbr0bNnT40ZM0a7du0K9WjoxHw+n6ZNm6Yrr7xSdrtdCQkJuu2224I+ZxAXBtGEsLJixQpNmzZNW7Zs0aFDh0I9DqAxY8bo8OHDOnz4sEpLSxUZGalbb7011GOhk/rkk0+UlJSkzZs3a968edq9e7eKi4s1cuRIZWZmhnq8Sx4fOYCwcezYMcXHx2vnzp167LHH9M1vflOPPvpoqMdCJ3bPPfeorq5O69evt7a98847uvnmm1VbW6vevXuHbjh0SuPGjdOuXbtUXV2trl27Bu2rq6vj13ldYFxpQth45ZVX1L9/f/Xr10933323Vq5cKZoe4eTYsWN6+eWX1bdvX/Xs2TPU46CTOXLkiIqLi5WZmXlGMEn8/tOLgU8ER9hYsWKF7r77bklfvCVSX1+vsrIyjRgxIrSDoVPbsGGDunXrJklqaGhQfHy8NmzYoIgI/psTF9df//pXBQIB9e/fP9SjdFr8rx5hobq6WhUVFbrzzjslSZGRkfrRj36kFStWhHgydHYjR45UVVWVqqqqVFFRoZSUFI0dO1b/+Mc/Qj0aOhmuvIceV5oQFlasWKGTJ0/K4/FY2wKBgOx2u5YtWyan0xnC6dCZde3aVX379rW+fvHFF+V0OvXCCy9o7ty5IZwMnc3VV18tm82mjz/+ONSjdFpcaULInTx5UmvWrNH8+fOt/6KvqqrSBx98II/Ho1/96lehHhGw2Gw2RURE6Pjx46EeBZ1Mjx49lJKSooKCAjU0NJyxv66u7uIP1ckQTQi5DRs26OjRo8rIyNCgQYOCHmlpabxFh5BqbGyUz+eTz+fT3r17NW3aNB07dky33XZbqEdDJ1RQUKBTp07p29/+tl577TXt27dPe/fu1ZIlS+T1ekM93iWPaELIrVixQsnJyWd9Cy4tLU07d+7kwwQRMsXFxYqPj1d8fLyGDRumHTt26NVXX+UHFBASV155pd577z2NHDlSDz30kAYNGqTvfve7Ki0t1fLly0M93iWPz2kCAAAwwJUmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGDg/wHYcjA4V2I5fgAAAABJRU5ErkJggg==\n"},"metadata":{}}],"source":["sns.countplot(x = train_labels)"]},{"cell_type":"code","execution_count":6,"id":"adf24a61","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1715707620975,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"adf24a61","outputId":"ac668479-f477-4691-c854-a9b88f430164"},"outputs":[{"output_type":"stream","name":"stdout","text":["Example from dataset\n","{   'context': \"Jordan's dog peed on the couch they were selling and Jordan \"\n","               'removed the odor as soon as possible.',\n","    'question': 'How would Jordan feel afterwards?',\n","    'answerA': 'selling a couch',\n","    'answerB': 'Disgusted',\n","    'answerC': 'Relieved'}\n","Label: B\n"]}],"source":["# View a sample of the dataset\n","print(\"Example from dataset\")\n","pprint(train_data[100], sort_dicts=False, indent=4)\n","print(f\"Label: {train_labels[100]}\")"]},{"cell_type":"code","execution_count":7,"id":"271a12e6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1715707620975,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"271a12e6","outputId":"9bc10fa8-1d72-4b76-e5e8-b0da03914882"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'context': 'kendall was a person who kept her word so she got my money the other day.',\n"," 'question': 'What will Others want to do next?',\n"," 'answerA': 'resent kendall',\n"," 'answerB': 'support kendall',\n"," 'answerC': 'hate kendall'}"]},"metadata":{},"execution_count":7}],"source":["train_data[500]"]},{"cell_type":"markdown","id":"99a6b904","metadata":{"id":"99a6b904"},"source":["## Task 1: Tokenization and Data Preperation (1 hour)"]},{"cell_type":"markdown","id":"0af7f506","metadata":{"id":"0af7f506"},"source":["As discussed in the lectures, BERT and other pretrained language models use sub-word tokenization i.e. individual words can also be split into constituent subwords to reduce the vocabulary size. The Transformer library provides tokenizer for all the popular language models. Below we demonstrate how to create and use these tokenizers."]},{"cell_type":"code","execution_count":8,"id":"4e8aa458","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["72eacd71006f4438a80de874bb6ded91","4851c0c97e154450b5a2d2b043c58fdd","f7ad800541c843afa178370cff41eb4f","a0dfb2fb51fb472c9949fcdaec756a72","aeed9a1d264f4f38a61e97e46e49d829","ba38ae15268649b680e963be8a8dab36","41f3b328891d43baaac58f2341ced50d","fd4ad52979d7438e9a60958933da4016","f75a95481e0d4bafaaca14fa6a88e597","7c2a40d646284c7ab37491ee06e96cf5","84e4fa1d5f8646a69a4be14197635a02","d993677081ee48139b5705697857f314","a5fcb79d6b6449f6af078209fc7107cc","726ae663b7224c1da3e15ab5bd1d0a0d","562ebcbeed9d4ca19a21df859e20032e","1ce17425296344e086d5850698eb3d75","d1e562b0a9b24d309b5fadb1af638990","a251376007234a3191a5af9e1c68fe83","adc7662a540642bbb705c40e2dffe146","2df217cce6ee408e9179ed5860e08b6c","cc26c81db34046bd8c087738d525bedf","1fa2d6bb53904b0d8c53ea3d5b2b0868","c3af3f3dc31940078c6a1068febd9d8b","a3955b91f7474cafb5c06953f8029d1a","0c36b13e1b08401c9ffc9782ab191a6b","8c85c4336fda42e5a4e11a8f28b51d9c","0c991046263a4336a3c39e9c2ba4d252","b31359ba7af14983bb741f2156f4703e","4f313e59d21d4c598a425d5700f5f3d4","79ef5e0412bd48e1988ef98519c7e716","3eb4b26137584fe3b1d92d5ab204034b","918ea117c1ee446c97dce16bd594a726","cfae38e6de304312b5b6f03c307fc10e","5d671e51e7e94d7b873e958b58638206","040fb8fc9b4a45b38a8d5b00d04170bb","5c0fdf39bc294a85a5750dc114921c77","9f66956c169c47ccad8098dc86b69754","1f67f6671c3747f7b655ff79d3611aa4","2a5ed7d418c0459b83c6d6f5b211a305","f1d20158d5924902b23599e9955d825b","4bdca2372dfe465ea96806bbaadda6bf","2d7b1a83cfc7494bb9f703a2d0aeaa86","3c000fc9b17d40cbad3881d9ff42760f","0b6ad73c6b1f48cfa2992f2747c8042e"]},"executionInfo":{"elapsed":3956,"status":"ok","timestamp":1715707624926,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"4e8aa458","outputId":"54dadf8f-d177-45a4-a275-dffdac52bd3c"},"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72eacd71006f4438a80de874bb6ded91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d993677081ee48139b5705697857f314"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3af3f3dc31940078c6a1068febd9d8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d671e51e7e94d7b873e958b58638206"}},"metadata":{}}],"source":["# Import the BertTokenizer from the library\n","from transformers import BertTokenizer\n","\n","# Load a pre-trained BERT Tokenizer\n","bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"]},{"cell_type":"markdown","id":"30724776","metadata":{"id":"30724776"},"source":["`BertTokenizer.from_pretrained` is used to load a pre-trained tokenizer. Notice that we provide the argument `\"bert-base-uncased\"` to the method. This refers to the variant of BERT that we want to use. The term \"base\" means we want to use the smaller BERT variant i.e. the one with 12 layers, and \"uncased\" refers to the fact that it treats upper-case and lower-case characters identically. There are 4 variants available for BERT which are:\n","    - `bert-base-uncased`\n","    - `bert-base-cased`\n","    - `bert-large-uncased`\n","    - `bert-large-cased`\n","Now that we have loaded the tokenizer, let's see how to use it."]},{"cell_type":"markdown","id":"13523cc5","metadata":{"id":"13523cc5"},"source":["`tokenize` method can be used to split the text into sequence of tokens"]},{"cell_type":"code","execution_count":9,"id":"5ccbad09","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1715707624926,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"5ccbad09","outputId":"aeec5584-7bf3-4273-fb20-2b25d4798a94"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['kendall',\n"," 'was',\n"," 'a',\n"," 'person',\n"," 'who',\n"," 'kept',\n"," 'her',\n"," 'word',\n"," 'exquisite',\n"," '##ly',\n"," ',',\n"," 'so',\n"," 'she',\n"," 'got',\n"," 'my',\n"," 'money',\n"," 'the',\n"," 'other',\n"," 'day']"]},"metadata":{},"execution_count":9}],"source":["bert_tokenizer.tokenize(\"kendall was a person who kept her word exquisitely, so she got my money the other day\")"]},{"cell_type":"markdown","id":"daced828","metadata":{"id":"daced828"},"source":["Notice how the tokenizer not only splits the text into words but also subwords like \"exquisitely\" is split into \"exquisite\" and \"ly\"."]},{"cell_type":"markdown","id":"d7260263","metadata":{"id":"d7260263"},"source":["Another use case of the tokenizer is to convert the tokens into indices. This is important because BERT and almost all language models takes as the inputs a sequence of token ids, which they use to map into embeddings. `convert_tokens_to_ids` method can be used to do this"]},{"cell_type":"code","execution_count":10,"id":"dbee421b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1715707624926,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"dbee421b","outputId":"dbe81056-feeb-46ba-eff0-df960602304b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[14509, 2001, 1037, 2711, 2040, 2921, 2014, 2773, 19401, 2135, 1010, 2061, 2016, 2288, 2026, 2769, 1996, 2060, 2154]\n"]}],"source":["sentence = \"kendall was a person who kept her word exquisitely, so she got my money the other day\"\n","tokens = bert_tokenizer.tokenize(sentence)\n","token_ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n","print(token_ids)"]},{"cell_type":"markdown","id":"efd54a05","metadata":{"id":"efd54a05"},"source":["The two steps can also be combined by simply calling the tokenizer object"]},{"cell_type":"code","execution_count":11,"id":"bfcbb696","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1715707624927,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"bfcbb696","outputId":"fc01eaef-3cd5-482b-caa0-3f6a2457a924"},"outputs":[{"output_type":"stream","name":"stdout","text":["{   'input_ids': [   101,\n","                     14509,\n","                     2001,\n","                     1037,\n","                     2711,\n","                     2040,\n","                     2921,\n","                     2014,\n","                     2773,\n","                     19401,\n","                     2135,\n","                     1010,\n","                     2061,\n","                     2016,\n","                     2288,\n","                     2026,\n","                     2769,\n","                     1996,\n","                     2060,\n","                     2154,\n","                     102],\n","    'token_type_ids': [   0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0],\n","    'attention_mask': [   1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1]}\n"]}],"source":["pprint(bert_tokenizer(sentence), sort_dicts=False, indent=4)"]},{"cell_type":"markdown","id":"2ab48ddf","metadata":{"id":"2ab48ddf"},"source":["Notice that it returns a bunch of things in addition to the ids. The `\"input_ids\"` are just the token ids that we obtained in the previous cell. However you will notice that it has a few additional ids, it starts with 101 and ends with 102. These are what we call special tokens and correspond the \\[CLS\\] and \\[SEP\\] tokens used by BERT. \\[CLS\\] token is mainly added to beginning of each sequence, and its representations are used to perform sequence classification. More on \\[SEP\\] token later.\n","\n","`\"token_type_ids\"` contains which sequence does a particular token belongs to.\n","\n","`\"attention_mask`\" is a mask vector that indicates if a particular token corresponds to padding. Padding is extremely important when we are dealing with variable length sequences, which is almost always the case. Through padding we can ensure that all the sequences in a batch are of same size. However, while processing the sequence we need ignore these padding tokens, hence a mask is required to identify such tokens.\n","\n","We can tokenize a batch of sequences by just providing a list instead of a string while calling the tokenizer and later pad them using the `.pad` method."]},{"cell_type":"code","execution_count":12,"id":"1d2c0925","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1715707624927,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"1d2c0925","outputId":"c2de8db9-c15a-4a7a-939e-7010f78c7c6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input Ids shape: torch.Size([4, 23])\n","Attention Mask shape: torch.Size([4, 23])\n","('Input Ids:\\n'\n"," ' tensor([[  101,  7232,  2787,  2000,  2031,  1037, 26375,  1998,  5935,  '\n"," '2014,\\n'\n"," '          2814,  2362,  1012,   102,     0,     0,     0,     0,     0,     '\n"," '0,\\n'\n"," '             0,     0,     0],\\n'\n"," '        [  101,  5553,  2734,  2000,  2507,  2041,  5841,  2005,  2019,  '\n"," '9046,\\n'\n"," '          2622,  2012,  2147,  1012,   102,     0,     0,     0,     0,     '\n"," '0,\\n'\n"," '             0,     0,     0],\\n'\n"," '        [  101, 22712,  2001,  2019,  6739, 19949,  1998,  2001,  2006,  '\n"," '1996,\\n'\n"," '          2300,  2007, 11928,  1012, 22712, 17395,  2098, 11928,  1005,  '\n"," '1055,\\n'\n"," '          8103,  1012,   102],\\n'\n"," '        [  101, 18403,  2435,  1037,  8549,  2000, 27970,  1005,  1055,  '\n"," '2365,\\n'\n"," '          2043,  2027,  2020,  3110,  2091,  1012,   102,     0,     0,     '\n"," '0,\\n'\n"," '             0,     0,     0]])\\n')\n","('Attention Mask:\\n'\n"," ' tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, '\n"," '0],\\n'\n"," '        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, '\n"," '0],\\n'\n"," '        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, '\n"," '1],\\n'\n"," '        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, '\n"," '0]])\\n')\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  warnings.warn(\n"]}],"source":["batch_size = 4\n","sentence_batch = [train_data[i][\"context\"] for i in range(batch_size)]\n","\n","#Tokenize the batch of sequences\n","tokenized_batch = bert_tokenizer(sentence_batch)\n","\n","# Pad the tokenized batch\n","tokenized_batch_padded = bert_tokenizer.pad(tokenized_batch, padding=True, max_length=32, return_tensors=\"pt\")\n","\n","input_ids = tokenized_batch_padded[\"input_ids\"]\n","attn_mask = tokenized_batch_padded[\"attention_mask\"]\n","print(f\"Input Ids shape: {input_ids.shape}\")\n","print(f\"Attention Mask shape: {attn_mask.shape}\")\n","\n","pprint(f\"Input Ids:\\n {input_ids}\\n\")\n","pprint(f\"Attention Mask:\\n {attn_mask}\\n\")"]},{"cell_type":"markdown","id":"fc12c5d0","metadata":{"id":"fc12c5d0"},"source":["Notice how 0s get appended to the input ids sequence, and the same is also reflected in the output of `attn_mask` where `0` indicates that the particular token was padded and `1` means otherwise. Setting `return_tensors=\"pt\"` results in the outputs as torch tensors"]},{"cell_type":"markdown","id":"c4e65595","metadata":{"id":"c4e65595"},"source":["Finally, for tasks involving reasoning over multiple sentences (like what we have for the SocialIQA dataset), it is common to seperate out each sentence using a \\[SEP\\] token:\n","\n","<img src=\"https://i.ibb.co/Nx8mK1P/bert-sentence-pair.jpg\" alt=\"bert-sentence-pair\" border=\"0\">\n","\n","We can achieve this by adding concatenating all sentences with the `[SEP]` token before calling the tokenizer"]},{"cell_type":"code","execution_count":13,"id":"19f0e7bd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1715707624927,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"19f0e7bd","outputId":"4173c740-9fe9-407f-b022-140e0cc720ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.[SEP]How would Jordan feel afterwards?[SEP]selling a couch\n","{   'input_ids': [   101,\n","                     5207,\n","                     1005,\n","                     1055,\n","                     3899,\n","                     21392,\n","                     2094,\n","                     2006,\n","                     1996,\n","                     6411,\n","                     2027,\n","                     2020,\n","                     4855,\n","                     1998,\n","                     5207,\n","                     3718,\n","                     1996,\n","                     19255,\n","                     2004,\n","                     2574,\n","                     2004,\n","                     2825,\n","                     1012,\n","                     102,\n","                     2129,\n","                     2052,\n","                     5207,\n","                     2514,\n","                     5728,\n","                     1029,\n","                     102,\n","                     4855,\n","                     1037,\n","                     6411,\n","                     102],\n","    'token_type_ids': [   0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0,\n","                          0],\n","    'attention_mask': [   1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1,\n","                          1]}\n"]}],"source":["example = train_data[100]\n","context = example[\"context\"]\n","question = example[\"question\"]\n","answerA = example[\"answerA\"]\n","\n","# Concatenate the context, question and answerA\n","cqa = context + bert_tokenizer.sep_token + question + bert_tokenizer.sep_token + answerA\n","print(cqa)\n","\n","tokenized_cqa = bert_tokenizer(cqa)\n","pprint(tokenized_cqa, sort_dicts=False, indent=4)"]},{"cell_type":"markdown","id":"db9af625","metadata":{"id":"db9af625"},"source":["For the reasons that will become clear once we work on the modeling part, we need three input tensors for each dataset example, one for concatenating each answer with the context and question."]},{"cell_type":"code","execution_count":14,"id":"4a8bddb8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1715707624927,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"4a8bddb8","outputId":"fc4713a7-5dd6-4126-ecb5-826d07393d99"},"outputs":[{"output_type":"stream","name":"stdout","text":["Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.[SEP]How would Jordan feel afterwards?[SEP]selling a couch\n","Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.[SEP]How would Jordan feel afterwards?[SEP]Disgusted\n","Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.[SEP]How would Jordan feel afterwards?[SEP]Relieved\n"]}],"source":["example = train_data[100]\n","context = example[\"context\"]\n","question = example[\"question\"]\n","answerA = example[\"answerA\"]\n","answerB = example[\"answerB\"]\n","answerC = example[\"answerC\"]\n","\n","cqaA = context + bert_tokenizer.sep_token + question + bert_tokenizer.sep_token + answerA\n","cqaB = context + bert_tokenizer.sep_token + question + bert_tokenizer.sep_token + answerB\n","cqaC = context + bert_tokenizer.sep_token + question + bert_tokenizer.sep_token + answerC\n","\n","print(cqaA)\n","print(cqaB)\n","print(cqaC)\n","\n","tokenized_cqaA = bert_tokenizer(cqaA)\n","tokenized_cqaB = bert_tokenizer(cqaB)\n","tokenized_cqaC = bert_tokenizer(cqaC)\n"]},{"cell_type":"markdown","id":"4627768e","metadata":{"id":"4627768e"},"source":["## Task 1.1: Custom Dataset Class\n","\n","Now that we know how to use the hugging face tokenizers we can define the custom `torch.utils.Dataset` class like we did in the previous assignments to process and store the data as well as provides a way to iterate through the dataset. Implement the `SIQABertDataset` class below. Recall to create a custom class you need to implement 3 methods `__init__`, `__len__` and `__getitem__`."]},{"cell_type":"code","execution_count":15,"id":"65ffd2ac","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1715707624927,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"65ffd2ac"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class SIQABertDataset(Dataset):\n","\n","    def __init__(self, data, labels, bert_variant = \"bert-base-uncased\"):\n","        \"\"\"\n","        Constructor for the `SST2BertDataset` class. Stores the `sentences` and `labels` which can then be used by\n","        other methods. Also initializes the tokenizer\n","\n","        Inputs:\n","            - data (list) : A list SIQA dataset examples\n","            - labels (list): A list of labels corresponding to each example\n","            - bert_variant (str): A string indicating the variant of BERT to be used.\n","        \"\"\"\n","        self.label2label_id = {\"A\": 0, \"B\": 1, \"C\": 2}\n","        self.data = data\n","        self.labels = labels\n","        self.tokenizer = BertTokenizer.from_pretrained(bert_variant)\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the length of the dataset\n","        \"\"\"\n","        length = len(self.data)\n","\n","        return length\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Returns the training example corresponding to review present at the `idx` position in the dataset\n","\n","        Inputs:\n","            - idx (int): Index corresponding to the review,label to be returned\n","\n","        Returns:\n","            - tokenized_input_dict (dict(str, dict)): A dictionary corresponding to tokenizer outputs for the three resulting sequences due to each answer choices as described above\n","            - label (int): Answer label for the corresponding sentence. We will use 0, 1 and 2 to represent A, B and C respectively.\n","\n","        Example Output:\n","            - tokenized_input_dict: {\n","                \"A\": {'input_ids': [101, 5207, 1005, 1055, 3899, 21392, 2094, 2006, 1996, 6411, 2027, 2020, 4855, 1998, 5207, 3718, 1996, 19255, 2004, 2574, 2004, 2825, 1012, 102, 2129, 2052, 5207, 2514, 5728, 1029, 102, 4855, 1037, 6411, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n","                \"B\": {'input_ids': [101, 5207, 1005, 1055, 3899, 21392, 2094, 2006, 1996, 6411, 2027, 2020, 4855, 1998, 5207, 3718, 1996, 19255, 2004, 2574, 2004, 2825, 1012, 102, 2129, 2052, 5207, 2514, 5728, 1029, 102, 17733, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n","                \"C\": {'input_ids': [101, 5207, 1005, 1055, 3899, 21392, 2094, 2006, 1996, 6411, 2027, 2020, 4855, 1998, 5207, 3718, 1996, 19255, 2004, 2574, 2004, 2825, 1012, 102, 2129, 2052, 5207, 2514, 5728, 1029, 102, 7653, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","            }\n","            - label: 0\n","\n","        \"\"\"\n","\n","\n","        d = self.data[idx]\n","        context = d['context']\n","        question = d['question']\n","        tokenized_input_dict = {\n","            i: self.tokenizer(str(self.tokenizer.sep_token).join([context, question, d[f'answer{i}']]))\n","            for i in \"ABC\"\n","        }\n","        label = self.label2label_id[self.labels[idx]]\n","\n","        return tokenized_input_dict, label"]},{"cell_type":"code","execution_count":16,"id":"7b8c8dea","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":933,"referenced_widgets":["9f1ab3ed6e8f4fb6922e1c82bd49f66e","18786cbb77c04dfd90cdbb979ec1fd83","f4e3baa463dc4e70b4f8b9075bc15b83","a250546ff8a543cc8cd1d12b42bdcb53","accc7d38b96749dc822ec1e51a45e46d","52f3f0de2404412da34a096791b705df","4131ee416f0b40d6a3d1f7419a949a16","687b27bb69644b1496d48e33d3dbb375","d12b98218d784d88bba67960a604fe5c","e31062399a124a119678614a61956c3d","cd21eab2ddd34b979e22c6a534f9cb97","629b37c990864ccf8b3bfbfe4f525ce7","988f0f4a99604a388d9ebc85e757feb0","f8249fedd97b486db725141eeff17808","cd5bf8b610314034bdc827448c41c403","74d138c833174a83a10655ede859af23","21d3be0da7444c64936083b672646fe5","345abc78fde14fabb0b784994cf3d5b5","381499427d3946d5b6271cca17a40d85","cb284ab8d3f64751a832b8a1adb86842","1aea6d504ddc434a93da75473a0ddee9","c3a3ab4c27684aaa86941c31432e5fd5","9753ce3536534661b2798caff147a825","00d288c13144471190928dcd1457e4a6","6192352d30724f148625af1634960391","b14b4b142d4c4ee1b022cf6a1dae90c2","6fa724dd43c0463681f7e596f31e4762","86574ea37ffc4f4ca17251bc835b5ed7","b431d4eca66e47af84b8e3d0067f565c","25244c63ebfd4e3294f9e0bc795997b8","8f7c34a6e96e4567a76f3e65bbb5c20d","72c965b0a1d2402fb975019ca3d19e05","57a3370ad8a74294aa85aa1a23c3da5d","16e7c4dce73b4c7a8a168da0d1df98f0","60394f2ee55a4b8eaf13713ed862d9a7","a15a970cbafe4d7f9215e45601d75f3f","5282c777dec64a8ba9526601fee4a0ef","8764f3848be64b7d8673f7ee8bec6db6","3086448b5bb241b7954a0ae502bd349c","128f31d4f90e46278062b583066fee62","aaa4c18c23304b898b547997f4af280b","9b8b936fc23141d58d3ec5d5ac785621","55a430d7d16a40e5977f9f2ce1fadf2e","f03e333c04d340c0a478271857c233ce"]},"executionInfo":{"elapsed":2343,"status":"ok","timestamp":1715707627264,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"7b8c8dea","outputId":"4fc8f549-a1df-42e5-f65f-54b598203ff9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running Sample Test Cases\n","Sample Test Case 1: Checking if `__len__` is implemented correctly\n","Dataset Length: 2\n","Expected Length: 2\n","Sample Test Case Passed!\n","****************************************\n","\n","Sample Test Case 2: Checking if `__getitem__` is implemented correctly for `idx= 0`\n","tokenized_input_dict:\n"," {'A': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 7052, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 6595, 2188, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 1037, 2204, 2767, 2000, 2031, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","Expected tokenized_input_dict:\n"," {'A': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 7052, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 6595, 2188, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 1037, 2204, 2767, 2000, 2031, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","label:\n"," 0\n","Expected label:\n"," 0\n","Sample Test Case Passed!\n","****************************************\n","\n","Sample Test Case 3: Checking if `__getitem__` is implemented correctly for `idx= 1`\n","tokenized_input_dict:\n"," {'A': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 21090, 2007, 5553, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 2131, 2000, 2147, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 7475, 2007, 1996, 14799, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","Expected tokenized_input_dict:\n"," {'A': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 21090, 2007, 5553, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 2131, 2000, 2147, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 7475, 2007, 1996, 14799, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","label:\n"," 1\n","Expected label:\n"," 1\n","Sample Test Case Passed!\n","****************************************\n","\n","Sample Test Case 4: Checking if `__getitem__` is implemented correctly for `idx= 0` for a different bert-variant\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f1ab3ed6e8f4fb6922e1c82bd49f66e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"629b37c990864ccf8b3bfbfe4f525ce7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9753ce3536534661b2798caff147a825"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16e7c4dce73b4c7a8a168da0d1df98f0"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["tokenized_input_dict:\n"," {'A': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6546, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6218, 1313, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 170, 1363, 1910, 1106, 1138, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","Expected tokenized_input_dict:\n"," {'A': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6546, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6218, 1313, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 170, 1363, 1910, 1106, 1138, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","label:\n"," 0\n","Expected label:\n"," 0\n","Sample Test Case Passed!\n","****************************************\n","\n"]}],"source":["print(\"Running Sample Test Cases\")\n","\n","sample_dataset = SIQABertDataset(train_data[:2], train_labels[:2], bert_variant=\"bert-base-uncased\")\n","\n","print(f\"Sample Test Case 1: Checking if `__len__` is implemented correctly\")\n","dataset_len= len(sample_dataset)\n","expected_len = 2\n","print(f\"Dataset Length: {dataset_len}\")\n","print(f\"Expected Length: {expected_len}\")\n","assert len(sample_dataset) == expected_len\n","print(\"Sample Test Case Passed!\")\n","print(\"****************************************\\n\")\n","\n","print(f\"Sample Test Case 2: Checking if `__getitem__` is implemented correctly for `idx= 0`\")\n","sample_idx = 0\n","tokenized_input_dict, label = sample_dataset.__getitem__(sample_idx)\n","expected_tokenized_input_dict = {'A': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 7052, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n","  'B': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 6595, 2188, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n","  'C': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 1037, 2204, 2767, 2000, 2031, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","expected_label = 0\n","print(f\"tokenized_input_dict:\\n {tokenized_input_dict}\")\n","print(f\"Expected tokenized_input_dict:\\n {expected_tokenized_input_dict}\")\n","assert (expected_tokenized_input_dict == tokenized_input_dict)\n","\n","print(f\"label:\\n {label}\")\n","print(f\"Expected label:\\n {expected_label}\")\n","assert expected_label == label\n","\n","print(\"Sample Test Case Passed!\")\n","print(\"****************************************\\n\")\n","\n","\n","print(f\"Sample Test Case 3: Checking if `__getitem__` is implemented correctly for `idx= 1`\")\n","sample_idx = 1\n","tokenized_input_dict, label = sample_dataset.__getitem__(sample_idx)\n","expected_tokenized_input_dict =  {'A': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 21090, 2007, 5553, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n","                                  'B': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 2131, 2000, 2147, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n","                                'C': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 7475, 2007, 1996, 14799, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","\n","\n","expected_label = 1\n","print(f\"tokenized_input_dict:\\n {tokenized_input_dict}\")\n","print(f\"Expected tokenized_input_dict:\\n {expected_tokenized_input_dict}\")\n","assert (expected_tokenized_input_dict == tokenized_input_dict)\n","\n","print(f\"label:\\n {label}\")\n","print(f\"Expected label:\\n {expected_label}\")\n","assert expected_label == label\n","\n","print(\"Sample Test Case Passed!\")\n","print(\"****************************************\\n\")\n","\n","print(f\"Sample Test Case 4: Checking if `__getitem__` is implemented correctly for `idx= 0` for a different bert-variant\")\n","sample_dataset = SIQABertDataset(train_data[:2], train_labels[:2], bert_variant=\"bert-base-cased\")\n","sample_idx = 0\n","tokenized_input_dict, label = sample_dataset.__getitem__(sample_idx)\n","expected_tokenized_input_dict = {'A': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6546, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n"," 'B': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6218, 1313, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n"," 'C': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 170, 1363, 1910, 1106, 1138, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n","expected_label = 0\n","print(f\"tokenized_input_dict:\\n {tokenized_input_dict}\")\n","print(f\"Expected tokenized_input_dict:\\n {expected_tokenized_input_dict}\")\n","assert (expected_tokenized_input_dict == tokenized_input_dict)\n","\n","print(f\"label:\\n {label}\")\n","print(f\"Expected label:\\n {expected_label}\")\n","assert expected_label == label\n","\n","print(\"Sample Test Case Passed!\")\n","print(\"****************************************\\n\")\n"]},{"cell_type":"markdown","id":"8ec2e35e","metadata":{"id":"8ec2e35e"},"source":["We can now create Dataset instances for both training and dev datasets"]},{"cell_type":"code","execution_count":17,"id":"d3994ab0","metadata":{"executionInfo":{"elapsed":606,"status":"ok","timestamp":1715707627866,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"d3994ab0"},"outputs":[],"source":["train_dataset = SIQABertDataset(train_data, train_labels, bert_variant=\"bert-base-uncased\")\n","dev_dataset = SIQABertDataset(dev_data, dev_labels, bert_variant=\"bert-base-uncased\")"]},{"cell_type":"markdown","id":"fdbc4e1e","metadata":{"id":"fdbc4e1e"},"source":["Before we instantiate the dataloaders for iterating over the dataset like last time, we need define a collate function, that creates batches from a list of dataset examples. In the last class we didn't have to create one, because all of our examples were of the same size, but that's not the case anymore, and we need to pad the sequences so that they all are of same size. We have implemented the collate_fn for you below, but we recommend going through it step by step, as it is used often in practice."]},{"cell_type":"code","execution_count":18,"id":"6ab6fb0b","metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1715707627867,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"6ab6fb0b"},"outputs":[],"source":["def collate_fn(tokenizer, batch):\n","    \"\"\"\n","    Collate function to be used when creating a data loader for the SIQA dataset.\n","    :param tokenizer: The tokenizer to be used to tokenize the inputs.\n","    :param batch: A list of tuples of the form (tokenized_input_dict, label)\n","    :return: A tuple of the form (tokenized_inputs_dict_batch, labels_batch)\n","    \"\"\"\n","\n","    tokenized_inputsA_batch = []\n","    tokenized_inputsB_batch = []\n","    tokenized_inputsC_batch = []\n","    labels_batch = []\n","    for tokenized_inputs_dict, label in batch:\n","        tokenized_inputsA_batch.append(tokenized_inputs_dict[\"A\"])\n","        tokenized_inputsB_batch.append(tokenized_inputs_dict[\"B\"])\n","        tokenized_inputsC_batch.append(tokenized_inputs_dict[\"C\"])\n","        labels_batch.append(label)\n","\n","    #Pad the inputs\n","    tokenized_inputsA_batch = tokenizer.pad(tokenized_inputsA_batch, padding=True, return_tensors=\"pt\")\n","    tokenized_inputsB_batch = tokenizer.pad(tokenized_inputsB_batch, padding=True, return_tensors=\"pt\")\n","    tokenized_inputsC_batch = tokenizer.pad(tokenized_inputsC_batch, padding=True, return_tensors=\"pt\")\n","\n","    # Convert labels list to a tensor\n","    labels_batch = torch.tensor(labels_batch)\n","    return (\n","        {\"A\": tokenized_inputsA_batch[\"input_ids\"], \"B\": tokenized_inputsB_batch[\"input_ids\"], \"C\": tokenized_inputsC_batch[\"input_ids\"]},\n","        {\"A\": tokenized_inputsA_batch[\"attention_mask\"], \"B\": tokenized_inputsB_batch[\"attention_mask\"], \"C\": tokenized_inputsC_batch[\"attention_mask\"]},\n","        labels_batch\n","    )\n"]},{"cell_type":"markdown","id":"2548c845","metadata":{"id":"2548c845"},"source":["Now that we have defined the collate_fn, lets create the dataloaders. It is common to use smaller batch size while fine-tuning these big models, as they occupy quite a lot of memory."]},{"cell_type":"code","execution_count":19,"id":"6c8784f5","metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1715707627867,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"6c8784f5"},"outputs":[],"source":["batch_size = 16\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=partial(collate_fn, train_dataset.tokenizer))\n","dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True, collate_fn=partial(collate_fn, dev_dataset.tokenizer))"]},{"cell_type":"code","execution_count":20,"id":"b9b42884","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1715707627868,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"b9b42884","outputId":"1f6e8e5b-6d0f-4a99-cefe-1d0ed89180dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["batch_input_ids:\n"," {'A': tensor([[  101, 14509,  2356,  3994,  1005,  1055,  2767,  2000,  2175,  2000,\n","          1996,  3153,  2023,  2746,  5958,  1012,   102,  2339,  2106, 14509,\n","          2079,  2023,  1029,   102,  3866,   102,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0],\n","        [  101,  8804,  2741,  5863,  2000,  2082,  2130,  2295,  2016,  2001,\n","          2025,  3110,  2092,  1012,   102,  2129,  2052,  5863,  2514,  2044,\n","          2023,  1029,   102,  6314,  2008,  8804,  2106,  2025,  2903,  2014,\n","           102,     0,     0,     0,     0,     0],\n","        [  101,  3389,  2354,  3071,  2012,  1996,  2283,  1012,  2027,  5720,\n","          2000,  2169,  1997,  2068,  1012,   102,  2054,  2097,  4148,  2000,\n","          3389,  1029,   102,  2131,  7167,  1997, 14806,  2015,   102,     0,\n","             0,     0,     0,     0,     0,     0],\n","        [  101,  5863,  2003,  1037,  3586, 10365,  1010,  1998,  2371,  2014,\n","          1051, 11149,  2008,  2016, 14172,  2000,  5194,  1012,   102,  2129,\n","          2052,  2017,  6235,  5863,  1029,   102, 23358,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0],\n","        [  101,  9036,  2359,  2000,  1996,  3942,  1996,  2307,  2813,  1997,\n","          2859,  2651,  1012,   102,  2054,  2097,  9036,  2215,  2000,  2079,\n","          2279,  1029,   102,  8849,  2138,  2002,  7459,  2108,  1037,  7538,\n","           102,     0,     0,     0,     0,     0],\n","        [  101,  5553,  2736, 27970,  1005,  1055,  2147,  2138, 27970,  2347,\n","          1005,  1056,  2583,  2000,  1012,   102,  2129,  2097, 27970,  2514,\n","          1029,   102,  8794,   102,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0],\n","        [  101,  4074,  2196,  2052,  2031,  2179,  2041,  2065,  2002,  2910,\n","          1005,  1056,  2179,  1996,  2293,  3661,  2010,  2564,  2741,  2000,\n","          2619,  2842,  1012,   102,  2129,  2052,  4074,  2514,  5728,  1029,\n","           102,  3866,   102,     0,     0,     0],\n","        [  101, 18961,  2253,  2000,  2026,  2767,  1005,  1055,  2160,  2000,\n","          2298,  2005,  2033,  2021,  6343,  2001,  2583,  2000,  2424,  2033,\n","          1012,   102,  2054,  2097,  2500,  2215,  2000,  2079,  2279,  1029,\n","           102,  3189,  2033,  4394,   102,     0],\n","        [  101,  5553,  2001,  2183,  2006,  1037, 13215,  4440,  1012,  5553,\n","          4149,  1037,  5777,  4524,  2013,  1037,  3573,  1012,   102,  2339,\n","          2106,  5553,  2079,  2023,  1029,   102,  4965,  1037,  9311,   102,\n","             0,     0,     0,     0,     0,     0],\n","        [  101, 18961,  1998, 18403,  2020,  1999,  1996,  2168,  2396,  2465,\n","          2061, 18961,  2787,  2000,  3443, 18403,  1005,  1055, 12492,  1012,\n","           102,  2129,  2052, 18961,  2514,  5728,  1029,   102,  4854,  2008,\n","          2016,  2787,  2000,  2079,  2009,   102],\n","        [  101,  5899,  2288,  2010,  4268,  3189,  4003,  1998,  2371,  2061,\n","          7098,  1997,  2037,  7022,  1012,   102,  2054,  2097,  2010,  4845,\n","          1005,  1055,  2215,  2000,  2079,  2279,  1029,   102,  2175,  2041,\n","          2000,  8439,   102,     0,     0,     0],\n","        [  101, 22712,  2777,  4074,  1005,  1055,  3899,  2005,  1996,  2034,\n","          2051,  2008,  2154,  1012, 22712,  3662,  4074,  1005,  1055,  3899,\n","          7167,  1997,  2293,  1012,   102,  2129,  2052, 22712,  2514,  2044,\n","          1029,   102,  2919,   102,     0,     0],\n","        [  101,  5863,  1005,  1055,  2457,  3058,  2633,  3369,  2073,  2027,\n","          2052,  2022,  2583,  2000,  3288,  1996,  7409,  2000,  3979,  1012,\n","           102,  2129,  2052,  2017,  6235,  5863,  1029,   102,  2019,  4905,\n","           102,     0,     0,     0,     0,     0],\n","        [  101, 22712, 14382,  2005,  1996,  2390,  1998,  2359,  2000,  2191,\n","          2014,  2155,  1998,  2406,  7098,  1012,   102,  2129,  2052,  2017,\n","          6235, 22712,  1029,   102,  2066,  1037,  2204,  6926,   102,     0,\n","             0,     0,     0,     0,     0,     0],\n","        [  101,  7627,  2288,  2014,  4268,  2362,  1998,  2741,  2037,  6656,\n","         17433,  2000,  2816,  1012,   102,  2339,  2106,  7627,  2079,  2023,\n","          1029,   102,  4604,  2068,  2000,  2082,  2279,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0],\n","        [  101,  8925,  2371, 14699,  2044,  2054,  3047,  2197,  2305,  1012,\n","           102,  2054,  2097,  8925,  2215,  2000,  2079,  2279,  1029,   102,\n","          2681,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0]]), 'B': tensor([[  101, 14509,  2356,  3994,  1005,  1055,  2767,  2000,  2175,  2000,\n","          1996,  3153,  2023,  2746,  5958,  1012,   102,  2339,  2106, 14509,\n","          2079,  2023,  1029,   102, 12230,   102,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0],\n","        [  101,  8804,  2741,  5863,  2000,  2082,  2130,  2295,  2016,  2001,\n","          2025,  3110,  2092,  1012,   102,  2129,  2052,  5863,  2514,  2044,\n","          2023,  1029,   102,  3201,  2000,  2202,  1037,  3328,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0],\n","        [  101,  3389,  2354,  3071,  2012,  1996,  2283,  1012,  2027,  5720,\n","          2000,  2169,  1997,  2068,  1012,   102,  2054,  2097,  4148,  2000,\n","          3389,  1029,   102,  2022,  2356,  2000,  2681,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0],\n","        [  101,  5863,  2003,  1037,  3586, 10365,  1010,  1998,  2371,  2014,\n","          1051, 11149,  2008,  2016, 14172,  2000,  5194,  1012,   102,  2129,\n","          2052,  2017,  6235,  5863,  1029,   102, 21591,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0],\n","        [  101,  9036,  2359,  2000,  1996,  3942,  1996,  2307,  2813,  1997,\n","          2859,  2651,  1012,   102,  2054,  2097,  9036,  2215,  2000,  2079,\n","          2279,  1029,   102,  2131,  2000,  1996,  2307,  2813,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0],\n","        [  101,  5553,  2736, 27970,  1005,  1055,  2147,  2138, 27970,  2347,\n","          1005,  1056,  2583,  2000,  1012,   102,  2129,  2097, 27970,  2514,\n","          1029,   102,  2022,  3755,   102,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0],\n","        [  101,  4074,  2196,  2052,  2031,  2179,  2041,  2065,  2002,  2910,\n","          1005,  1056,  2179,  1996,  2293,  3661,  2010,  2564,  2741,  2000,\n","          2619,  2842,  1012,   102,  2129,  2052,  4074,  2514,  5728,  1029,\n","           102, 12266,   102,     0,     0,     0,     0],\n","        [  101, 18961,  2253,  2000,  2026,  2767,  1005,  1055,  2160,  2000,\n","          2298,  2005,  2033,  2021,  6343,  2001,  2583,  2000,  2424,  2033,\n","          1012,   102,  2054,  2097,  2500,  2215,  2000,  2079,  2279,  1029,\n","           102,  2298,  7880,  2842,   102,     0,     0],\n","        [  101,  5553,  2001,  2183,  2006,  1037, 13215,  4440,  1012,  5553,\n","          4149,  1037,  5777,  4524,  2013,  1037,  3573,  1012,   102,  2339,\n","          2106,  5553,  2079,  2023,  1029,   102,  2031,  1037,  3835,  3637,\n","           102,     0,     0,     0,     0,     0,     0],\n","        [  101, 18961,  1998, 18403,  2020,  1999,  1996,  2168,  2396,  2465,\n","          2061, 18961,  2787,  2000,  3443, 18403,  1005,  1055, 12492,  1012,\n","           102,  2129,  2052, 18961,  2514,  5728,  1029,   102,  7098,  2008,\n","          2016,  2001,  2583,  2000,  2079,  2009,   102],\n","        [  101,  5899,  2288,  2010,  4268,  3189,  4003,  1998,  2371,  2061,\n","          7098,  1997,  2037,  7022,  1012,   102,  2054,  2097,  2010,  4845,\n","          1005,  1055,  2215,  2000,  2079,  2279,  1029,   102,  2175, 19840,\n","          2125,  2006, 19453,   102,     0,     0,     0],\n","        [  101, 22712,  2777,  4074,  1005,  1055,  3899,  2005,  1996,  2034,\n","          2051,  2008,  2154,  1012, 22712,  3662,  4074,  1005,  1055,  3899,\n","          7167,  1997,  2293,  1012,   102,  2129,  2052, 22712,  2514,  2044,\n","          1029,   102,  5905,   102,     0,     0,     0],\n","        [  101,  5863,  1005,  1055,  2457,  3058,  2633,  3369,  2073,  2027,\n","          2052,  2022,  2583,  2000,  3288,  1996,  7409,  2000,  3979,  1012,\n","           102,  2129,  2052,  2017,  6235,  5863,  1029,   102,  6091,   102,\n","             0,     0,     0,     0,     0,     0,     0],\n","        [  101, 22712, 14382,  2005,  1996,  2390,  1998,  2359,  2000,  2191,\n","          2014,  2155,  1998,  2406,  7098,  1012,   102,  2129,  2052,  2017,\n","          6235, 22712,  1029,   102,  3110,  2844,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0],\n","        [  101,  7627,  2288,  2014,  4268,  2362,  1998,  2741,  2037,  6656,\n","         17433,  2000,  2816,  1012,   102,  2339,  2106,  7627,  2079,  2023,\n","          1029,   102,  2359,  2000,  2191,  2469,  1996,  2336,  2020,  5851,\n","           102,     0,     0,     0,     0,     0,     0],\n","        [  101,  8925,  2371, 14699,  2044,  2054,  3047,  2197,  2305,  1012,\n","           102,  2054,  2097,  8925,  2215,  2000,  2079,  2279,  1029,   102,\n","          2175,  2185,   102,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0]]), 'C': tensor([[  101, 14509,  2356,  3994,  1005,  1055,  2767,  2000,  2175,  2000,\n","          1996,  3153,  2023,  2746,  5958,  1012,   102,  2339,  2106, 14509,\n","          2079,  2023,  1029,   102, 13135,   102,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0],\n","        [  101,  8804,  2741,  5863,  2000,  2082,  2130,  2295,  2016,  2001,\n","          2025,  3110,  2092,  1012,   102,  2129,  2052,  5863,  2514,  2044,\n","          2023,  1029,   102,  1037,  9384,  6687,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0],\n","        [  101,  3389,  2354,  3071,  2012,  1996,  2283,  1012,  2027,  5720,\n","          2000,  2169,  1997,  2068,  1012,   102,  2054,  2097,  4148,  2000,\n","          3389,  1029,   102,  2031,  1037,  6659,  2051,   102,     0,     0,\n","             0,     0,     0,     0,     0],\n","        [  101,  5863,  2003,  1037,  3586, 10365,  1010,  1998,  2371,  2014,\n","          1051, 11149,  2008,  2016, 14172,  2000,  5194,  1012,   102,  2129,\n","          2052,  2017,  6235,  5863,  1029,   102,  2777,  2594, 16203,   102,\n","             0,     0,     0,     0,     0],\n","        [  101,  9036,  2359,  2000,  1996,  3942,  1996,  2307,  2813,  1997,\n","          2859,  2651,  1012,   102,  2054,  2097,  9036,  2215,  2000,  2079,\n","          2279,  1029,   102,  2156,  1996, 15925,  1997,  2859,   102,     0,\n","             0,     0,     0,     0,     0],\n","        [  101,  5553,  2736, 27970,  1005,  1055,  2147,  2138, 27970,  2347,\n","          1005,  1056,  2583,  2000,  1012,   102,  2129,  2097, 27970,  2514,\n","          1029,   102,  2131,  1037,  2154,  2125,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0],\n","        [  101,  4074,  2196,  2052,  2031,  2179,  2041,  2065,  2002,  2910,\n","          1005,  1056,  2179,  1996,  2293,  3661,  2010,  2564,  2741,  2000,\n","          2619,  2842,  1012,   102,  2129,  2052,  4074,  2514,  5728,  1029,\n","           102, 12056,   102,     0,     0],\n","        [  101, 18961,  2253,  2000,  2026,  2767,  1005,  1055,  2160,  2000,\n","          2298,  2005,  2033,  2021,  6343,  2001,  2583,  2000,  2424,  2033,\n","          1012,   102,  2054,  2097,  2500,  2215,  2000,  2079,  2279,  1029,\n","           102,  2203,  1996,  3945,   102],\n","        [  101,  5553,  2001,  2183,  2006,  1037, 13215,  4440,  1012,  5553,\n","          4149,  1037,  5777,  4524,  2013,  1037,  3573,  1012,   102,  2339,\n","          2106,  5553,  2079,  2023,  1029,   102,  2933,  2073,  2027,  2024,\n","          2183, 13215,   102,     0,     0],\n","        [  101, 18961,  1998, 18403,  2020,  1999,  1996,  2168,  2396,  2465,\n","          2061, 18961,  2787,  2000,  3443, 18403,  1005,  1055, 12492,  1012,\n","           102,  2129,  2052, 18961,  2514,  5728,  1029,   102, 10206,  2008,\n","          2016,  2106,  2009,   102,     0],\n","        [  101,  5899,  2288,  2010,  4268,  3189,  4003,  1998,  2371,  2061,\n","          7098,  1997,  2037,  7022,  1012,   102,  2054,  2097,  2010,  4845,\n","          1005,  1055,  2215,  2000,  2079,  2279,  1029,   102,  2175,  2000,\n","          2793,   102,     0,     0,     0],\n","        [  101, 22712,  2777,  4074,  1005,  1055,  3899,  2005,  1996,  2034,\n","          2051,  2008,  2154,  1012, 22712,  3662,  4074,  1005,  1055,  3899,\n","          7167,  1997,  2293,  1012,   102,  2129,  2052, 22712,  2514,  2044,\n","          1029,   102,  2204,   102,     0],\n","        [  101,  5863,  1005,  1055,  2457,  3058,  2633,  3369,  2073,  2027,\n","          2052,  2022,  2583,  2000,  3288,  1996,  7409,  2000,  3979,  1012,\n","           102,  2129,  2052,  2017,  6235,  5863,  1029,   102,  1037,  3648,\n","           102,     0,     0,     0,     0],\n","        [  101, 22712, 14382,  2005,  1996,  2390,  1998,  2359,  2000,  2191,\n","          2014,  2155,  1998,  2406,  7098,  1012,   102,  2129,  2052,  2017,\n","          6235, 22712,  1029,   102,  3110, 12230,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0],\n","        [  101,  7627,  2288,  2014,  4268,  2362,  1998,  2741,  2037,  6656,\n","         17433,  2000,  2816,  1012,   102,  2339,  2106,  7627,  2079,  2023,\n","          1029,   102, 12210,  2027,  2357,  1999,  1996, 17433,  2279,   102,\n","             0,     0,     0,     0,     0],\n","        [  101,  8925,  2371, 14699,  2044,  2054,  3047,  2197,  2305,  1012,\n","           102,  2054,  2097,  8925,  2215,  2000,  2079,  2279,  1029,   102,\n","          9483,  6650,   102,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0]])}\n","batch_attn_mask:\n"," {'A': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'B': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'C': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n","batch_labels:\n"," tensor([0, 0, 0, 2, 1, 0, 2, 0, 1, 1, 0, 2, 1, 0, 1, 2])\n"]}],"source":["batch_input_ids, batch_attn_mask, batch_labels = next(iter(train_loader))\n","print(f\"batch_input_ids:\\n {batch_input_ids}\")\n","print(f\"batch_attn_mask:\\n {batch_attn_mask}\")\n","print(f\"batch_labels:\\n {batch_labels}\")"]},{"cell_type":"markdown","id":"849edfc0","metadata":{"id":"849edfc0"},"source":["## Task 2: Implementing and Training BERT-based Multiple Choice Classifier (1 hour 30 minutes)\n","\n","Similar to pretrained tokenizers, the transformers library also provide numerous pre-trained language models that can be fine-tuned on a wide variety of downstream tasks. We demonstrate usage of these models below."]},{"cell_type":"code","execution_count":21,"id":"c9322600","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":781,"referenced_widgets":["af87bc0d27a94cfebb30ff85d25c5c23","0d70707d2b9840989916f60c08a8fb4d","78009e2fe6a54d34877b7aad8eec5b7e","d71ef10644904600bef97571e2abf391","729e77353f814a67b778558458ed5b0e","47ff5d014bd24b68b9d642c074831efa","1c0bcf7b9be3434aadabd6011139330e","a8f05782a0d14730b0ffccc6d7b9b10c","7709420a948741faa92e7ce4e0364897","16124dfb5aaf4170917b8f33710807c8","1066fa77715e440498cc137a48f23345"]},"executionInfo":{"elapsed":1833,"status":"ok","timestamp":1715707629692,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"c9322600","outputId":"4cb61412-f565-45c9-b4f0-18ec3a00ecef"},"outputs":[{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af87bc0d27a94cfebb30ff85d25c5c23"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0-11): 12 x BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":21}],"source":["# Import BertModel from the library\n","from transformers import BertModel\n","\n","# Create an instance of pretrained BERT\n","bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n","bert_model"]},{"cell_type":"markdown","id":"4a7f074b","metadata":{"id":"4a7f074b"},"source":["As you can see very similar to how we created pre-trained tokenizer, we can load a pretrained BERT model by calling `BertModel.from_pretrained(bert-base-uncased)`. This can actually be considered just a Pytorch `nn.Module` like `nn.Linear` and can be similarly plugged into a network architecture. Also, notice the model contains 12 BERT layers, where each layer consists of a Self Attention layer followed by a sequence of linear layers and activation functions (MLP), as we discussed when talking about Transformer architecture in the lecture."]},{"cell_type":"code","execution_count":22,"id":"c994b139","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2333,"status":"ok","timestamp":1715707632023,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"c994b139","outputId":"d03938e6-954f-49c1-81d7-cf81bf6421c9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2823, -0.2353, -0.3529,  ..., -0.0834,  0.2548,  0.4870],\n","         [ 0.4055, -1.1768, -0.2842,  ..., -0.3740,  0.3920, -0.4480],\n","         [ 0.0377, -0.7788, -0.1174,  ..., -0.4201, -0.3078,  0.1824],\n","         ...,\n","         [-1.1595, -1.5650, -0.2526,  ..., -0.4569, -0.5474,  0.2315],\n","         [-1.0644, -0.5952, -0.3912,  ...,  0.2788, -0.0207, -0.1262],\n","         [ 0.5158,  0.4573,  0.0263,  ...,  0.1445, -0.6398, -0.5258]]],\n","       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-7.1342e-01, -3.3350e-01, -4.4551e-01,  4.5434e-01,  5.0347e-01,\n","         -8.4619e-02,  4.8077e-01,  8.4160e-02, -2.2890e-01, -9.9974e-01,\n","         -2.4549e-01,  7.9481e-01,  9.8030e-01, -2.8880e-02,  9.1588e-01,\n","         -2.4136e-01,  5.3666e-01, -5.0656e-01,  2.2218e-01,  3.2123e-01,\n","          6.4942e-01,  9.9757e-01,  4.7173e-01,  2.0766e-01,  3.6500e-01,\n","          8.8725e-01, -4.2194e-01,  9.3109e-01,  9.1743e-01,  7.2114e-01,\n","         -1.4355e-02, -1.8941e-02, -9.8951e-01,  3.0721e-02, -2.8992e-01,\n","         -9.8191e-01,  1.7840e-01, -5.9347e-01,  1.1879e-01,  2.5965e-01,\n","         -8.6462e-01,  1.4619e-01,  9.9952e-01, -4.7740e-01,  1.0605e-01,\n","         -1.2106e-01, -9.9986e-01,  2.2875e-01, -8.5771e-01,  5.0007e-01,\n","          2.7700e-01,  7.1537e-01,  9.0395e-02,  3.0770e-01,  3.6540e-01,\n","         -7.2003e-02, -2.7237e-01, -2.9246e-02, -2.4881e-01, -4.1984e-01,\n","         -6.4784e-01,  1.4369e-01, -5.4503e-01, -7.8703e-01,  4.1153e-01,\n","          2.9400e-01, -1.3110e-01, -8.3965e-02,  6.9508e-03, -1.4154e-01,\n","          6.8059e-01,  8.8732e-02,  1.5826e-02, -8.0095e-01,  1.2098e-01,\n","          1.5737e-01, -5.1418e-01,  1.0000e+00,  4.2994e-02, -9.8423e-01,\n","          1.7809e-01,  2.3727e-01,  3.4371e-01,  4.4902e-01, -2.5424e-01,\n","         -1.0000e+00,  3.7378e-01, -2.9287e-03, -9.9117e-01,  1.0483e-01,\n","          4.8637e-01, -2.1140e-01, -1.3496e-01,  4.0792e-01,  5.8095e-02,\n","         -1.5223e-01, -2.3065e-01, -4.2501e-01, -1.0952e-01, -1.6091e-01,\n","          2.2054e-01,  7.6145e-02, -4.6461e-02, -2.1545e-01,  2.0504e-01,\n","         -4.3479e-01,  2.3062e-02,  4.2032e-01, -3.0370e-01,  5.1830e-01,\n","          3.7295e-01, -1.9882e-01,  2.8470e-01, -9.4566e-01,  3.9429e-01,\n","         -2.5140e-01, -9.8011e-01, -4.6321e-01, -9.9085e-01,  6.2031e-01,\n","          3.0362e-02, -2.4470e-01,  9.4980e-01,  4.4613e-01,  1.0056e-01,\n","          1.2657e-01, -4.9239e-01, -1.0000e+00, -3.8979e-01,  1.7306e-02,\n","          6.2905e-02,  7.5699e-03, -9.6858e-01, -9.6126e-01,  3.2523e-01,\n","          9.4696e-01,  2.4653e-02,  9.9795e-01, -1.0900e-01,  9.2609e-01,\n","          2.7076e-01, -2.3979e-01,  2.4420e-03, -4.1432e-01,  3.1651e-01,\n","         -2.5690e-01, -5.1866e-02,  1.3838e-01, -1.3608e-01,  1.5090e-02,\n","         -2.7091e-01,  1.0166e-02, -6.0731e-02, -9.0828e-01, -2.3963e-01,\n","          9.5270e-01, -1.2421e-01, -5.1564e-01,  4.5463e-01, -9.4697e-02,\n","          1.9799e-02,  6.7040e-01,  3.5773e-01,  3.0287e-01, -3.0182e-01,\n","          3.2714e-01, -3.3120e-01,  4.4667e-01, -5.9530e-01,  3.9171e-01,\n","          2.3601e-01, -1.9295e-01, -1.1556e-01, -9.7889e-01, -2.1943e-01,\n","          1.7873e-01,  9.8326e-01,  6.1193e-01,  1.2009e-01,  4.6305e-01,\n","         -2.2367e-01,  5.1362e-01, -9.4201e-01,  9.8198e-01,  2.3595e-02,\n","          1.6000e-01, -1.6324e-01,  2.4670e-01, -8.0511e-01, -3.3924e-01,\n","          4.8846e-01, -5.1100e-01, -7.3509e-01,  4.6975e-02, -3.3179e-01,\n","         -1.8198e-01, -4.1462e-01,  1.0465e-01, -2.1432e-01, -3.4277e-01,\n","          9.7266e-02,  9.3661e-01,  7.2161e-01,  4.8405e-01, -3.6871e-01,\n","          3.1814e-01, -8.4417e-01, -4.6677e-01,  2.3358e-02,  8.2281e-02,\n","         -3.5102e-02,  9.9018e-01, -2.8120e-01,  1.5138e-01, -8.7715e-01,\n","         -9.8174e-01, -1.8074e-01, -8.3024e-01, -1.6251e-01, -4.6465e-01,\n","          4.1388e-01, -5.0448e-01,  7.9151e-03,  1.6464e-01, -8.4324e-01,\n","         -5.9927e-01,  3.1949e-01, -1.7097e-01,  3.2296e-01, -2.7389e-01,\n","          9.0340e-01,  6.0541e-01, -4.7819e-01, -3.6939e-01,  9.1910e-01,\n","         -2.9349e-01, -7.2003e-01,  3.8927e-01, -1.0532e-01,  5.2019e-01,\n","         -4.1045e-01,  9.4339e-01,  6.6610e-01,  4.9983e-01, -8.7163e-01,\n","          1.0599e-02, -6.2514e-01,  5.1351e-02,  7.7707e-04, -5.0177e-01,\n","          2.8171e-01,  4.3400e-01,  2.9356e-01,  7.4288e-01, -4.1969e-02,\n","          8.6796e-01, -9.1876e-01, -9.4036e-01, -7.8274e-01,  9.0406e-02,\n","         -9.8649e-01,  1.0411e-01,  1.7797e-01, -1.0403e-01, -2.1737e-01,\n","         -1.7412e-01, -9.4650e-01,  3.6575e-01, -4.9280e-02,  9.1307e-01,\n","         -3.6831e-01, -6.6574e-01, -4.2411e-01, -9.2306e-01, -2.2497e-01,\n","         -1.3000e-01,  6.1594e-02, -1.6496e-01, -9.4146e-01,  4.3231e-01,\n","          4.3532e-01,  4.4700e-01, -1.5150e-01,  9.6835e-01,  9.9996e-01,\n","          9.6499e-01,  8.9856e-01,  4.0772e-01, -9.9106e-01, -7.2477e-01,\n","          9.9990e-01, -8.8650e-01, -9.9999e-01, -8.8209e-01, -3.2363e-01,\n","         -1.0676e-02, -1.0000e+00, -6.8102e-02,  2.3007e-01, -8.1374e-01,\n","          1.3577e-01,  9.7153e-01,  9.1199e-01, -1.0000e+00,  7.6972e-01,\n","          9.3895e-01, -4.7636e-01,  7.2471e-01, -1.8839e-01,  9.6564e-01,\n","          2.1396e-01,  3.6877e-01, -6.0258e-02,  2.6505e-01, -5.1107e-01,\n","         -4.2091e-01,  1.9413e-02, -2.5991e-01,  9.7003e-01, -2.1713e-02,\n","         -4.3161e-01, -8.6556e-01,  2.8448e-01,  5.9014e-02, -4.4330e-01,\n","         -9.4856e-01, -1.1291e-01,  2.5513e-02,  3.7114e-01,  9.9050e-03,\n","          5.2219e-02, -3.4138e-01, -3.1813e-02, -3.0110e-01, -5.4604e-02,\n","          5.3167e-01, -9.1741e-01, -2.2246e-01, -1.0530e-02, -4.1509e-01,\n","          3.0833e-01, -9.7295e-01,  9.5327e-01, -2.9202e-01,  5.2656e-01,\n","          1.0000e+00, -1.5967e-02, -7.8224e-01,  2.5205e-01,  8.0371e-02,\n","         -1.1654e-01,  1.0000e+00,  5.2342e-01, -9.7992e-01, -4.5646e-01,\n","          4.4127e-01, -3.6342e-01, -5.4687e-01,  9.9663e-01, -1.6571e-01,\n","         -2.4606e-01,  7.1765e-02,  9.8694e-01, -9.8773e-01,  9.2531e-01,\n","         -7.8466e-01, -9.7499e-01,  9.5526e-01,  9.4014e-01, -3.1455e-02,\n","         -3.7029e-01, -8.3566e-02,  7.2885e-02,  7.8481e-02, -8.5670e-01,\n","          2.4106e-01,  1.2382e-01,  2.2440e-02,  9.0840e-01,  4.9766e-02,\n","         -4.8200e-01,  1.0067e-01, -3.3606e-01,  2.7572e-01,  5.1258e-01,\n","          3.4788e-01,  2.0840e-02, -4.1822e-02,  2.2078e-02, -5.3827e-01,\n","         -9.6365e-01,  5.3595e-01,  1.0000e+00,  1.7988e-01,  2.2127e-01,\n","          1.1171e-01,  4.2887e-02, -2.8482e-01,  2.7275e-01,  2.8186e-01,\n","         -1.9890e-01, -6.9904e-01,  5.4585e-01, -8.2020e-01, -9.8801e-01,\n","          3.9557e-01,  1.4115e-01, -9.0601e-02,  9.9788e-01,  5.2465e-02,\n","          8.7988e-02, -6.1069e-02,  8.0411e-01, -1.0107e-01,  4.5896e-02,\n","          2.8721e-01,  9.7186e-01, -5.0391e-02,  4.3895e-01,  5.3262e-01,\n","         -3.7363e-01, -1.4349e-01, -5.3335e-01, -1.7289e-01, -9.3699e-01,\n","          2.4583e-01, -9.5441e-01,  9.4962e-01,  6.8467e-01,  2.8843e-01,\n","          2.7844e-02,  1.9399e-01,  1.0000e+00, -5.5154e-01,  2.7470e-01,\n","          7.3334e-01,  2.1665e-01, -9.9376e-01, -6.3638e-01, -4.0457e-01,\n","          8.1165e-02, -1.0932e-01, -1.6521e-01,  1.1090e-01, -9.6194e-01,\n","          1.4183e-01,  2.6996e-01, -9.0304e-01, -9.8854e-01, -1.8411e-01,\n","         -1.4559e-01,  1.0233e-01, -8.8277e-01, -4.2014e-01, -5.6760e-01,\n","          2.0938e-01, -7.8962e-02, -9.2779e-01,  3.7535e-01, -3.2771e-01,\n","          3.7110e-01, -1.7455e-02,  4.4611e-01,  3.7664e-01,  8.9600e-01,\n","         -1.4597e-01, -4.1541e-02, -2.2711e-02, -5.6408e-01,  4.9451e-01,\n","         -6.0685e-01, -5.7528e-01,  1.6080e-02,  1.0000e+00, -2.4831e-01,\n","          4.7874e-01,  3.2856e-01,  4.0354e-01,  3.5562e-02,  7.1661e-02,\n","          5.1131e-01,  1.7113e-01, -5.2168e-04, -2.9695e-01,  7.3751e-01,\n","         -1.8529e-01,  4.4554e-01,  2.2911e-01,  2.8562e-02,  7.7319e-01,\n","          5.5270e-01,  1.0272e-01,  2.6211e-01, -1.2720e-03,  9.7222e-01,\n","         -2.8842e-02,  5.6738e-02, -2.8775e-01, -1.6172e-02, -1.9353e-01,\n","          4.9592e-01,  1.0000e+00,  8.4379e-02, -1.5369e-01, -9.8806e-01,\n","         -3.3429e-01, -7.0185e-01,  9.9986e-01,  7.6013e-01, -6.0292e-01,\n","          3.8539e-01,  2.5654e-01, -1.1947e-01,  3.1904e-01, -2.9968e-02,\n","         -8.2039e-02, -3.6788e-02, -4.1350e-02,  9.4251e-01, -3.8563e-01,\n","         -9.6644e-01, -1.8887e-01,  3.3731e-01, -9.5321e-01,  9.9444e-01,\n","         -3.1648e-01, -7.6926e-02, -2.2873e-01, -1.2125e-01, -8.2212e-01,\n","         -1.8324e-01, -9.8104e-01,  3.3368e-03,  6.5977e-02,  9.6485e-01,\n","          6.2124e-02, -4.1838e-01, -8.8928e-01,  4.6512e-01,  1.6997e-01,\n","         -4.9288e-01, -9.0313e-01,  9.4608e-01, -9.6796e-01,  4.0968e-01,\n","          9.9998e-01,  2.5584e-01, -4.0539e-01,  1.3442e-01, -1.9956e-01,\n","          2.1901e-01, -7.6149e-02,  4.1793e-01, -9.3571e-01, -2.5734e-01,\n","         -2.6220e-02,  1.9381e-01, -1.0859e-02,  1.0624e-02,  5.5476e-01,\n","          1.3884e-01, -3.7362e-01, -5.1091e-01, -2.0794e-02,  2.3886e-01,\n","          4.4453e-01, -1.9578e-01,  2.4322e-02,  1.2190e-01,  4.5708e-02,\n","         -8.9249e-01, -2.3091e-01, -2.3275e-01, -9.9933e-01,  3.8215e-01,\n","         -1.0000e+00,  2.4109e-01, -3.3744e-01, -9.5531e-02,  7.7688e-01,\n","          6.7944e-01,  5.0124e-01, -5.4854e-01, -4.4367e-01,  7.4304e-01,\n","          6.9326e-01, -1.1128e-01,  5.8066e-02, -5.1514e-01, -1.8508e-02,\n","          4.7066e-02, -3.9159e-02, -1.2526e-01,  6.4872e-01, -2.8116e-01,\n","          1.0000e+00,  9.4141e-02, -1.6344e-01, -8.3718e-01,  9.6640e-02,\n","         -1.5380e-01,  9.9999e-01, -4.3615e-01, -9.4652e-01,  1.8444e-01,\n","         -3.2900e-01, -7.3108e-01,  2.8296e-01, -1.6438e-01, -5.5741e-01,\n","         -5.1496e-01,  9.4393e-01,  1.5646e-01, -4.8041e-01,  3.7670e-01,\n","         -7.6502e-02, -3.4085e-01, -2.3005e-01,  4.6745e-01,  9.8561e-01,\n","          1.8930e-01,  6.5902e-01, -1.4607e-01, -5.6216e-02,  9.6718e-01,\n","          2.0109e-01, -4.4703e-01,  2.6056e-03,  1.0000e+00,  2.5435e-01,\n","         -8.5391e-01,  1.4343e-01, -9.7004e-01,  6.7542e-02, -9.1234e-01,\n","          2.3352e-01, -1.0207e-01,  9.0571e-01, -1.2382e-01,  8.9837e-01,\n","         -2.5035e-01, -9.6540e-02, -1.3500e-02,  7.9872e-02,  3.4886e-01,\n","         -9.0798e-01, -9.8696e-01, -9.8399e-01,  2.2660e-01, -2.6349e-01,\n","         -1.7568e-03,  2.5412e-01, -4.6728e-02,  3.2300e-01,  2.3782e-01,\n","         -1.0000e+00,  9.4718e-01,  2.8214e-01,  5.5079e-01,  9.5964e-01,\n","          4.3230e-01,  2.9824e-01,  1.1796e-01, -9.8534e-01, -9.1499e-01,\n","         -2.2060e-01, -2.2493e-01,  4.2716e-01,  4.1828e-01,  7.7361e-01,\n","          2.5652e-01, -4.4088e-01, -5.4283e-01, -1.9669e-01, -9.2347e-01,\n","         -9.9092e-01,  1.6564e-01, -2.1975e-02, -7.5007e-01,  9.5135e-01,\n","         -4.2565e-01,  4.1158e-02,  3.4651e-01, -3.7918e-01,  5.3755e-01,\n","          6.6361e-01, -1.7167e-01, -1.6186e-01,  3.5413e-01,  8.6273e-01,\n","          5.4597e-01,  9.7368e-01, -3.1056e-01,  3.8867e-01, -3.5970e-01,\n","          3.0381e-01,  7.2659e-01, -8.7832e-01,  4.5919e-02,  5.9937e-02,\n","          1.7157e-01,  1.6686e-01, -1.6294e-01, -7.8675e-01,  4.3243e-02,\n","         -2.4215e-01,  9.7457e-02, -2.9032e-01,  2.1996e-01, -2.2742e-01,\n","          4.9331e-02, -3.6432e-01, -2.3407e-01,  5.0983e-01, -1.0163e-01,\n","          9.1016e-01,  5.7243e-01,  4.2006e-02, -4.0053e-01, -9.6790e-03,\n","         -1.7138e-01, -8.6321e-01,  6.2138e-01,  2.0165e-01,  2.1101e-01,\n","          1.2278e-01, -1.7986e-01,  8.6453e-01, -5.4444e-01, -3.0941e-01,\n","         -3.3675e-01, -3.4412e-01,  7.0476e-01, -5.8997e-01, -3.5156e-01,\n","         -5.8885e-02,  4.5485e-01,  1.9294e-01,  9.9796e-01, -1.2719e-01,\n","         -3.9385e-01, -3.1919e-01, -2.4795e-01,  2.6886e-01,  3.8581e-02,\n","         -1.0000e+00,  1.7946e-01, -1.6161e-01, -9.5156e-02, -2.4447e-01,\n","          3.1448e-01, -3.1113e-01, -9.0355e-01, -1.8078e-02,  5.0788e-01,\n","          4.1148e-01, -4.0674e-01, -3.7335e-01,  4.8784e-01, -2.1414e-01,\n","          8.2619e-01,  8.2344e-01, -1.7268e-01,  6.6992e-01,  5.1202e-01,\n","         -4.5926e-01, -5.4121e-01,  9.0145e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"]},"metadata":{},"execution_count":22}],"source":["sentence = \"kendall was a person who kept her word exquisitely, so she got my money the other day\"\n","tokenizer_output = bert_tokenizer(sentence, return_tensors=\"pt\")\n","input_ids, attn_mask = tokenizer_output[\"input_ids\"], tokenizer_output[\"attention_mask\"]\n","\n","output = bert_model(input_ids, attention_mask = attn_mask)\n","output"]},{"cell_type":"markdown","id":"919f7bfc","metadata":{"id":"919f7bfc"},"source":["As you can see, calling `bert_model` returns a bunch of different things. Let's go through them one by one and understand"]},{"cell_type":"code","execution_count":23,"id":"1f337132","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1715707632023,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"1f337132","outputId":"77c089dc-4b58-4ea1-8bad-0853cef65931"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_ids shape: torch.Size([1, 21])\n","last_hidden_state shape: torch.Size([1, 21, 768])\n"]}],"source":["last_hidden_state = output.last_hidden_state\n","print(f\"input_ids shape: {input_ids.shape}\")\n","print(f\"last_hidden_state shape: {last_hidden_state.shape}\")"]},{"cell_type":"markdown","id":"7fa9e5c2","metadata":{"id":"7fa9e5c2"},"source":["For an input of shape `[1,21]` which just means a single sequence of 21 tokens, last_hidden_state is a tensor of shape `[1, 21, 768]` denoting the contextual embedding of each of the 21 tokens in the sequence. These representations can be then used for solving a downstream task, by adding a linear layer or MLP layer on top. These can be useful for sequence labelling type of tasks."]},{"cell_type":"code","execution_count":24,"id":"c9c00632","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1715707632023,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"c9c00632","outputId":"0547c00d-4708-408d-fa06-85546c41d791"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_ids shape: torch.Size([1, 21])\n","pooler_output shape: torch.Size([1, 768])\n"]}],"source":["pooler_output = output.pooler_output\n","print(f\"input_ids shape: {input_ids.shape}\")\n","print(f\"pooler_output shape: {pooler_output.shape}\")"]},{"cell_type":"markdown","id":"88dae74d","metadata":{"id":"88dae74d"},"source":["`pooler_output` is an aggregate representation of the entire sentence and can be thought of as a sentence embedding. It is obtained by passing the representation of the \\[CLS\\] token through a linear layer. This can be useful for sentence-level tasks like sentiment analysis as well as multiple choice classification tasks etc."]},{"cell_type":"markdown","id":"5785c1f7","metadata":{"id":"5785c1f7"},"source":["Apart from these two we can also obtain other values by providing additional arguments. Like if we want to obtain attention maps which can be useful for interpretating the model's behavior, we can just specify `output_attentions=True` while calling the model"]},{"cell_type":"code","execution_count":25,"id":"50ae6ee7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1715707632023,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"50ae6ee7","outputId":"92c24228-05d0-462b-ee3d-b34c20248544"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data type of attentions output: <class 'tuple'>\n","Number of elements: 12\n","Shape of individual element: torch.Size([1, 12, 21, 21])\n","Example attention map: tensor([[0.0365, 0.0188, 0.0239, 0.0918, 0.0375, 0.0409, 0.0390, 0.0327, 0.0256,\n","         0.0260, 0.0488, 0.0387, 0.0532, 0.0188, 0.0251, 0.0408, 0.0321, 0.0840,\n","         0.0793, 0.0262, 0.1801],\n","        [0.0178, 0.0460, 0.0240, 0.0306, 0.0482, 0.0194, 0.0510, 0.0919, 0.0260,\n","         0.0818, 0.0425, 0.0482, 0.0159, 0.0576, 0.0703, 0.1526, 0.0337, 0.0405,\n","         0.0194, 0.0371, 0.0454],\n","        [0.0527, 0.0466, 0.0899, 0.0241, 0.0464, 0.0245, 0.0645, 0.0587, 0.0256,\n","         0.0401, 0.0298, 0.0606, 0.0302, 0.0735, 0.1222, 0.0573, 0.0297, 0.0204,\n","         0.0224, 0.0539, 0.0268],\n","        [0.0425, 0.0528, 0.0454, 0.0338, 0.0402, 0.0353, 0.0515, 0.0666, 0.0397,\n","         0.0464, 0.0336, 0.0518, 0.0492, 0.0609, 0.0529, 0.0731, 0.0509, 0.0580,\n","         0.0381, 0.0492, 0.0280],\n","        [0.0492, 0.1514, 0.0892, 0.0078, 0.0237, 0.0282, 0.0478, 0.0639, 0.0288,\n","         0.0346, 0.0376, 0.0427, 0.0386, 0.0865, 0.0633, 0.0612, 0.0589, 0.0099,\n","         0.0217, 0.0326, 0.0225],\n","        [0.0328, 0.0851, 0.0669, 0.0210, 0.0342, 0.0278, 0.0618, 0.0965, 0.0244,\n","         0.0388, 0.0382, 0.0554, 0.0281, 0.0809, 0.0671, 0.0634, 0.0372, 0.0276,\n","         0.0302, 0.0289, 0.0536],\n","        [0.0390, 0.0402, 0.1059, 0.0350, 0.0359, 0.0453, 0.0443, 0.0639, 0.0261,\n","         0.0437, 0.0524, 0.0837, 0.0440, 0.0611, 0.0661, 0.0560, 0.0265, 0.0284,\n","         0.0247, 0.0368, 0.0410],\n","        [0.0637, 0.0480, 0.0736, 0.0488, 0.0387, 0.0301, 0.0386, 0.1177, 0.0174,\n","         0.0286, 0.0322, 0.0448, 0.0351, 0.0715, 0.0595, 0.0795, 0.0202, 0.0495,\n","         0.0239, 0.0297, 0.0488],\n","        [0.0379, 0.0930, 0.0471, 0.0373, 0.0634, 0.0240, 0.0501, 0.0724, 0.0077,\n","         0.0576, 0.1025, 0.0672, 0.0309, 0.0510, 0.0276, 0.0513, 0.0243, 0.0229,\n","         0.0249, 0.0429, 0.0641],\n","        [0.0287, 0.1002, 0.0683, 0.0142, 0.0493, 0.0313, 0.0819, 0.0516, 0.0350,\n","         0.0476, 0.0602, 0.0453, 0.0288, 0.0470, 0.1209, 0.0661, 0.0310, 0.0140,\n","         0.0197, 0.0225, 0.0367],\n","        [0.0285, 0.0755, 0.0461, 0.0240, 0.0530, 0.0329, 0.0516, 0.0549, 0.0636,\n","         0.0900, 0.0263, 0.0202, 0.0290, 0.0448, 0.0628, 0.0496, 0.0781, 0.0263,\n","         0.0399, 0.0470, 0.0559],\n","        [0.0478, 0.0464, 0.0582, 0.0576, 0.0513, 0.0424, 0.0580, 0.0508, 0.0362,\n","         0.0565, 0.0378, 0.0387, 0.0565, 0.0367, 0.0458, 0.0434, 0.0477, 0.0464,\n","         0.0433, 0.0513, 0.0475],\n","        [0.0320, 0.0923, 0.0587, 0.0300, 0.0370, 0.0399, 0.0736, 0.1011, 0.0183,\n","         0.0490, 0.0473, 0.0639, 0.0212, 0.0637, 0.0596, 0.0799, 0.0234, 0.0308,\n","         0.0306, 0.0149, 0.0330],\n","        [0.0335, 0.0893, 0.1025, 0.0227, 0.0361, 0.0223, 0.0555, 0.1325, 0.0104,\n","         0.0403, 0.0382, 0.0476, 0.0227, 0.0636, 0.0781, 0.0949, 0.0216, 0.0269,\n","         0.0159, 0.0252, 0.0202],\n","        [0.0544, 0.1423, 0.0956, 0.0254, 0.0281, 0.0320, 0.0471, 0.0546, 0.0136,\n","         0.0673, 0.0418, 0.0395, 0.0531, 0.0422, 0.0400, 0.1038, 0.0275, 0.0331,\n","         0.0201, 0.0151, 0.0234],\n","        [0.0403, 0.0612, 0.0584, 0.0240, 0.0489, 0.0368, 0.0703, 0.0585, 0.0444,\n","         0.0276, 0.0329, 0.0323, 0.0453, 0.0484, 0.1187, 0.0948, 0.0410, 0.0257,\n","         0.0229, 0.0184, 0.0490],\n","        [0.0249, 0.0599, 0.0252, 0.0498, 0.0809, 0.0517, 0.0329, 0.0799, 0.0372,\n","         0.0461, 0.0336, 0.0263, 0.0299, 0.0659, 0.0677, 0.0610, 0.0249, 0.0287,\n","         0.0350, 0.0519, 0.0864],\n","        [0.0298, 0.0571, 0.0516, 0.0481, 0.0414, 0.0276, 0.0582, 0.0824, 0.0294,\n","         0.0469, 0.0406, 0.0386, 0.0444, 0.0660, 0.0562, 0.0832, 0.0321, 0.0636,\n","         0.0414, 0.0343, 0.0273],\n","        [0.0209, 0.0511, 0.0464, 0.0348, 0.0476, 0.0474, 0.0860, 0.0561, 0.0389,\n","         0.0365, 0.0628, 0.0366, 0.0508, 0.0486, 0.0764, 0.0663, 0.0431, 0.0291,\n","         0.0349, 0.0329, 0.0528],\n","        [0.0228, 0.1058, 0.1149, 0.0203, 0.0668, 0.0299, 0.0469, 0.0448, 0.0492,\n","         0.0439, 0.0376, 0.0540, 0.0458, 0.0640, 0.0903, 0.0373, 0.0353, 0.0134,\n","         0.0297, 0.0104, 0.0368],\n","        [0.0342, 0.0267, 0.0292, 0.0788, 0.0409, 0.0381, 0.0354, 0.0651, 0.0243,\n","         0.0478, 0.0643, 0.0683, 0.0461, 0.0384, 0.0317, 0.0849, 0.0342, 0.0737,\n","         0.0565, 0.0304, 0.0511]], grad_fn=<SelectBackward0>)\n"]}],"source":["output = bert_model(input_ids, attention_mask = attn_mask, output_attentions=True)\n","attentions = output.attentions\n","print(f\"Data type of attentions output: {type(attentions)}\")\n","print(f\"Number of elements: {len(attentions)}\")\n","print(f\"Shape of individual element: {attentions[0].shape}\")\n","print(f\"Example attention map: {attentions[0][0,0]}\")"]},{"cell_type":"markdown","id":"a3a4519b","metadata":{"id":"a3a4519b"},"source":["As you can see `attentions` is a tuple containing 12 elements which corresponds to the attention maps of each of the 12 layers in the network. Further each layer's attention maps also contains 12 attention maps corresponding to 12 heads in each layer. A single attention map as you can see is a 18x18 matrix representing the attention pattern for all the tokens in the sequence"]},{"cell_type":"markdown","id":"8a79f03f","metadata":{"id":"8a79f03f"},"source":["### Task 2.1: Implementing BERT-based Classifier for Multiple Choice Classification\n","\n","In this task you will implement a bert-based classifier in Pytorch very similar to how we created bag of word classifiers in the previous assignments. The architecture of the model is as follows:\n","\n","![architecture](https://i.ibb.co/hVmS9Qx/siqa-bert-arch-excalli.png)\n","\n","Essentially, what we have here is a model that takes a context and question, and scores a particular answer (denoted as a score(a)). At the backbone we have the BERT model, using which we obtain the contextualized representation of the [context, question, answer] sequence. We then use the \\[CLS\\] token's embedding as the sequence representation and feed it to a 2 layer MLP (Linear(768, 768) -> ReLU -> Linear(768, 1)) that scores the answer. To predict the correct answer, score each of the three answers, obtain their scores and normalize them by applying softmax, that gives us the probability of each option being the correct answer.\n","\n","\n","![forward pass](https://i.ibb.co/r3SrLHY/siqa-bert-forward-excalli.png)\n","\n","Implement the architecture and forward pass in `BertMultiChoiceClassifierModel` class below:"]},{"cell_type":"code","execution_count":26,"id":"0f626042","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1715707632023,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"0f626042"},"outputs":[],"source":["class BertMultiChoiceClassifierModel(nn.Module):\n","\n","    def __init__(self, d_hidden = 768, bert_variant = \"bert-base-uncased\"):\n","        \"\"\"\n","        Define the architecture of Bert-Based mulit-choice classifier.\n","        You will mainly need to define 3 components, first a BERT layer\n","        using `BertModel` from transformers library,\n","        a two layer MLP layer to map the representation from Bert to the output i.e. (Linear(d_hidden, d_hidden) -> ReLU -> Linear(d_hidden, 1)),\n","        and a log sftmax layer to map the scores to a probabilities\n","\n","        Inputs:\n","            - d_hidden (int): Size of the hidden representations of bert\n","            - bert_variant (str): BERT variant to use\n","        \"\"\"\n","        super(BertMultiChoiceClassifierModel, self).__init__()\n","        self.bert_layer = BertModel.from_pretrained(bert_variant)\n","        self.mlp_layer = nn.Sequential(nn.Linear(d_hidden, d_hidden), nn.ReLU(), nn.Linear(d_hidden, 1))\n","        self.log_softmax_layer = nn.LogSoftmax()\n","\n","    def forward(self, input_ids_dict, attn_mask_dict):\n","        \"\"\"\n","        Forward Passes the inputs through the network and obtains the prediction\n","\n","        Inputs:\n","            - input_ids_dict (dict(str,torch.tensor)): A dictionary containing input_ids corresponding to each answer choice. Keys are A, B and C and value is a torch tensor of shape [batch_size, seq_len]\n","                                        representing the sequence of token ids\n","            - attn_mask_dict (dict(str,torch.tensor)): A dictionary containing attention mask corresponding to each answer choice. Keys are A, B and C and value is a torch tensor of shape [batch_size, seq_len]\n","\n","        Returns:\n","          - output (torch.tensor): A torch tensor of shape [batch_size,] obtained after passing the input to the network\n","\n","\n","        Hints:\n","            1. Recall which of the outputs from BertModel is appropriate for the sentence classification task and how to access it.\n","            2. `torch.cat` might come in handy before performing softmax\n","        \"\"\"\n","        out = []\n","        for i in \"ABC\":\n","            input_ids = input_ids_dict[i]\n","            attn_mask = attn_mask_dict[i]\n","            bert_out = self.bert_layer(input_ids, attention_mask=attn_mask).pooler_output\n","            mlp_out = self.mlp_layer(bert_out)\n","            out.append(mlp_out)\n","\n","        return self.log_softmax_layer(torch.cat(out, axis=1))"]},{"cell_type":"code","execution_count":27,"id":"1c65d7c7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8069,"status":"ok","timestamp":1715707640086,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"1c65d7c7","outputId":"891101bc-b67c-4da6-d750-10959e3ffe46"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running Sample Test Cases!\n","Sample Test Case 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return self._call_impl(*args, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Model Output: [[-1.1189675 -1.0885006 -1.0886753]\n"," [-1.1045516 -1.0834142 -1.108049 ]\n"," [-1.1027124 -1.0822923 -1.1110513]\n"," [-1.1008494 -1.0936637 -1.1013424]\n"," [-1.0921423 -1.0974909 -1.1062545]\n"," [-1.0798944 -1.1088551 -1.1073539]\n"," [-1.1030428 -1.0939085 -1.0989064]\n"," [-1.0971036 -1.0970919 -1.1016482]\n"," [-1.1319212 -1.082568  -1.0821619]\n"," [-1.0961349 -1.1014836 -1.0982256]\n"," [-1.0979306 -1.0836825 -1.1144607]\n"," [-1.1034716 -1.0959275 -1.0964555]\n"," [-1.101945  -1.0958116 -1.0980897]\n"," [-1.1050864 -1.0986388 -1.0921534]\n"," [-1.1013197 -1.082134  -1.1126211]\n"," [-1.102798  -1.0906713 -1.1024151]]\n","Expected Output: [[-1.1189675 -1.0885007 -1.0886753]\n"," [-1.1045516 -1.0834142 -1.108049 ]\n"," [-1.1027125 -1.0822924 -1.1110513]\n"," [-1.1008494 -1.0936636 -1.1013424]\n"," [-1.0921422 -1.0974907 -1.1062546]\n"," [-1.0798943 -1.1088552 -1.1073538]\n"," [-1.1030427 -1.0939085 -1.0989065]\n"," [-1.0971034 -1.097092  -1.1016482]\n"," [-1.131921  -1.0825679 -1.0821619]\n"," [-1.0961349 -1.1014836 -1.0982255]\n"," [-1.0979307 -1.0836827 -1.1144608]\n"," [-1.1034715 -1.0959275 -1.0964555]\n"," [-1.1019452 -1.0958116 -1.0980899]\n"," [-1.1050864 -1.0986389 -1.0921533]\n"," [-1.1013198 -1.0821339 -1.112621 ]\n"," [-1.1027979 -1.0906712 -1.1024152]]\n","Test Case Passed! :)\n","******************************\n","\n","Sample Test Case 2\n","Model Output: [[-1.100536  -1.1009303 -1.094384 ]\n"," [-1.0732511 -1.1178818 -1.1052345]\n"," [-1.1025076 -1.094363  -1.098983 ]\n"," [-1.1236264 -1.1056153 -1.0674216]\n"," [-1.0999552 -1.1014045 -1.0944903]\n"," [-1.0953273 -1.0959655 -1.1045707]\n"," [-1.10844   -1.0971686 -1.0903118]\n"," [-1.0993489 -1.1130905 -1.0836148]\n"," [-1.1031718 -1.0897288 -1.1029955]\n"," [-1.0929244 -1.1077558 -1.0952207]\n"," [-1.0995094 -1.0998485 -1.0964826]\n"," [-1.1419643 -1.1081928 -1.0479566]\n"," [-1.1052556 -1.0851237 -1.1055952]\n"," [-1.0840428 -1.1084775 -1.1034834]\n"," [-1.0872697 -1.1025085 -1.1061593]\n"," [-1.1060572 -1.0939908 -1.0958309]]\n","Expected Output: [[-1.1005359 -1.1009303 -1.094384 ]\n"," [-1.073251  -1.1178819 -1.1052346]\n"," [-1.1025076 -1.094363  -1.098983 ]\n"," [-1.1236262 -1.1056151 -1.0674216]\n"," [-1.0999551 -1.1014045 -1.0944905]\n"," [-1.0953273 -1.0959654 -1.1045709]\n"," [-1.1084402 -1.0971687 -1.0903118]\n"," [-1.099349  -1.1130908 -1.0836148]\n"," [-1.1031718 -1.0897288 -1.1029954]\n"," [-1.0929244 -1.1077557 -1.0952206]\n"," [-1.0995092 -1.0998485 -1.0964826]\n"," [-1.1419646 -1.1081928 -1.0479565]\n"," [-1.1052557 -1.0851235 -1.1055952]\n"," [-1.0840428 -1.1084775 -1.1034834]\n"," [-1.0872697 -1.1025085 -1.1061592]\n"," [-1.1060572 -1.0939908 -1.095831 ]]\n","Test Case Passed! :)\n","******************************\n","\n"]}],"source":["print(f\"Running Sample Test Cases!\")\n","torch.manual_seed(42)\n","model = BertMultiChoiceClassifierModel()\n","\n","print(\"Sample Test Case 1\")\n","batch_input_ids, batch_attn_mask, batch_labels = next(iter(train_loader))\n","bert_out = model(batch_input_ids, batch_attn_mask).detach().numpy()\n","expected_bert_out = np.array([[-1.1189675, -1.0885007, -1.0886753],\n","                            [-1.1045516, -1.0834142, -1.108049 ],\n","                            [-1.1027125, -1.0822924, -1.1110513],\n","                            [-1.1008494, -1.0936636, -1.1013424],\n","                            [-1.0921422, -1.0974907, -1.1062546],\n","                            [-1.0798943, -1.1088552, -1.1073538],\n","                            [-1.1030427, -1.0939085, -1.0989065],\n","                            [-1.0971034, -1.097092 , -1.1016482],\n","                            [-1.131921 , -1.0825679, -1.0821619],\n","                            [-1.0961349, -1.1014836, -1.0982255],\n","                            [-1.0979307, -1.0836827, -1.1144608],\n","                            [-1.1034715, -1.0959275, -1.0964555],\n","                            [-1.1019452, -1.0958116, -1.0980899],\n","                            [-1.1050864, -1.0986389, -1.0921533],\n","                            [-1.1013198, -1.0821339, -1.112621 ],\n","                            [-1.1027979, -1.0906712, -1.1024152]],)\n","print(f\"Model Output: {bert_out}\")\n","print(f\"Expected Output: {expected_bert_out}\")\n","\n","assert bert_out.shape == expected_bert_out.shape\n","assert np.allclose(bert_out, expected_bert_out, 1e-4)\n","print(\"Test Case Passed! :)\")\n","print(\"******************************\\n\")\n","\n","print(\"Sample Test Case 2\")\n","batch_input_ids, batch_attn_mask, batch_labels = next(iter(dev_loader))\n","bert_out = model(batch_input_ids, batch_attn_mask).detach().numpy()\n","expected_bert_out = np.array([[-1.1005359, -1.1009303, -1.094384 ],\n","                            [-1.073251 , -1.1178819, -1.1052346],\n","                            [-1.1025076, -1.094363 , -1.098983 ],\n","                            [-1.1236262, -1.1056151, -1.0674216],\n","                            [-1.0999551, -1.1014045, -1.0944905],\n","                            [-1.0953273, -1.0959654, -1.1045709],\n","                            [-1.1084402, -1.0971687, -1.0903118],\n","                            [-1.099349 , -1.1130908, -1.0836148],\n","                            [-1.1031718, -1.0897288, -1.1029954],\n","                            [-1.0929244, -1.1077557, -1.0952206],\n","                            [-1.0995092, -1.0998485, -1.0964826],\n","                            [-1.1419646, -1.1081928, -1.0479565],\n","                            [-1.1052557, -1.0851235, -1.1055952],\n","                            [-1.0840428, -1.1084775, -1.1034834],\n","                            [-1.0872697, -1.1025085, -1.1061592],\n","                            [-1.1060572, -1.0939908, -1.095831 ]])\n","print(f\"Model Output: {bert_out}\")\n","print(f\"Expected Output: {expected_bert_out}\")\n","\n","assert bert_out.shape == expected_bert_out.shape\n","assert np.allclose(bert_out, expected_bert_out, 1e-4)\n","print(\"Test Case Passed! :)\")\n","print(\"******************************\\n\")"]},{"cell_type":"markdown","id":"f38f523b","metadata":{"id":"f38f523b"},"source":["### Task 2.2: Training and Evaluating the Model\n","\n","Now that we have implemented the custom Dataset and a BERT based classifier model, we can start training and evaluating the model. This time we will modify the training loop slightly. At the end of each training epoch we will now evaluate on the validation data and check the accuracy. Based on this we will select the best model across the epochs that obtains highest validation accuracy. You will need to implement the `train` and `evaluate` functions below."]},{"cell_type":"code","execution_count":31,"id":"af1e43b1","metadata":{"executionInfo":{"elapsed":1068,"status":"ok","timestamp":1715707802625,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"af1e43b1"},"outputs":[],"source":["def evaluate(model, test_dataloader, device = \"cpu\"):\n","    \"\"\"\n","    Evaluates `model` on test dataset\n","\n","    Inputs:\n","        - model (BertMultiChoiceClassifierModel): BERT based multiple choice classifier model to be evaluated\n","        - test_dataloader (torch.utils.DataLoader): A dataloader defined over the test dataset\n","\n","    Returns:\n","        - accuracy (float): Average accuracy over the test dataset\n","    \"\"\"\n","\n","    model.eval()\n","    model = model.to(device)\n","    accuracy = 0\n","\n","    with torch.no_grad():\n","        for test_batch in test_dataloader:\n","\n","            # Read the batch from dataloader\n","            input_ids_dict, attn_mask_dict, labels = test_batch\n","\n","            # Send all values of dicts to device\n","            for key in input_ids_dict.keys():\n","                input_ids_dict[key] = input_ids_dict[key].to(device)\n","                attn_mask_dict[key] = attn_mask_dict[key].to(device)\n","            labels = labels.float()\n","\n","            # Step 1: Compute model's prediction on the test batch (Note here you need to get the final prediction from the model's output)\n","            preds = model(input_ids_dict, attn_mask_dict).detach().numpy()\n","\n","            # Step 2: then compute accuracy and store it in batch_accuracy\n","            batch_accuracy = (preds == labels).sum().item() / len(labels)\n","\n","            accuracy += batch_accuracy\n","\n","    accuracy = accuracy / len(test_dataloader)\n","    return accuracy\n","\n","\n","def train(model, train_dataloader, val_dataloader,\n","          lr = 1e-5, num_epochs = 3,\n","          device = \"cpu\"):\n","    \"\"\"\n","    Runs the training loop. Define the loss function as BCELoss like the last tine\n","    and optimizer as Adam and traine for `num_epochs` epochs.\n","\n","    Inputs:\n","        - model (BertMultiChoiceClassifierModel): BERT based classifer model to be trained\n","        - train_dataloader (torch.utils.DataLoader): A dataloader defined over the training dataset\n","        - val_dataloader (torch.utils.DataLoader): A dataloader defined over the validation dataset\n","        - lr (float): The learning rate for the optimizer\n","        - num_epochs (int): Number of epochs to train the model for.\n","        - device (str): Device to train the model on. Can be either 'cuda' (for using gpu) or 'cpu'\n","\n","    Returns:\n","        - best_model (BertMultiChoiceClassifierModel): model corresponding to the highest validation accuracy (checked at the end of each epoch)\n","        - best_val_accuracy (float): Validation accuracy corresponding to the best epoch\n","    \"\"\"\n","    epoch_loss = 0\n","    model = model.to(device)\n","\n","    best_val_accuracy = float(\"-inf\")\n","    best_model = None\n","\n","    # 1. Define Loss function and optimizer\n","    loss_fn = nn.NLLLoss()\n","    optimizer = Adam(model.parameters(), lr=lr)\n","\n","    # Iterate over `num_epochs`\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0 # We can use this to keep track of how the loss value changes as we train the model.\n","        # Iterate over each batch using the `train_dataloader`\n","        for train_batch in tqdm(train_dataloader):\n","\n","            # Zero out any gradients stored in the previous steps\n","            optimizer.zero_grad()\n","\n","            # Read the batch from dataloader\n","            input_ids_dict, attn_mask_dict, labels = train_batch\n","\n","            # Send all values of dicts to device\n","            for key in input_ids_dict.keys():\n","                input_ids_dict[key] = input_ids_dict[key].to(device)\n","                attn_mask_dict[key] = attn_mask_dict[key].to(device)\n","            labels = labels.to(device)\n","\n","            # Step 3: Feed the input features to the model to get outputs log-probabilities\n","            model_outs = model(input_ids_dict, attn_mask_dict).to(device)\n","\n","            # Step 4: Compute the loss and perform backward pass\n","            loss = loss_fn(model_outs, labels)\n","            loss.backward()\n","\n","            # Step 5: Take optimizer step\n","            optimizer.step()\n","\n","            # Store loss value for tracking\n","            epoch_loss += loss.item()\n","\n","        epoch_loss = epoch_loss / len(train_dataloader)\n","        # Step 6. Evaluate on validation data by calling `evaluate` and store the validation accuracy in `val_accurracy`\n","        total = 0\n","        correct = 0\n","        for val_batch in val_dataloader:\n","            input_ids_dict, attn_mask_dict, labels = val_batch\n","            for key in input_ids_dict.keys():\n","                input_ids_dict[key] = input_ids_dict[key].to(device)\n","                attn_mask_dict[key] = attn_mask_dict[key].to(device)\n","            labels = labels.to(device)\n","            pred = torch.argmax(model(input_ids_dict, attn_mask_dict).to(device), axis=1)\n","            correct += (pred==labels).sum().item()\n","            total += len(labels)\n","        val_accuracy = total / correct\n","\n","        # Model selection\n","        if val_accuracy > best_val_accuracy:\n","            best_val_accuracy = val_accuracy\n","            best_model = copy.deepcopy(model) # Create a copy of model\n","\n","        print(f\"Epoch {epoch} completed | Average Training Loss: {epoch_loss} | Validation Accuracy: {val_accuracy}\")\n","\n","    return best_model, best_val_accuracy"]},{"cell_type":"code","execution_count":32,"id":"b408695c","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":376,"referenced_widgets":["a774620dca634d19a69b51481ca1f5d1","906f316162a1458397cf64128f413787","075f94a71d024a0886a6d06c9c504414","e7aa079e44964f4daf6ab7b95a05fac7","8a953ee83566478488c27f189d2d7e3a","7b19cb538f6a45d090abd16c81f3ad59","ca2ae6b2ff2e49b7b7a95aab0ccb1cdc","d3e478cddabd405ab6f4c42adcadf6b9","696ca5f5c54b407ea988bcd283e5c358","91d28d996eb84f09966b59f8f08de998","9b506ac3afcb46c48dbf237bdcaaf10c","db5f00a7601a4953974a8949e96eda26","3097ec1ba7a6494e8ab30035db06d997","af9584a98dcf4c94807acd1940866797","c9dd4a5bccad412a838025fad3868975","ffd266da58984e4793e4c79842eb2b2a","ff2a0dc879d4489bada2f82b662a8eec","f6f36cf0c46d46bbb2a151ac804ded5a","e5cff852273d4427993fa8f9f929ef88","19c510ba3fbc486f8d8aca78eb08bf89","c7458af6fcae462d84b6ccb4e9fab78d","88f89af6b15a473d817338a1fc6eb807","1cff078c7aba411bb007a81a5fde1b59","62f80dc6369f42db870e55421484e34b","e94ee3fcd82b4c6fb0e1720d81061fed","f4aa07ae03144ad0b328cbd098be6776","b8cbc1ae7dd34597a33fa39fd29fbe88","97a234cae7274c86980ea428cd7a158c","09714dfa9e9d44dc8817b01fa41a61a8","a91335f4107545238a7390b6cfbe2717","06ab9eabc8424f2e820f8da4e3f5d25c","e6b1604fb6d1413fbe5ba32d1b281090","d2eb22cd61b74978b6ae1ff89ab04634","5c5820d085ce48de80b44fa5a96053b5","5161cbcea2d74553adc25d8a580f8293","b61424e6be3c412dbe800681b9700039","21683c67b888404ca1b38e77b3f46fff","b870724440f64f2fb856cf4b931494ec","1d756555859f4906a2fbf666da481659","536e7d4ed02b48dbb716e03e70cf063d","7e3f0dfa5a5b4ee5b90ca908a22d525b","e09f8ae48e66481391f7718e70d067f4","006dcb2712a441c69ed9408bed8ba338","d05f885d581441afaf01434adc01a332","575ee175448141559b5a25c80996eebb","7f3d9408c3bd41239d5e4e4df4bf9ab1","e9c1d08571d5423caaca051a09d3383a","dc026f03e2b84d388ff1b7a024d5e23a","834b3b220b634191b1ec46a8c8c9d69f","b63c2d29144c4d5187a3ac4fc61addaf","bc8cb692013848528046db0fcc7a33fa","26f6b849b019415a83c6777b3970b22f","8816af9fe3e148758d8c77308b6bd31b","30706676dc554d5bb5e2009225780bed","534453d0249a4e468b91de711eef372e"]},"executionInfo":{"elapsed":26760,"status":"ok","timestamp":1715707832470,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"},"user_tz":-330},"id":"b408695c","outputId":"b3c0d0e3-3a7a-48dd-844b-cfa3b0f52d58"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training on 100 data points for sanity check\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a774620dca634d19a69b51481ca1f5d1"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return self._call_impl(*args, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0 completed | Average Training Loss: 1.099404911994934 | Validation Accuracy: 1.2820512820512822\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db5f00a7601a4953974a8949e96eda26"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1 completed | Average Training Loss: 1.005476462841034 | Validation Accuracy: 1.1363636363636365\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cff078c7aba411bb007a81a5fde1b59"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 2 completed | Average Training Loss: 0.5784622395038604 | Validation Accuracy: 1.0416666666666667\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c5820d085ce48de80b44fa5a96053b5"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 3 completed | Average Training Loss: 0.34795909315347673 | Validation Accuracy: 1.0\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"575ee175448141559b5a25c80996eebb"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 4 completed | Average Training Loss: 0.10256962414830922 | Validation Accuracy: 1.0\n","Best Validation Accuracy: 1.2820512820512822\n","Expected Best Validation Accuracy: 1.0\n"]}],"source":["torch.manual_seed(42)\n","print(\"Training on 100 data points for sanity check\")\n","sample_data = train_data[:100]\n","sample_labels = train_labels[:100]\n","sample_dataset = SIQABertDataset(sample_data, sample_labels)\n","sample_dataloader = DataLoader(sample_dataset, batch_size=4, collate_fn=partial(collate_fn, sample_dataset.tokenizer))\n","\n","model = BertMultiChoiceClassifierModel()\n","best_model, best_val_acc = train(model, sample_dataloader, sample_dataloader, num_epochs = 5, device = \"cuda\")\n","print(f\"Best Validation Accuracy: {best_val_acc}\")\n","print(f\"Expected Best Validation Accuracy: {1.0}\")"]},{"cell_type":"markdown","id":"e958b3c7","metadata":{"id":"e958b3c7"},"source":[" You can expect the validation accuracy of 1.0 by the end of training. This is so high because we trained on just 100 examples and just use those for validation for a sanity check. This is often done to debug the model and training loop. Let's now train on the entire dataset. This can take some time approximately 50 minutes per epoch, since we are fine-tuning all the 12 layers of BERT."]},{"cell_type":"code","execution_count":34,"id":"298b6cf0","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":137,"referenced_widgets":["9c9d8c4d82c34dabae13dda21cbeb984","b539c503cb5a4072bee3b118e2e3ba7d","dc6626a53e034811b808969762fc6672","4d18fc7795964e70b773906623248524","11079d39f5d4487fabaa9e92f546111b","81f9e51455af470d9819b43d23319b6e","64bb15e63bd84b1faad2c2c0c6e4f3e3","4fd2aa9759694b9d84a9f98a50009b41","00a08695fd8c480ea0ae95209150bfdd","10aafbef473847d29be30fac9e77e3aa","34b7ad4e75474b94ad29ffadf0c982ed","7e6367fa8c41439eb9551596d01bc892","4e7f73f06e7c49938c3bda1e896867a6","3eca24357b474435b6efa10c2b02bd46","dbc0e25adadf457fad3012e5ab90c675","33c9b06b865647f98edf5bd4f2771623","2949966b3722460882024dcf3a533f6e","125d9cb151654b2285c431ce3b798ca1","1e807121b1b3454cb7804e21b67bc565","ed2f235080bd4426af1ecf888ebf0d79","946ba990bbbd49e8aa07c41b08d21ff8","6694a6817258484cbb6e00cbe8a66cf6"]},"id":"298b6cf0","outputId":"698a49d0-193b-40d7-ff49-7f0d5c49d041","executionInfo":{"status":"ok","timestamp":1715709554634,"user_tz":-330,"elapsed":1685480,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2089 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c9d8c4d82c34dabae13dda21cbeb984"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 0 completed | Average Training Loss: 0.7121439405167257 | Validation Accuracy: 1.672945205479452\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2089 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e6367fa8c41439eb9551596d01bc892"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1 completed | Average Training Loss: 0.4363252982843173 | Validation Accuracy: 1.6801375752364573\n"]}],"source":["model = BertMultiChoiceClassifierModel()\n","best_model, best_val_acc = train(model, train_loader, dev_loader, num_epochs = 2, device = \"cuda\")"]},{"cell_type":"markdown","id":"a397015f","metadata":{"id":"a397015f"},"source":["You should expect about ~61% validation accuracy (random classifier will have an accuracy of 33%), which is around what's reported in the SocialIQA paper. Note that this is a much more complex task than the news classification that we had in the last lab. You can further improve the performance by using bigger models like bert-base-large or roberta-large."]},{"cell_type":"markdown","id":"95b18060","metadata":{"id":"95b18060"},"source":["Now that we have a model ready for the task, we can save it on disk, so we can use it later (This will come handy for Assignment2)"]},{"cell_type":"code","execution_count":35,"id":"c3168433","metadata":{"id":"c3168433","executionInfo":{"status":"ok","timestamp":1715709568062,"user_tz":-330,"elapsed":4351,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"}}},"outputs":[],"source":["# Save the best model\n","save_dir = \"models/siqa_bert-base-uncased/\"\n","if not os.path.exists(save_dir):\n","    os.makedirs(save_dir)\n","\n","torch.save(best_model.state_dict(), f\"{save_dir}/model.pt\")"]},{"cell_type":"markdown","id":"82f7ea51","metadata":{"id":"82f7ea51"},"source":["### Task 2.3: Making Predictions from scratch\n","\n","Similar to assignment 1, implement the function `predict_siqa` that takes as input the context, question and answers and runs them through the BERT classifier model to obtain the prediction."]},{"cell_type":"code","execution_count":61,"id":"639651fe","metadata":{"id":"639651fe","executionInfo":{"status":"ok","timestamp":1715711046151,"user_tz":-330,"elapsed":3,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"}}},"outputs":[],"source":["def predict_text(siqa_instance, model, tokenizer,device = \"cpu\"):\n","    \"\"\"\n","    Predicts the correct answer for a piece of a Social IQA instance using the BERT classifier model\n","\n","    Inputs:\n","        - siqa_instance (dict(str, str)): An SIQA instance containing the context, question and the three answer choices.\n","        - model (BertMultiChoiceClassifierModel): Fine-tuned BERT based classifer model\n","        - tokenizer (BertTokenizer): Pre-trained BERT tokenizer\n","    Returns:\n","        - pred_label (float): Predicted answer for `siqa_instance`\n","    \"\"\"\n","\n","    model = model.to(device)\n","    model.eval()\n","\n","    d = siqa_instance\n","    context = d['context']\n","    question = d['question']\n","    tokenized_input_dict = {\n","        i: tokenizer(str(tokenizer.sep_token).join([context, question, d[f'answer{i}']]))\n","        for i in \"ABC\"\n","    }\n","\n","    pred_label = None\n","\n","    # Step 1: Tokenize the [sentence, question, answer] triplet using the tokenizer and create input_ids_dict and attn_mask_dict, as done in the Dataset class\n","    # (Don't forget to convert the lists to tensors, torch.Tensor() can come handy or just use return_tensors = \"pt\" while calling the tokenizer)\n","\n","    input_ids_dict = {i: torch.Tensor(tokenized_input_dict[i]['input_ids']).to(device) for i in \"ABC\"}\n","    attn_mask_dict = {i: torch.Tensor(tokenized_input_dict[i]['attention_mask']).to(device) for i in \"ABC\"}\n","\n","    # Step 2: Feed the input_ids_dict and attn_mask_dict to the model and get the final predictions\n","    # (Don't forget torch.no_grad())\n","    pred_label = None\n","    with torch.no_grad():\n","      pred_label = model(input_ids_dict, attn_mask_dict)\n","\n","    # Step 3: Make the predicted human readable i.e. convert 0 to A, 1 to B and 2 to C\n","    pred_label_hr = \"ABC\"[pred_label]\n","\n","    return pred_label_hr"]},{"cell_type":"code","execution_count":60,"id":"b79990de","metadata":{"id":"b79990de","colab":{"base_uri":"https://localhost:8080/","height":506,"referenced_widgets":["6185b7ac55bf4e25bed994e3eeb1e9bb","af92bc8ed41c4d6aa1b3af9524433d7b","9f16149328cf4dfaabbb64144031f09b","1ed2d84558624e5aa67fa8b3f761b3b7","a467c1f4b4c14532a010330131fb0732","bf7d7a8058cd44219525cf94e088eb9e","60a9c93dad9c4df3bd60258114c27173","96378a18cc2d494dafe50a04af966462","27ae85e2bb8c4c0ea94825d8837a699a","4ec3d21dd9ed4730994a3c64df9054cf","3bfdd2fb866f4d7fa35da617eca495c1"]},"executionInfo":{"status":"error","timestamp":1715711041840,"user_tz":-330,"elapsed":479,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"}},"outputId":"a73f0be4-8d07-4ec3-ac1b-007641373810"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running Sample Test Case. If the implementation is correct, we should get the same accuracy as best_val_acc above, by predicting on each example of the dev data\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1954 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6185b7ac55bf4e25bed994e3eeb1e9bb"}},"metadata":{}},{"output_type":"error","ename":"ValueError","evalue":"not enough values to unpack (expected 2, got 1)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-60-1b031504fb0e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running Sample Test Case. If the implementation is correct, we should get the same accuracy as best_val_acc above, by predicting on each example of the dev data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m preds = [\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mpredict_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiqa_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msiqa_instance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ]\n","\u001b[0;32m<ipython-input-60-1b031504fb0e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running Sample Test Case. If the implementation is correct, we should get the same accuracy as best_val_acc above, by predicting on each example of the dev data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m preds = [\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpredict_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiqa_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msiqa_instance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ]\n","\u001b[0;32m<ipython-input-59-fe375a6628a0>\u001b[0m in \u001b[0;36mpredict_text\u001b[0;34m(siqa_instance, model, tokenizer, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Step 3: Make the predicted human readable i.e. convert 0 to A, 1 to B and 2 to C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-f6257fd6733c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids_dict, attn_mask_dict)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mattn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_mask_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mbert_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mmlp_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify either input_ids or inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"]}],"source":["print(\"Running Sample Test Case. If the implementation is correct, we should get the same accuracy as best_val_acc above, by predicting on each example of the dev data\")\n","preds = [\n","    predict_text(siqa_instance, best_model, bert_tokenizer, device = \"cuda\")\n","    for siqa_instance in tqdm(dev_data)\n","]\n","test_case_accuracy = (np.array(preds) == np.array(dev_labels)).mean()\n","print(f\"Accuracy by calling `predict_text`: {test_case_accuracy}\")\n","print(f\"Expected Accuracy: {best_val_acc}\")\n","\n","assert np.allclose(test_case_accuracy, best_val_acc, 1e-2)\n","print(\"Test Case Passed! :)\")\n","print(\"******************************\\n\")"]},{"cell_type":"code","execution_count":56,"id":"7c969000","metadata":{"id":"7c969000","colab":{"base_uri":"https://localhost:8080/","height":418},"executionInfo":{"status":"error","timestamp":1715710697939,"user_tz":-330,"elapsed":597,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"}},"outputId":"975fa9e6-6281-4b87-adf9-93fb406592a8"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"not enough values to unpack (expected 2, got 1)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-8c731a9433c9>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msample_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredicted_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mexpected_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"C\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_dicts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-54-fe375a6628a0>\u001b[0m in \u001b[0;36mpredict_text\u001b[0;34m(siqa_instance, model, tokenizer, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Step 3: Make the predicted human readable i.e. convert 0 to A, 1 to B and 2 to C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-f6257fd6733c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids_dict, attn_mask_dict)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mattn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_mask_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mbert_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mmlp_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify either input_ids or inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"]}],"source":["idx = 0\n","sample_data= dev_data[idx]\n","predicted_label = predict_text(sample_data, best_model, bert_tokenizer)\n","expected_label = \"C\"\n","pprint(sample_data, sort_dicts=False, indent = 4)\n","print(f\"Predicted Label: {predicted_label}\")\n","print(f\"Gold Label: {dev_labels[idx]}\")\n","print(\"**********************************\\n\")\n","\n","idx = 100\n","sample_data= dev_data[idx]\n","predicted_label = predict_text(sample_data, best_model, bert_tokenizer)\n","expected_label = \"C\"\n","pprint(sample_data, sort_dicts=False, indent = 4)\n","print(f\"Predicted Label: {predicted_label}\")\n","print(f\"Gold Label: {dev_labels[idx]}\")\n","print(\"**********************************\\n\")\n","\n","\n","idx = 200\n","sample_data= dev_data[idx]\n","predicted_label = predict_text(sample_data, best_model, bert_tokenizer)\n","expected_label = \"C\"\n","pprint(sample_data, sort_dicts=False, indent = 4)\n","print(f\"Predicted Label: {predicted_label}\")\n","print(f\"Gold Label: {dev_labels[idx]}\")\n","\n","print(\"**********************************\\n\")"]},{"cell_type":"code","execution_count":null,"id":"5ef516c5","metadata":{"id":"5ef516c5","executionInfo":{"status":"aborted","timestamp":1715707664188,"user_tz":-330,"elapsed":7,"user":{"displayName":"Rajat Jacob","userId":"06897348020561788018"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"72eacd71006f4438a80de874bb6ded91":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4851c0c97e154450b5a2d2b043c58fdd","IPY_MODEL_f7ad800541c843afa178370cff41eb4f","IPY_MODEL_a0dfb2fb51fb472c9949fcdaec756a72"],"layout":"IPY_MODEL_aeed9a1d264f4f38a61e97e46e49d829"}},"4851c0c97e154450b5a2d2b043c58fdd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba38ae15268649b680e963be8a8dab36","placeholder":"â€‹","style":"IPY_MODEL_41f3b328891d43baaac58f2341ced50d","value":"tokenizer_config.json:â€‡100%"}},"f7ad800541c843afa178370cff41eb4f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd4ad52979d7438e9a60958933da4016","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f75a95481e0d4bafaaca14fa6a88e597","value":48}},"a0dfb2fb51fb472c9949fcdaec756a72":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c2a40d646284c7ab37491ee06e96cf5","placeholder":"â€‹","style":"IPY_MODEL_84e4fa1d5f8646a69a4be14197635a02","value":"â€‡48.0/48.0â€‡[00:00&lt;00:00,â€‡2.58kB/s]"}},"aeed9a1d264f4f38a61e97e46e49d829":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba38ae15268649b680e963be8a8dab36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41f3b328891d43baaac58f2341ced50d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd4ad52979d7438e9a60958933da4016":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f75a95481e0d4bafaaca14fa6a88e597":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7c2a40d646284c7ab37491ee06e96cf5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84e4fa1d5f8646a69a4be14197635a02":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d993677081ee48139b5705697857f314":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a5fcb79d6b6449f6af078209fc7107cc","IPY_MODEL_726ae663b7224c1da3e15ab5bd1d0a0d","IPY_MODEL_562ebcbeed9d4ca19a21df859e20032e"],"layout":"IPY_MODEL_1ce17425296344e086d5850698eb3d75"}},"a5fcb79d6b6449f6af078209fc7107cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1e562b0a9b24d309b5fadb1af638990","placeholder":"â€‹","style":"IPY_MODEL_a251376007234a3191a5af9e1c68fe83","value":"vocab.txt:â€‡100%"}},"726ae663b7224c1da3e15ab5bd1d0a0d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_adc7662a540642bbb705c40e2dffe146","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2df217cce6ee408e9179ed5860e08b6c","value":231508}},"562ebcbeed9d4ca19a21df859e20032e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc26c81db34046bd8c087738d525bedf","placeholder":"â€‹","style":"IPY_MODEL_1fa2d6bb53904b0d8c53ea3d5b2b0868","value":"â€‡232k/232kâ€‡[00:00&lt;00:00,â€‡1.41MB/s]"}},"1ce17425296344e086d5850698eb3d75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1e562b0a9b24d309b5fadb1af638990":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a251376007234a3191a5af9e1c68fe83":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"adc7662a540642bbb705c40e2dffe146":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2df217cce6ee408e9179ed5860e08b6c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cc26c81db34046bd8c087738d525bedf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fa2d6bb53904b0d8c53ea3d5b2b0868":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3af3f3dc31940078c6a1068febd9d8b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a3955b91f7474cafb5c06953f8029d1a","IPY_MODEL_0c36b13e1b08401c9ffc9782ab191a6b","IPY_MODEL_8c85c4336fda42e5a4e11a8f28b51d9c"],"layout":"IPY_MODEL_0c991046263a4336a3c39e9c2ba4d252"}},"a3955b91f7474cafb5c06953f8029d1a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b31359ba7af14983bb741f2156f4703e","placeholder":"â€‹","style":"IPY_MODEL_4f313e59d21d4c598a425d5700f5f3d4","value":"tokenizer.json:â€‡100%"}},"0c36b13e1b08401c9ffc9782ab191a6b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_79ef5e0412bd48e1988ef98519c7e716","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3eb4b26137584fe3b1d92d5ab204034b","value":466062}},"8c85c4336fda42e5a4e11a8f28b51d9c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_918ea117c1ee446c97dce16bd594a726","placeholder":"â€‹","style":"IPY_MODEL_cfae38e6de304312b5b6f03c307fc10e","value":"â€‡466k/466kâ€‡[00:00&lt;00:00,â€‡1.90MB/s]"}},"0c991046263a4336a3c39e9c2ba4d252":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b31359ba7af14983bb741f2156f4703e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f313e59d21d4c598a425d5700f5f3d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79ef5e0412bd48e1988ef98519c7e716":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3eb4b26137584fe3b1d92d5ab204034b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"918ea117c1ee446c97dce16bd594a726":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfae38e6de304312b5b6f03c307fc10e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d671e51e7e94d7b873e958b58638206":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_040fb8fc9b4a45b38a8d5b00d04170bb","IPY_MODEL_5c0fdf39bc294a85a5750dc114921c77","IPY_MODEL_9f66956c169c47ccad8098dc86b69754"],"layout":"IPY_MODEL_1f67f6671c3747f7b655ff79d3611aa4"}},"040fb8fc9b4a45b38a8d5b00d04170bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a5ed7d418c0459b83c6d6f5b211a305","placeholder":"â€‹","style":"IPY_MODEL_f1d20158d5924902b23599e9955d825b","value":"config.json:â€‡100%"}},"5c0fdf39bc294a85a5750dc114921c77":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bdca2372dfe465ea96806bbaadda6bf","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2d7b1a83cfc7494bb9f703a2d0aeaa86","value":570}},"9f66956c169c47ccad8098dc86b69754":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c000fc9b17d40cbad3881d9ff42760f","placeholder":"â€‹","style":"IPY_MODEL_0b6ad73c6b1f48cfa2992f2747c8042e","value":"â€‡570/570â€‡[00:00&lt;00:00,â€‡31.3kB/s]"}},"1f67f6671c3747f7b655ff79d3611aa4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a5ed7d418c0459b83c6d6f5b211a305":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1d20158d5924902b23599e9955d825b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4bdca2372dfe465ea96806bbaadda6bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d7b1a83cfc7494bb9f703a2d0aeaa86":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3c000fc9b17d40cbad3881d9ff42760f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b6ad73c6b1f48cfa2992f2747c8042e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f1ab3ed6e8f4fb6922e1c82bd49f66e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_18786cbb77c04dfd90cdbb979ec1fd83","IPY_MODEL_f4e3baa463dc4e70b4f8b9075bc15b83","IPY_MODEL_a250546ff8a543cc8cd1d12b42bdcb53"],"layout":"IPY_MODEL_accc7d38b96749dc822ec1e51a45e46d"}},"18786cbb77c04dfd90cdbb979ec1fd83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52f3f0de2404412da34a096791b705df","placeholder":"â€‹","style":"IPY_MODEL_4131ee416f0b40d6a3d1f7419a949a16","value":"tokenizer_config.json:â€‡100%"}},"f4e3baa463dc4e70b4f8b9075bc15b83":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_687b27bb69644b1496d48e33d3dbb375","max":49,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d12b98218d784d88bba67960a604fe5c","value":49}},"a250546ff8a543cc8cd1d12b42bdcb53":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e31062399a124a119678614a61956c3d","placeholder":"â€‹","style":"IPY_MODEL_cd21eab2ddd34b979e22c6a534f9cb97","value":"â€‡49.0/49.0â€‡[00:00&lt;00:00,â€‡1.91kB/s]"}},"accc7d38b96749dc822ec1e51a45e46d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52f3f0de2404412da34a096791b705df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4131ee416f0b40d6a3d1f7419a949a16":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"687b27bb69644b1496d48e33d3dbb375":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d12b98218d784d88bba67960a604fe5c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e31062399a124a119678614a61956c3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd21eab2ddd34b979e22c6a534f9cb97":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"629b37c990864ccf8b3bfbfe4f525ce7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_988f0f4a99604a388d9ebc85e757feb0","IPY_MODEL_f8249fedd97b486db725141eeff17808","IPY_MODEL_cd5bf8b610314034bdc827448c41c403"],"layout":"IPY_MODEL_74d138c833174a83a10655ede859af23"}},"988f0f4a99604a388d9ebc85e757feb0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21d3be0da7444c64936083b672646fe5","placeholder":"â€‹","style":"IPY_MODEL_345abc78fde14fabb0b784994cf3d5b5","value":"vocab.txt:â€‡100%"}},"f8249fedd97b486db725141eeff17808":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_381499427d3946d5b6271cca17a40d85","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cb284ab8d3f64751a832b8a1adb86842","value":213450}},"cd5bf8b610314034bdc827448c41c403":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1aea6d504ddc434a93da75473a0ddee9","placeholder":"â€‹","style":"IPY_MODEL_c3a3ab4c27684aaa86941c31432e5fd5","value":"â€‡213k/213kâ€‡[00:00&lt;00:00,â€‡1.31MB/s]"}},"74d138c833174a83a10655ede859af23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21d3be0da7444c64936083b672646fe5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"345abc78fde14fabb0b784994cf3d5b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"381499427d3946d5b6271cca17a40d85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb284ab8d3f64751a832b8a1adb86842":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1aea6d504ddc434a93da75473a0ddee9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3a3ab4c27684aaa86941c31432e5fd5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9753ce3536534661b2798caff147a825":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_00d288c13144471190928dcd1457e4a6","IPY_MODEL_6192352d30724f148625af1634960391","IPY_MODEL_b14b4b142d4c4ee1b022cf6a1dae90c2"],"layout":"IPY_MODEL_6fa724dd43c0463681f7e596f31e4762"}},"00d288c13144471190928dcd1457e4a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86574ea37ffc4f4ca17251bc835b5ed7","placeholder":"â€‹","style":"IPY_MODEL_b431d4eca66e47af84b8e3d0067f565c","value":"tokenizer.json:â€‡100%"}},"6192352d30724f148625af1634960391":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_25244c63ebfd4e3294f9e0bc795997b8","max":435797,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8f7c34a6e96e4567a76f3e65bbb5c20d","value":435797}},"b14b4b142d4c4ee1b022cf6a1dae90c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_72c965b0a1d2402fb975019ca3d19e05","placeholder":"â€‹","style":"IPY_MODEL_57a3370ad8a74294aa85aa1a23c3da5d","value":"â€‡436k/436kâ€‡[00:00&lt;00:00,â€‡1.75MB/s]"}},"6fa724dd43c0463681f7e596f31e4762":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86574ea37ffc4f4ca17251bc835b5ed7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b431d4eca66e47af84b8e3d0067f565c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25244c63ebfd4e3294f9e0bc795997b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f7c34a6e96e4567a76f3e65bbb5c20d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"72c965b0a1d2402fb975019ca3d19e05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57a3370ad8a74294aa85aa1a23c3da5d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"16e7c4dce73b4c7a8a168da0d1df98f0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_60394f2ee55a4b8eaf13713ed862d9a7","IPY_MODEL_a15a970cbafe4d7f9215e45601d75f3f","IPY_MODEL_5282c777dec64a8ba9526601fee4a0ef"],"layout":"IPY_MODEL_8764f3848be64b7d8673f7ee8bec6db6"}},"60394f2ee55a4b8eaf13713ed862d9a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3086448b5bb241b7954a0ae502bd349c","placeholder":"â€‹","style":"IPY_MODEL_128f31d4f90e46278062b583066fee62","value":"config.json:â€‡100%"}},"a15a970cbafe4d7f9215e45601d75f3f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aaa4c18c23304b898b547997f4af280b","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9b8b936fc23141d58d3ec5d5ac785621","value":570}},"5282c777dec64a8ba9526601fee4a0ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55a430d7d16a40e5977f9f2ce1fadf2e","placeholder":"â€‹","style":"IPY_MODEL_f03e333c04d340c0a478271857c233ce","value":"â€‡570/570â€‡[00:00&lt;00:00,â€‡37.7kB/s]"}},"8764f3848be64b7d8673f7ee8bec6db6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3086448b5bb241b7954a0ae502bd349c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"128f31d4f90e46278062b583066fee62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aaa4c18c23304b898b547997f4af280b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b8b936fc23141d58d3ec5d5ac785621":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"55a430d7d16a40e5977f9f2ce1fadf2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f03e333c04d340c0a478271857c233ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af87bc0d27a94cfebb30ff85d25c5c23":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0d70707d2b9840989916f60c08a8fb4d","IPY_MODEL_78009e2fe6a54d34877b7aad8eec5b7e","IPY_MODEL_d71ef10644904600bef97571e2abf391"],"layout":"IPY_MODEL_729e77353f814a67b778558458ed5b0e"}},"0d70707d2b9840989916f60c08a8fb4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_47ff5d014bd24b68b9d642c074831efa","placeholder":"â€‹","style":"IPY_MODEL_1c0bcf7b9be3434aadabd6011139330e","value":"model.safetensors:â€‡100%"}},"78009e2fe6a54d34877b7aad8eec5b7e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8f05782a0d14730b0ffccc6d7b9b10c","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7709420a948741faa92e7ce4e0364897","value":440449768}},"d71ef10644904600bef97571e2abf391":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16124dfb5aaf4170917b8f33710807c8","placeholder":"â€‹","style":"IPY_MODEL_1066fa77715e440498cc137a48f23345","value":"â€‡440M/440Mâ€‡[00:01&lt;00:00,â€‡266MB/s]"}},"729e77353f814a67b778558458ed5b0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47ff5d014bd24b68b9d642c074831efa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c0bcf7b9be3434aadabd6011139330e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a8f05782a0d14730b0ffccc6d7b9b10c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7709420a948741faa92e7ce4e0364897":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"16124dfb5aaf4170917b8f33710807c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1066fa77715e440498cc137a48f23345":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a774620dca634d19a69b51481ca1f5d1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_906f316162a1458397cf64128f413787","IPY_MODEL_075f94a71d024a0886a6d06c9c504414","IPY_MODEL_e7aa079e44964f4daf6ab7b95a05fac7"],"layout":"IPY_MODEL_8a953ee83566478488c27f189d2d7e3a"}},"906f316162a1458397cf64128f413787":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b19cb538f6a45d090abd16c81f3ad59","placeholder":"â€‹","style":"IPY_MODEL_ca2ae6b2ff2e49b7b7a95aab0ccb1cdc","value":"100%"}},"075f94a71d024a0886a6d06c9c504414":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3e478cddabd405ab6f4c42adcadf6b9","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_696ca5f5c54b407ea988bcd283e5c358","value":25}},"e7aa079e44964f4daf6ab7b95a05fac7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91d28d996eb84f09966b59f8f08de998","placeholder":"â€‹","style":"IPY_MODEL_9b506ac3afcb46c48dbf237bdcaaf10c","value":"â€‡25/25â€‡[00:04&lt;00:00,â€‡â€‡5.24it/s]"}},"8a953ee83566478488c27f189d2d7e3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b19cb538f6a45d090abd16c81f3ad59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca2ae6b2ff2e49b7b7a95aab0ccb1cdc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3e478cddabd405ab6f4c42adcadf6b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"696ca5f5c54b407ea988bcd283e5c358":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"91d28d996eb84f09966b59f8f08de998":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b506ac3afcb46c48dbf237bdcaaf10c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db5f00a7601a4953974a8949e96eda26":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3097ec1ba7a6494e8ab30035db06d997","IPY_MODEL_af9584a98dcf4c94807acd1940866797","IPY_MODEL_c9dd4a5bccad412a838025fad3868975"],"layout":"IPY_MODEL_ffd266da58984e4793e4c79842eb2b2a"}},"3097ec1ba7a6494e8ab30035db06d997":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff2a0dc879d4489bada2f82b662a8eec","placeholder":"â€‹","style":"IPY_MODEL_f6f36cf0c46d46bbb2a151ac804ded5a","value":"100%"}},"af9584a98dcf4c94807acd1940866797":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5cff852273d4427993fa8f9f929ef88","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_19c510ba3fbc486f8d8aca78eb08bf89","value":25}},"c9dd4a5bccad412a838025fad3868975":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7458af6fcae462d84b6ccb4e9fab78d","placeholder":"â€‹","style":"IPY_MODEL_88f89af6b15a473d817338a1fc6eb807","value":"â€‡25/25â€‡[00:03&lt;00:00,â€‡â€‡6.47it/s]"}},"ffd266da58984e4793e4c79842eb2b2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff2a0dc879d4489bada2f82b662a8eec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6f36cf0c46d46bbb2a151ac804ded5a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5cff852273d4427993fa8f9f929ef88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19c510ba3fbc486f8d8aca78eb08bf89":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c7458af6fcae462d84b6ccb4e9fab78d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88f89af6b15a473d817338a1fc6eb807":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1cff078c7aba411bb007a81a5fde1b59":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_62f80dc6369f42db870e55421484e34b","IPY_MODEL_e94ee3fcd82b4c6fb0e1720d81061fed","IPY_MODEL_f4aa07ae03144ad0b328cbd098be6776"],"layout":"IPY_MODEL_b8cbc1ae7dd34597a33fa39fd29fbe88"}},"62f80dc6369f42db870e55421484e34b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_97a234cae7274c86980ea428cd7a158c","placeholder":"â€‹","style":"IPY_MODEL_09714dfa9e9d44dc8817b01fa41a61a8","value":"100%"}},"e94ee3fcd82b4c6fb0e1720d81061fed":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a91335f4107545238a7390b6cfbe2717","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_06ab9eabc8424f2e820f8da4e3f5d25c","value":25}},"f4aa07ae03144ad0b328cbd098be6776":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6b1604fb6d1413fbe5ba32d1b281090","placeholder":"â€‹","style":"IPY_MODEL_d2eb22cd61b74978b6ae1ff89ab04634","value":"â€‡25/25â€‡[00:03&lt;00:00,â€‡â€‡6.25it/s]"}},"b8cbc1ae7dd34597a33fa39fd29fbe88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97a234cae7274c86980ea428cd7a158c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09714dfa9e9d44dc8817b01fa41a61a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a91335f4107545238a7390b6cfbe2717":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06ab9eabc8424f2e820f8da4e3f5d25c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6b1604fb6d1413fbe5ba32d1b281090":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2eb22cd61b74978b6ae1ff89ab04634":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c5820d085ce48de80b44fa5a96053b5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5161cbcea2d74553adc25d8a580f8293","IPY_MODEL_b61424e6be3c412dbe800681b9700039","IPY_MODEL_21683c67b888404ca1b38e77b3f46fff"],"layout":"IPY_MODEL_b870724440f64f2fb856cf4b931494ec"}},"5161cbcea2d74553adc25d8a580f8293":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d756555859f4906a2fbf666da481659","placeholder":"â€‹","style":"IPY_MODEL_536e7d4ed02b48dbb716e03e70cf063d","value":"100%"}},"b61424e6be3c412dbe800681b9700039":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e3f0dfa5a5b4ee5b90ca908a22d525b","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e09f8ae48e66481391f7718e70d067f4","value":25}},"21683c67b888404ca1b38e77b3f46fff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_006dcb2712a441c69ed9408bed8ba338","placeholder":"â€‹","style":"IPY_MODEL_d05f885d581441afaf01434adc01a332","value":"â€‡25/25â€‡[00:04&lt;00:00,â€‡â€‡6.41it/s]"}},"b870724440f64f2fb856cf4b931494ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d756555859f4906a2fbf666da481659":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"536e7d4ed02b48dbb716e03e70cf063d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e3f0dfa5a5b4ee5b90ca908a22d525b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e09f8ae48e66481391f7718e70d067f4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"006dcb2712a441c69ed9408bed8ba338":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d05f885d581441afaf01434adc01a332":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"575ee175448141559b5a25c80996eebb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7f3d9408c3bd41239d5e4e4df4bf9ab1","IPY_MODEL_e9c1d08571d5423caaca051a09d3383a","IPY_MODEL_dc026f03e2b84d388ff1b7a024d5e23a"],"layout":"IPY_MODEL_834b3b220b634191b1ec46a8c8c9d69f"}},"7f3d9408c3bd41239d5e4e4df4bf9ab1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b63c2d29144c4d5187a3ac4fc61addaf","placeholder":"â€‹","style":"IPY_MODEL_bc8cb692013848528046db0fcc7a33fa","value":"100%"}},"e9c1d08571d5423caaca051a09d3383a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_26f6b849b019415a83c6777b3970b22f","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8816af9fe3e148758d8c77308b6bd31b","value":25}},"dc026f03e2b84d388ff1b7a024d5e23a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30706676dc554d5bb5e2009225780bed","placeholder":"â€‹","style":"IPY_MODEL_534453d0249a4e468b91de711eef372e","value":"â€‡25/25â€‡[00:03&lt;00:00,â€‡â€‡6.52it/s]"}},"834b3b220b634191b1ec46a8c8c9d69f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b63c2d29144c4d5187a3ac4fc61addaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc8cb692013848528046db0fcc7a33fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"26f6b849b019415a83c6777b3970b22f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8816af9fe3e148758d8c77308b6bd31b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"30706676dc554d5bb5e2009225780bed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"534453d0249a4e468b91de711eef372e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c9d8c4d82c34dabae13dda21cbeb984":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b539c503cb5a4072bee3b118e2e3ba7d","IPY_MODEL_dc6626a53e034811b808969762fc6672","IPY_MODEL_4d18fc7795964e70b773906623248524"],"layout":"IPY_MODEL_11079d39f5d4487fabaa9e92f546111b"}},"b539c503cb5a4072bee3b118e2e3ba7d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_81f9e51455af470d9819b43d23319b6e","placeholder":"â€‹","style":"IPY_MODEL_64bb15e63bd84b1faad2c2c0c6e4f3e3","value":"100%"}},"dc6626a53e034811b808969762fc6672":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fd2aa9759694b9d84a9f98a50009b41","max":2089,"min":0,"orientation":"horizontal","style":"IPY_MODEL_00a08695fd8c480ea0ae95209150bfdd","value":2089}},"4d18fc7795964e70b773906623248524":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10aafbef473847d29be30fac9e77e3aa","placeholder":"â€‹","style":"IPY_MODEL_34b7ad4e75474b94ad29ffadf0c982ed","value":"â€‡2089/2089â€‡[13:43&lt;00:00,â€‡â€‡3.28it/s]"}},"11079d39f5d4487fabaa9e92f546111b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81f9e51455af470d9819b43d23319b6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64bb15e63bd84b1faad2c2c0c6e4f3e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4fd2aa9759694b9d84a9f98a50009b41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00a08695fd8c480ea0ae95209150bfdd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"10aafbef473847d29be30fac9e77e3aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34b7ad4e75474b94ad29ffadf0c982ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e6367fa8c41439eb9551596d01bc892":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4e7f73f06e7c49938c3bda1e896867a6","IPY_MODEL_3eca24357b474435b6efa10c2b02bd46","IPY_MODEL_dbc0e25adadf457fad3012e5ab90c675"],"layout":"IPY_MODEL_33c9b06b865647f98edf5bd4f2771623"}},"4e7f73f06e7c49938c3bda1e896867a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2949966b3722460882024dcf3a533f6e","placeholder":"â€‹","style":"IPY_MODEL_125d9cb151654b2285c431ce3b798ca1","value":"100%"}},"3eca24357b474435b6efa10c2b02bd46":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e807121b1b3454cb7804e21b67bc565","max":2089,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ed2f235080bd4426af1ecf888ebf0d79","value":2089}},"dbc0e25adadf457fad3012e5ab90c675":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_946ba990bbbd49e8aa07c41b08d21ff8","placeholder":"â€‹","style":"IPY_MODEL_6694a6817258484cbb6e00cbe8a66cf6","value":"â€‡2089/2089â€‡[13:45&lt;00:00,â€‡â€‡2.95it/s]"}},"33c9b06b865647f98edf5bd4f2771623":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2949966b3722460882024dcf3a533f6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"125d9cb151654b2285c431ce3b798ca1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e807121b1b3454cb7804e21b67bc565":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed2f235080bd4426af1ecf888ebf0d79":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"946ba990bbbd49e8aa07c41b08d21ff8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6694a6817258484cbb6e00cbe8a66cf6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6185b7ac55bf4e25bed994e3eeb1e9bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_af92bc8ed41c4d6aa1b3af9524433d7b","IPY_MODEL_9f16149328cf4dfaabbb64144031f09b","IPY_MODEL_1ed2d84558624e5aa67fa8b3f761b3b7"],"layout":"IPY_MODEL_a467c1f4b4c14532a010330131fb0732"}},"af92bc8ed41c4d6aa1b3af9524433d7b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf7d7a8058cd44219525cf94e088eb9e","placeholder":"â€‹","style":"IPY_MODEL_60a9c93dad9c4df3bd60258114c27173","value":"â€‡â€‡0%"}},"9f16149328cf4dfaabbb64144031f09b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_96378a18cc2d494dafe50a04af966462","max":1954,"min":0,"orientation":"horizontal","style":"IPY_MODEL_27ae85e2bb8c4c0ea94825d8837a699a","value":0}},"1ed2d84558624e5aa67fa8b3f761b3b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ec3d21dd9ed4730994a3c64df9054cf","placeholder":"â€‹","style":"IPY_MODEL_3bfdd2fb866f4d7fa35da617eca495c1","value":"â€‡0/1954â€‡[00:00&lt;?,â€‡?it/s]"}},"a467c1f4b4c14532a010330131fb0732":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf7d7a8058cd44219525cf94e088eb9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60a9c93dad9c4df3bd60258114c27173":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96378a18cc2d494dafe50a04af966462":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27ae85e2bb8c4c0ea94825d8837a699a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4ec3d21dd9ed4730994a3c64df9054cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bfdd2fb866f4d7fa35da617eca495c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}