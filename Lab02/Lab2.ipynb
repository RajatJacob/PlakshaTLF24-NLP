{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "19c247ef",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "19c247ef",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a55d3ed06eee4d3a7da8b7d095623f91",
          "grade": false,
          "grade_id": "cell-23a7f8d3de35de0b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Lab 2: Fine-tuning BERT To Perform Common Sense Reasoning\n",
        "\n",
        "## May 13, 2024\n",
        "\n",
        "Welcome to the Lab 2 of our course on Natural Language Processing. As the name suggests in this lab you will learn how to fine-tune a pretrained model like BERT on a downstream task to improve much more superior performance compared to the methods discussed so far. We will be working with the [SocialIQA](https://arxiv.org/abs/1904.09728) dataset this week, which is a multiple choice classification dataset designed to learn and measure social and emotional intelligence in NLP models.\n",
        "\n",
        "\n",
        "This assignment will also make heavy use of the [ðŸ¤— Transformers Library](https://huggingface.co/docs/transformers/index). Don't worry if you are not familiar with the library, we will discuss its usage in detail.\n",
        "\n",
        "Note: Access to a GPU will be crucial for working on this assignment. So do select a GPU runtime in Colab before you start working.\n",
        "\n",
        "Learning Outcomes from this Lab:\n",
        "- Learn how to use ðŸ¤— Transformer library to load and fine-tune pre-trained langauge models\n",
        "- Learn how to solve common sense reasoning problems using Masked Language Models like BERT\n",
        "\n",
        "Suggested Reading:\n",
        "- [Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*](https://arxiv.org/pdf/1810.04805.pdf)\n",
        "- [Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social IQa: Commonsense Reasoning about Social Interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4463â€“4473, Hong Kong, China. Association for Computational Linguistics.] (https://arxiv.org/pdf/1810.04805.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "26c077e7",
      "metadata": {
        "id": "26c077e7"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "siqa_data_dir = \"./data/socialiqa-train-dev/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3242992d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "3242992d",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9b746d7065e47851c69e7bd3f33db670",
          "grade": false,
          "grade_id": "cell-00e4313e5ec6522c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (1.26.4)\n",
            "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: pandas in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: torch in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (2.3.0)\n",
            "Requirement already satisfied: filelock in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: tqdm in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (4.66.4)\n",
            "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: matplotlib in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (3.8.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.21 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=8 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from matplotlib) (10.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: transformers in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from transformers) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from transformers) (2024.5.10)\n",
            "Requirement already satisfied: requests in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
            "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: scikit-learn in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from scikit-learn) (1.12.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
            "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: tqdm in /Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages (4.66.4)\n",
            "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# If using Colab, NO NEED TO INSTALL ANYTHING\n",
        "# Install required libraries\n",
        "%pip install numpy\n",
        "%pip install pandas\n",
        "%pip install torch\n",
        "%pip install tqdm\n",
        "%pip install matplotlib\n",
        "%pip install transformers\n",
        "%pip install scikit-learn\n",
        "%pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9e7b0e64",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "9e7b0e64",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6f6521e81b123c3d00a56e3fca57d658",
          "grade": false,
          "grade_id": "cell-eb2f90d7f62cad18",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# We start by importing libraries that we will be making use of in the assignment.\n",
        "import os\n",
        "from functools import partial\n",
        "import json\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import copy\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from transformers.utils import logging\n",
        "logging.set_verbosity(40) # to avoid warnings from transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c04bc11",
      "metadata": {
        "id": "3c04bc11"
      },
      "source": [
        "## SocialIQA Dataset\n",
        "\n",
        "We start by discussing the dataset that we will making use of in today's Lab. As described above SocialIQA was designed to learn and measure social and emotional intelligence in NLP models. It is a multiple choice classification task, where you are given a context of some social situation, a question about the context and then three possible answers to the questions. The task is to predict which of the three options answers the question given the context.\n",
        "\n",
        "![siqa dataset](https://i.ibb.co/s5tMpY8/siqa.png)\n",
        "\n",
        "Below we load the dataset in memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b3f82e9d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b3f82e9d",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5d3586a8ea681ba874073f19efd8d10d",
          "grade": false,
          "grade_id": "cell-61ae8095088b1abc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Training Examples: 33410\n",
            "Number of Validation Examples: 1954\n"
          ]
        }
      ],
      "source": [
        "def load_siqa_data(split):\n",
        "\n",
        "    # We first load the file containing context, question and answers\n",
        "    with open(f\"data/socialiqa-train-dev/{split}.jsonl\") as f:\n",
        "        data = [json.loads(jline) for jline in f.read().splitlines()]\n",
        "\n",
        "    # We then load the file containing the correct answer for each question\n",
        "    with open(f\"data/socialiqa-train-dev/{split}-labels.lst\") as f:\n",
        "        labels = f.read().splitlines()\n",
        "\n",
        "    labels_dict = {\"1\": \"A\", \"2\": \"B\", \"3\": \"C\"}\n",
        "    labels = [labels_dict[label] for label in labels]\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "train_data, train_labels = load_siqa_data(\"train\")\n",
        "dev_data, dev_labels = load_siqa_data(\"dev\")\n",
        "\n",
        "print(f\"Number of Training Examples: {len(train_data)}\")\n",
        "print(f\"Number of Validation Examples: {len(dev_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3dc04307",
      "metadata": {
        "id": "3dc04307"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: ylabel='count'>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sns.countplot(x = train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "adf24a61",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "adf24a61",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "70e3ecb51abfd2ca15b0b8f5f24de0e7",
          "grade": false,
          "grade_id": "cell-602fe165abcf4503",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example from dataset\n",
            "{   'context': \"Jordan's dog peed on the couch they were selling and Jordan \"\n",
            "               'removed the odor as soon as possible.',\n",
            "    'question': 'How would Jordan feel afterwards?',\n",
            "    'answerA': 'selling a couch',\n",
            "    'answerB': 'Disgusted',\n",
            "    'answerC': 'Relieved'}\n",
            "Label: B\n"
          ]
        }
      ],
      "source": [
        "# View a sample of the dataset\n",
        "print(\"Example from dataset\")\n",
        "pprint(train_data[100], sort_dicts=False, indent=4)\n",
        "print(f\"Label: {train_labels[100]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "271a12e6",
      "metadata": {
        "id": "271a12e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': 'kendall was a person who kept her word so she got my money the other day.',\n",
              " 'question': 'What will Others want to do next?',\n",
              " 'answerA': 'resent kendall',\n",
              " 'answerB': 'support kendall',\n",
              " 'answerC': 'hate kendall'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[500]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99a6b904",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "99a6b904",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e77cbee4bcad7c097c84525407a83d89",
          "grade": false,
          "grade_id": "cell-23562a8e437add81",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## Task 1: Tokenization and Data Preperation (1 hour)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0af7f506",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "0af7f506",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9634252b2664ab163c234b45f7cac77f",
          "grade": false,
          "grade_id": "cell-ee0e1d7bb6a346a4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "As discussed in the lectures, BERT and other pretrained language models use sub-word tokenization i.e. individual words can also be split into constituent subwords to reduce the vocabulary size. The Transformer library provides tokenizer for all the popular language models. Below we demonstrate how to create and use these tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4e8aa458",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "4e8aa458",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1791efdd95ba6e41368fcf3d0e742e0b",
          "grade": false,
          "grade_id": "cell-cf4d278c7b6855d7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Import the BertTokenizer from the library\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load a pre-trained BERT Tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30724776",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "30724776",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ec587bee5c51d19bfe0135af7f8514b4",
          "grade": false,
          "grade_id": "cell-e5d5e6b75df21d75",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "`BertTokenizer.from_pretrained` is used to load a pre-trained tokenizer. Notice that we provide the argument `\"bert-base-uncased\"` to the method. This refers to the variant of BERT that we want to use. The term \"base\" means we want to use the smaller BERT variant i.e. the one with 12 layers, and \"uncased\" refers to the fact that it treats upper-case and lower-case characters identically. There are 4 variants available for BERT which are:\n",
        "    - `bert-base-uncased`\n",
        "    - `bert-base-cased`\n",
        "    - `bert-large-uncased`\n",
        "    - `bert-large-cased`\n",
        "Now that we have loaded the tokenizer, let's see how to use it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13523cc5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "13523cc5",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4cce224a12854b52864b4a47c8cf6afc",
          "grade": false,
          "grade_id": "cell-64c955b108dc7281",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "`tokenize` method can be used to split the text into sequence of tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5ccbad09",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5ccbad09",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b3bbc88079b2d0531a7be74670e9edb6",
          "grade": false,
          "grade_id": "cell-d232c469d8bd9d8b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['kendall',\n",
              " 'was',\n",
              " 'a',\n",
              " 'person',\n",
              " 'who',\n",
              " 'kept',\n",
              " 'her',\n",
              " 'word',\n",
              " 'exquisite',\n",
              " '##ly',\n",
              " ',',\n",
              " 'so',\n",
              " 'she',\n",
              " 'got',\n",
              " 'my',\n",
              " 'money',\n",
              " 'the',\n",
              " 'other',\n",
              " 'day']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert_tokenizer.tokenize(\"kendall was a person who kept her word exquisitely, so she got my money the other day\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daced828",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "daced828",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0b2c517ab6b6cf3d2e479b89ce221764",
          "grade": false,
          "grade_id": "cell-20733e8cd4cc9db8",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Notice how the tokenizer not only splits the text into words but also subwords like \"exquisitely\" is split into \"exquisite\" and \"ly\"."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7260263",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "d7260263",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e24402cff8fc0e8a3dcfda8fe8eead78",
          "grade": false,
          "grade_id": "cell-c2120e5f1d50ce3e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Another use case of the tokenizer is to convert the tokens into indices. This is important because BERT and almost all language models takes as the inputs a sequence of token ids, which they use to map into embeddings. `convert_tokens_to_ids` method can be used to do this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "dbee421b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "dbee421b",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7d55893b443134e6f92a1f6620c0f751",
          "grade": false,
          "grade_id": "cell-3a05f425316cea54",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[14509, 2001, 1037, 2711, 2040, 2921, 2014, 2773, 19401, 2135, 1010, 2061, 2016, 2288, 2026, 2769, 1996, 2060, 2154]\n"
          ]
        }
      ],
      "source": [
        "sentence = \"kendall was a person who kept her word exquisitely, so she got my money the other day\"\n",
        "tokens = bert_tokenizer.tokenize(sentence)\n",
        "token_ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efd54a05",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "efd54a05",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1e4e23bc6201fae04c7d4fd982288e98",
          "grade": false,
          "grade_id": "cell-4ee63c56c4ce3e10",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "The two steps can also be combined by simply calling the tokenizer object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bfcbb696",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "bfcbb696",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f1887b57da3c78d52ecf03ce6e405a29",
          "grade": false,
          "grade_id": "cell-0e587013451e87df",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{   'input_ids': [   101,\n",
            "                     14509,\n",
            "                     2001,\n",
            "                     1037,\n",
            "                     2711,\n",
            "                     2040,\n",
            "                     2921,\n",
            "                     2014,\n",
            "                     2773,\n",
            "                     19401,\n",
            "                     2135,\n",
            "                     1010,\n",
            "                     2061,\n",
            "                     2016,\n",
            "                     2288,\n",
            "                     2026,\n",
            "                     2769,\n",
            "                     1996,\n",
            "                     2060,\n",
            "                     2154,\n",
            "                     102],\n",
            "    'token_type_ids': [   0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0],\n",
            "    'attention_mask': [   1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1]}\n"
          ]
        }
      ],
      "source": [
        "pprint(bert_tokenizer(sentence), sort_dicts=False, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ab48ddf",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2ab48ddf",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7b1a833f0f503757099493ad4e6bad0f",
          "grade": false,
          "grade_id": "cell-19c0f71b49a7786a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Notice that it returns a bunch of things in addition to the ids. The `\"input_ids\"` are just the token ids that we obtained in the previous cell. However you will notice that it has a few additional ids, it starts with 101 and ends with 102. These are what we call special tokens and correspond the \\[CLS\\] and \\[SEP\\] tokens used by BERT. \\[CLS\\] token is mainly added to beginning of each sequence, and its representations are used to perform sequence classification. More on \\[SEP\\] token later.\n",
        "\n",
        "`\"token_type_ids\"` contains which sequence does a particular token belongs to.\n",
        "\n",
        "`\"attention_mask`\" is a mask vector that indicates if a particular token corresponds to padding. Padding is extremely important when we are dealing with variable length sequences, which is almost always the case. Through padding we can ensure that all the sequences in a batch are of same size. However, while processing the sequence we need ignore these padding tokens, hence a mask is required to identify such tokens.\n",
        "\n",
        "We can tokenize a batch of sequences by just providing a list instead of a string while calling the tokenizer and later pad them using the `.pad` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1d2c0925",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "1d2c0925",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d4a48370450acebae8f9a4b7953eff16",
          "grade": false,
          "grade_id": "cell-14fe0d9b9608e600",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Ids shape: torch.Size([4, 23])\n",
            "Attention Mask shape: torch.Size([4, 23])\n",
            "('Input Ids:\\n'\n",
            " ' tensor([[  101,  7232,  2787,  2000,  2031,  1037, 26375,  1998,  5935,  '\n",
            " '2014,\\n'\n",
            " '          2814,  2362,  1012,   102,     0,     0,     0,     0,     0,     '\n",
            " '0,\\n'\n",
            " '             0,     0,     0],\\n'\n",
            " '        [  101,  5553,  2734,  2000,  2507,  2041,  5841,  2005,  2019,  '\n",
            " '9046,\\n'\n",
            " '          2622,  2012,  2147,  1012,   102,     0,     0,     0,     0,     '\n",
            " '0,\\n'\n",
            " '             0,     0,     0],\\n'\n",
            " '        [  101, 22712,  2001,  2019,  6739, 19949,  1998,  2001,  2006,  '\n",
            " '1996,\\n'\n",
            " '          2300,  2007, 11928,  1012, 22712, 17395,  2098, 11928,  1005,  '\n",
            " '1055,\\n'\n",
            " '          8103,  1012,   102],\\n'\n",
            " '        [  101, 18403,  2435,  1037,  8549,  2000, 27970,  1005,  1055,  '\n",
            " '2365,\\n'\n",
            " '          2043,  2027,  2020,  3110,  2091,  1012,   102,     0,     0,     '\n",
            " '0,\\n'\n",
            " '             0,     0,     0]])\\n')\n",
            "('Attention Mask:\\n'\n",
            " ' tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, '\n",
            " '0],\\n'\n",
            " '        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, '\n",
            " '0],\\n'\n",
            " '        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, '\n",
            " '1],\\n'\n",
            " '        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, '\n",
            " '0]])\\n')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "batch_size = 4\n",
        "sentence_batch = [train_data[i][\"context\"] for i in range(batch_size)]\n",
        "\n",
        "#Tokenize the batch of sequences\n",
        "tokenized_batch = bert_tokenizer(sentence_batch)\n",
        "\n",
        "# Pad the tokenized batch\n",
        "tokenized_batch_padded = bert_tokenizer.pad(tokenized_batch, padding=True, max_length=32, return_tensors=\"pt\")\n",
        "\n",
        "input_ids = tokenized_batch_padded[\"input_ids\"]\n",
        "attn_mask = tokenized_batch_padded[\"attention_mask\"]\n",
        "print(f\"Input Ids shape: {input_ids.shape}\")\n",
        "print(f\"Attention Mask shape: {attn_mask.shape}\")\n",
        "\n",
        "pprint(f\"Input Ids:\\n {input_ids}\\n\")\n",
        "pprint(f\"Attention Mask:\\n {attn_mask}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc12c5d0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fc12c5d0",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "fce3fa2c6deedb93b575d9cb4e28d20f",
          "grade": false,
          "grade_id": "cell-c42c7d946429adc4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Notice how 0s get appended to the input ids sequence, and the same is also reflected in the output of `attn_mask` where `0` indicates that the particular token was padded and `1` means otherwise. Setting `return_tensors=\"pt\"` results in the outputs as torch tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4e65595",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "c4e65595",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "db13d05cd5920e98eeaf5bb761b1075c",
          "grade": false,
          "grade_id": "cell-673d0a006e2f027a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Finally, for tasks involving reasoning over multiple sentences (like what we have for the SocialIQA dataset), it is common to seperate out each sentence using a \\[SEP\\] token:\n",
        "\n",
        "<img src=\"https://i.ibb.co/Nx8mK1P/bert-sentence-pair.jpg\" alt=\"bert-sentence-pair\" border=\"0\">\n",
        "\n",
        "We can achieve this by adding concatenating all sentences with the `[SEP]` token before calling the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "19f0e7bd",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "19f0e7bd",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6cd951f70f83b450715fb2af85c023fe",
          "grade": false,
          "grade_id": "cell-a7fcef5940c3feb8",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.[SEP]How would Jordan feel afterwards?[SEP]selling a couch\n",
            "{   'input_ids': [   101,\n",
            "                     5207,\n",
            "                     1005,\n",
            "                     1055,\n",
            "                     3899,\n",
            "                     21392,\n",
            "                     2094,\n",
            "                     2006,\n",
            "                     1996,\n",
            "                     6411,\n",
            "                     2027,\n",
            "                     2020,\n",
            "                     4855,\n",
            "                     1998,\n",
            "                     5207,\n",
            "                     3718,\n",
            "                     1996,\n",
            "                     19255,\n",
            "                     2004,\n",
            "                     2574,\n",
            "                     2004,\n",
            "                     2825,\n",
            "                     1012,\n",
            "                     102,\n",
            "                     2129,\n",
            "                     2052,\n",
            "                     5207,\n",
            "                     2514,\n",
            "                     5728,\n",
            "                     1029,\n",
            "                     102,\n",
            "                     4855,\n",
            "                     1037,\n",
            "                     6411,\n",
            "                     102],\n",
            "    'token_type_ids': [   0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0,\n",
            "                          0],\n",
            "    'attention_mask': [   1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1,\n",
            "                          1]}\n"
          ]
        }
      ],
      "source": [
        "example = train_data[100]\n",
        "context = example[\"context\"]\n",
        "question = example[\"question\"]\n",
        "answerA = example[\"answerA\"]\n",
        "\n",
        "# Concatenate the context, question and answerA\n",
        "cqa = context + bert_tokenizer.sep_token + question + bert_tokenizer.sep_token + answerA\n",
        "print(cqa)\n",
        "\n",
        "tokenized_cqa = bert_tokenizer(cqa)\n",
        "pprint(tokenized_cqa, sort_dicts=False, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db9af625",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "db9af625",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f52a28ddb4d75b1276a34f6979c44e06",
          "grade": false,
          "grade_id": "cell-b6df8e66edd0daed",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "For the reasons that will become clear once we work on the modeling part, we need three input tensors for each dataset example, one for concatenating each answer with the context and question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4a8bddb8",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "4a8bddb8",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "08e2c414ebaa5ef36cf53cb8bd764c54",
          "grade": false,
          "grade_id": "cell-01d931a0dfdd5054",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.[SEP]How would Jordan feel afterwards?[SEP]selling a couch\n",
            "Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.[SEP]How would Jordan feel afterwards?[SEP]Disgusted\n",
            "Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.[SEP]How would Jordan feel afterwards?[SEP]Relieved\n"
          ]
        }
      ],
      "source": [
        "example = train_data[100]\n",
        "context = example[\"context\"]\n",
        "question = example[\"question\"]\n",
        "answerA = example[\"answerA\"]\n",
        "answerB = example[\"answerB\"]\n",
        "answerC = example[\"answerC\"]\n",
        "\n",
        "cqaA = context + bert_tokenizer.sep_token + question + bert_tokenizer.sep_token + answerA\n",
        "cqaB = context + bert_tokenizer.sep_token + question + bert_tokenizer.sep_token + answerB\n",
        "cqaC = context + bert_tokenizer.sep_token + question + bert_tokenizer.sep_token + answerC\n",
        "\n",
        "print(cqaA)\n",
        "print(cqaB)\n",
        "print(cqaC)\n",
        "\n",
        "tokenized_cqaA = bert_tokenizer(cqaA)\n",
        "tokenized_cqaB = bert_tokenizer(cqaB)\n",
        "tokenized_cqaC = bert_tokenizer(cqaC)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4627768e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "4627768e",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "32623222e465ee736771be5decbe6f1c",
          "grade": false,
          "grade_id": "cell-a3cb841c571f2b47",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## Task 1.1: Custom Dataset Class\n",
        "\n",
        "Now that we know how to use the hugging face tokenizers we can define the custom `torch.utils.Dataset` class like we did in the previous assignments to process and store the data as well as provides a way to iterate through the dataset. Implement the `SIQABertDataset` class below. Recall to create a custom class you need to implement 3 methods `__init__`, `__len__` and `__getitem__`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "65ffd2ac",
      "metadata": {
        "deletable": false,
        "id": "65ffd2ac",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5b1458f97a435cb685eec31b40452871",
          "grade": false,
          "grade_id": "cell-84536c665947d263",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SIQABertDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, labels, bert_variant = \"bert-base-uncased\"):\n",
        "        \"\"\"\n",
        "        Constructor for the `SST2BertDataset` class. Stores the `sentences` and `labels` which can then be used by\n",
        "        other methods. Also initializes the tokenizer\n",
        "\n",
        "        Inputs:\n",
        "            - data (list) : A list SIQA dataset examples\n",
        "            - labels (list): A list of labels corresponding to each example\n",
        "            - bert_variant (str): A string indicating the variant of BERT to be used.\n",
        "        \"\"\"\n",
        "        self.label2label_id = {\"A\": 0, \"B\": 1, \"C\": 2}\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(bert_variant)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the length of the dataset\n",
        "        \"\"\"\n",
        "        length = len(self.data)\n",
        "\n",
        "        return length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns the training example corresponding to review present at the `idx` position in the dataset\n",
        "\n",
        "        Inputs:\n",
        "            - idx (int): Index corresponding to the review,label to be returned\n",
        "\n",
        "        Returns:\n",
        "            - tokenized_input_dict (dict(str, dict)): A dictionary corresponding to tokenizer outputs for the three resulting sequences due to each answer choices as described above\n",
        "            - label (int): Answer label for the corresponding sentence. We will use 0, 1 and 2 to represent A, B and C respectively.\n",
        "\n",
        "        Example Output:\n",
        "            - tokenized_input_dict: {\n",
        "                \"A\": {'input_ids': [101, 5207, 1005, 1055, 3899, 21392, 2094, 2006, 1996, 6411, 2027, 2020, 4855, 1998, 5207, 3718, 1996, 19255, 2004, 2574, 2004, 2825, 1012, 102, 2129, 2052, 5207, 2514, 5728, 1029, 102, 4855, 1037, 6411, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
        "                \"B\": {'input_ids': [101, 5207, 1005, 1055, 3899, 21392, 2094, 2006, 1996, 6411, 2027, 2020, 4855, 1998, 5207, 3718, 1996, 19255, 2004, 2574, 2004, 2825, 1012, 102, 2129, 2052, 5207, 2514, 5728, 1029, 102, 17733, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
        "                \"C\": {'input_ids': [101, 5207, 1005, 1055, 3899, 21392, 2094, 2006, 1996, 6411, 2027, 2020, 4855, 1998, 5207, 3718, 1996, 19255, 2004, 2574, 2004, 2825, 1012, 102, 2129, 2052, 5207, 2514, 5728, 1029, 102, 7653, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
        "            }\n",
        "            - label: 0\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        d = self.data[idx]\n",
        "        context = d['context']\n",
        "        question = d['question']\n",
        "        tokenized_input_dict = {\n",
        "            i: self.tokenizer(str(self.tokenizer.sep_token).join([context, question, d[f'answer{i}']]))\n",
        "            for i in \"ABC\"\n",
        "        }\n",
        "        label = self.label2label_id[self.labels[idx]]\n",
        "\n",
        "        return tokenized_input_dict, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7b8c8dea",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7b8c8dea",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "dafc70d40580a2a14cc89778b4a1c88d",
          "grade": true,
          "grade_id": "cell-0ff4b72642c7bacb",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Sample Test Cases\n",
            "Sample Test Case 1: Checking if `__len__` is implemented correctly\n",
            "Dataset Length: 2\n",
            "Expected Length: 2\n",
            "Sample Test Case Passed!\n",
            "****************************************\n",
            "\n",
            "Sample Test Case 2: Checking if `__getitem__` is implemented correctly for `idx= 0`\n",
            "tokenized_input_dict:\n",
            " {'A': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 7052, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 6595, 2188, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 1037, 2204, 2767, 2000, 2031, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n",
            "Expected tokenized_input_dict:\n",
            " {'A': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 7052, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 6595, 2188, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 1037, 2204, 2767, 2000, 2031, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n",
            "label:\n",
            " 0\n",
            "Expected label:\n",
            " 0\n",
            "Sample Test Case Passed!\n",
            "****************************************\n",
            "\n",
            "Sample Test Case 3: Checking if `__getitem__` is implemented correctly for `idx= 1`\n",
            "tokenized_input_dict:\n",
            " {'A': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 21090, 2007, 5553, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 2131, 2000, 2147, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 7475, 2007, 1996, 14799, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n",
            "Expected tokenized_input_dict:\n",
            " {'A': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 21090, 2007, 5553, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 2131, 2000, 2147, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 7475, 2007, 1996, 14799, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n",
            "label:\n",
            " 1\n",
            "Expected label:\n",
            " 1\n",
            "Sample Test Case Passed!\n",
            "****************************************\n",
            "\n",
            "Sample Test Case 4: Checking if `__getitem__` is implemented correctly for `idx= 0` for a different bert-variant\n",
            "tokenized_input_dict:\n",
            " {'A': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6546, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6218, 1313, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 170, 1363, 1910, 1106, 1138, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n",
            "Expected tokenized_input_dict:\n",
            " {'A': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6546, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'B': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6218, 1313, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'C': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 170, 1363, 1910, 1106, 1138, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n",
            "label:\n",
            " 0\n",
            "Expected label:\n",
            " 0\n",
            "Sample Test Case Passed!\n",
            "****************************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Running Sample Test Cases\")\n",
        "\n",
        "sample_dataset = SIQABertDataset(train_data[:2], train_labels[:2], bert_variant=\"bert-base-uncased\")\n",
        "\n",
        "print(f\"Sample Test Case 1: Checking if `__len__` is implemented correctly\")\n",
        "dataset_len= len(sample_dataset)\n",
        "expected_len = 2\n",
        "print(f\"Dataset Length: {dataset_len}\")\n",
        "print(f\"Expected Length: {expected_len}\")\n",
        "assert len(sample_dataset) == expected_len\n",
        "print(\"Sample Test Case Passed!\")\n",
        "print(\"****************************************\\n\")\n",
        "\n",
        "print(f\"Sample Test Case 2: Checking if `__getitem__` is implemented correctly for `idx= 0`\")\n",
        "sample_idx = 0\n",
        "tokenized_input_dict, label = sample_dataset.__getitem__(sample_idx)\n",
        "expected_tokenized_input_dict = {'A': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 7052, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
        "  'B': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 2066, 6595, 2188, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
        "  'C': {'input_ids': [101, 7232, 2787, 2000, 2031, 1037, 26375, 1998, 5935, 2014, 2814, 2362, 1012, 102, 2129, 2052, 2500, 2514, 2004, 1037, 2765, 1029, 102, 1037, 2204, 2767, 2000, 2031, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n",
        "expected_label = 0\n",
        "print(f\"tokenized_input_dict:\\n {tokenized_input_dict}\")\n",
        "print(f\"Expected tokenized_input_dict:\\n {expected_tokenized_input_dict}\")\n",
        "assert (expected_tokenized_input_dict == tokenized_input_dict)\n",
        "\n",
        "print(f\"label:\\n {label}\")\n",
        "print(f\"Expected label:\\n {expected_label}\")\n",
        "assert expected_label == label\n",
        "\n",
        "print(\"Sample Test Case Passed!\")\n",
        "print(\"****************************************\\n\")\n",
        "\n",
        "\n",
        "print(f\"Sample Test Case 3: Checking if `__getitem__` is implemented correctly for `idx= 1`\")\n",
        "sample_idx = 1\n",
        "tokenized_input_dict, label = sample_dataset.__getitem__(sample_idx)\n",
        "expected_tokenized_input_dict =  {'A': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 21090, 2007, 5553, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
        "                                  'B': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 2131, 2000, 2147, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
        "                                'C': {'input_ids': [101, 5553, 2734, 2000, 2507, 2041, 5841, 2005, 2019, 9046, 2622, 2012, 2147, 1012, 102, 2054, 2097, 2500, 2215, 2000, 2079, 2279, 1029, 102, 7475, 2007, 1996, 14799, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n",
        "\n",
        "\n",
        "expected_label = 1\n",
        "print(f\"tokenized_input_dict:\\n {tokenized_input_dict}\")\n",
        "print(f\"Expected tokenized_input_dict:\\n {expected_tokenized_input_dict}\")\n",
        "assert (expected_tokenized_input_dict == tokenized_input_dict)\n",
        "\n",
        "print(f\"label:\\n {label}\")\n",
        "print(f\"Expected label:\\n {expected_label}\")\n",
        "assert expected_label == label\n",
        "\n",
        "print(\"Sample Test Case Passed!\")\n",
        "print(\"****************************************\\n\")\n",
        "\n",
        "print(f\"Sample Test Case 4: Checking if `__getitem__` is implemented correctly for `idx= 0` for a different bert-variant\")\n",
        "sample_dataset = SIQABertDataset(train_data[:2], train_labels[:2], bert_variant=\"bert-base-cased\")\n",
        "sample_idx = 0\n",
        "tokenized_input_dict, label = sample_dataset.__getitem__(sample_idx)\n",
        "expected_tokenized_input_dict = {'A': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6546, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
        " 'B': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 1176, 6218, 1313, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
        " 'C': {'input_ids': [101, 6681, 1879, 1106, 1138, 170, 2927, 3962, 27138, 1105, 5260, 1123, 2053, 1487, 119, 102, 1731, 1156, 8452, 1631, 1112, 170, 1871, 136, 102, 170, 1363, 1910, 1106, 1138, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n",
        "expected_label = 0\n",
        "print(f\"tokenized_input_dict:\\n {tokenized_input_dict}\")\n",
        "print(f\"Expected tokenized_input_dict:\\n {expected_tokenized_input_dict}\")\n",
        "assert (expected_tokenized_input_dict == tokenized_input_dict)\n",
        "\n",
        "print(f\"label:\\n {label}\")\n",
        "print(f\"Expected label:\\n {expected_label}\")\n",
        "assert expected_label == label\n",
        "\n",
        "print(\"Sample Test Case Passed!\")\n",
        "print(\"****************************************\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ec2e35e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8ec2e35e",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "cc5d11b79dc8589bdd3573c800432f66",
          "grade": false,
          "grade_id": "cell-cd1f8ca8abf3200c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "We can now create Dataset instances for both training and dev datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d3994ab0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "d3994ab0",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "735c155fc2aa10ce309a01ee3633e498",
          "grade": false,
          "grade_id": "cell-8429b84248f83374",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "train_dataset = SIQABertDataset(train_data, train_labels, bert_variant=\"bert-base-uncased\")\n",
        "dev_dataset = SIQABertDataset(dev_data, dev_labels, bert_variant=\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdbc4e1e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fdbc4e1e",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4be15d36f57f79a7c99c9ac3c6999af7",
          "grade": false,
          "grade_id": "cell-4c64650b199dc2fc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Before we instantiate the dataloaders for iterating over the dataset like last time, we need define a collate function, that creates batches from a list of dataset examples. In the last class we didn't have to create one, because all of our examples were of the same size, but that's not the case anymore, and we need to pad the sequences so that they all are of same size. We have implemented the collate_fn for you below, but we recommend going through it step by step, as it is used often in practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6ab6fb0b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "6ab6fb0b",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a11cdf1b79593a7073b05a78506d044b",
          "grade": false,
          "grade_id": "cell-9047ce580136ef0f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def collate_fn(tokenizer, batch):\n",
        "    \"\"\"\n",
        "    Collate function to be used when creating a data loader for the SIQA dataset.\n",
        "    :param tokenizer: The tokenizer to be used to tokenize the inputs.\n",
        "    :param batch: A list of tuples of the form (tokenized_input_dict, label)\n",
        "    :return: A tuple of the form (tokenized_inputs_dict_batch, labels_batch)\n",
        "    \"\"\"\n",
        "\n",
        "    tokenized_inputsA_batch = []\n",
        "    tokenized_inputsB_batch = []\n",
        "    tokenized_inputsC_batch = []\n",
        "    labels_batch = []\n",
        "    for tokenized_inputs_dict, label in batch:\n",
        "        tokenized_inputsA_batch.append(tokenized_inputs_dict[\"A\"])\n",
        "        tokenized_inputsB_batch.append(tokenized_inputs_dict[\"B\"])\n",
        "        tokenized_inputsC_batch.append(tokenized_inputs_dict[\"C\"])\n",
        "        labels_batch.append(label)\n",
        "\n",
        "    #Pad the inputs\n",
        "    tokenized_inputsA_batch = tokenizer.pad(tokenized_inputsA_batch, padding=True, return_tensors=\"pt\")\n",
        "    tokenized_inputsB_batch = tokenizer.pad(tokenized_inputsB_batch, padding=True, return_tensors=\"pt\")\n",
        "    tokenized_inputsC_batch = tokenizer.pad(tokenized_inputsC_batch, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Convert labels list to a tensor\n",
        "    labels_batch = torch.tensor(labels_batch)\n",
        "    return (\n",
        "        {\"A\": tokenized_inputsA_batch[\"input_ids\"], \"B\": tokenized_inputsB_batch[\"input_ids\"], \"C\": tokenized_inputsC_batch[\"input_ids\"]},\n",
        "        {\"A\": tokenized_inputsA_batch[\"attention_mask\"], \"B\": tokenized_inputsB_batch[\"attention_mask\"], \"C\": tokenized_inputsC_batch[\"attention_mask\"]},\n",
        "        labels_batch\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2548c845",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2548c845",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "276f9714f43f47be2ce8d32465f0dda0",
          "grade": false,
          "grade_id": "cell-e25eecdae3525f47",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Now that we have defined the collate_fn, lets create the dataloaders. It is common to use smaller batch size while fine-tuning these big models, as they occupy quite a lot of memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6c8784f5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "6c8784f5",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "38520c5b41b69dc2a467c460409e961c",
          "grade": false,
          "grade_id": "cell-0155f0a92349c779",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=partial(collate_fn, train_dataset.tokenizer))\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True, collate_fn=partial(collate_fn, dev_dataset.tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "b9b42884",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b9b42884",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7b9f3c4ce63be8d665ca5a5b7ea0f1cc",
          "grade": false,
          "grade_id": "cell-04007c519041a199",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch_input_ids:\n",
            " {'A': tensor([[  101, 22712,  4055,  1996,  9850,  2046,  5020,  3033,  2000,  5676,\n",
            "          2008,  2169,  1997,  1996,  6368,  8267,  2019,  5020,  7451, 14704,\n",
            "          1012,   102,  2129,  2052,  1996,  6368,  2514,  2004,  1037,  2765,\n",
            "          1029,   102, 16465,   102,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  7232,  2165,  3903,  2007,  2068,  2138,  2027,  2245,  2009,\n",
            "          2001,  1996,  2157,  2518,  2000,  2079,  1012,   102,  2054,  2097,\n",
            "          4148,  2000,  2500,  1029,   102,  5382,  2016,  2001,  3308,   102,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101, 15983,  2001, 11243,  2000,  8329,  2082,  1998,  2356,  3389,\n",
            "          2005,  1037, 12832,  3661,  1012,   102,  2054,  2106, 15983,  2342,\n",
            "          2000,  2079,  2077,  2023,  1029,   102,  3926,  1996,  8329,  3653,\n",
            "          2890, 24871,  2015,   102,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101, 22712,  2106,  2025,  2131,  2247,  2092,  2007,  2014,  2388,\n",
            "          1998,  2027,  2439,  3967,  1012,   102,  2054,  2097, 22712,  2215,\n",
            "          2000,  2079,  2279,  1029,   102,  2202,  2014,  2388,  2070, 23827,\n",
            "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101, 27970,  4342,  2446,  1999,  2082,  1998,  2359,  2000,  2175,\n",
            "          2000,  4068,  1012,   102,  2054,  2515, 27970,  2342,  2000,  2079,\n",
            "          2077,  2023,  1029,   102,  3218,  2014,  2446,  1999,  1037,  2446,\n",
            "          2103,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  7232,  2716,  2037,  2155,  2188,  2013,  1996, 10885,  2000,\n",
            "          1996,  3509,  1999,  3516,  1012,   102,  2339,  2106,  7232,  2079,\n",
            "          2023,  1029,   102,  4965,  4946,  9735,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  5863,  2467,  2318, 13175,  2151,  2051,  2027,  2106,  2025,\n",
            "          2131,  2037,  2126,  1012,   102,  2129,  2052,  2017,  6235,  5863,\n",
            "          1029,   102,  1037,  8242,  2711,   102,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  8925,  8546,  5553,  2007,  2062,  3947,  2000,  2131,  2068,\n",
            "          2000,  2079,  2037,  2190,  1012,   102,  2054,  2097,  8925,  2215,\n",
            "          2000,  2079,  2279,  1029,   102,  2831,  2055,  2068,  2092,   102,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  8804,  7765,  6683,  1005,  1055,  3093,  5238,  2007,  5142,\n",
            "          1999,  2037,  2159,  1012,   102,  2054,  2515,  8804,  2342,  2000,\n",
            "          2079,  2077,  2023,  1029,   102,  3610,  8804,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101, 22712,  2787,  2000,  4965,  6265,  2005,  1996,  2878,  2147,\n",
            "          3626,  1012,   102,  2054,  2515, 22712,  2342,  2000,  2079,  2077,\n",
            "          2023,  1029,   102,  2175,  2188,   102,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101, 14509,  2318,  1037, 14380,  9648,  3233,  2648,  2010,  2160,\n",
            "          2006,  1996,  5353,  1012,   102,  2054,  2515, 14509,  2342,  2000,\n",
            "          2079,  2077,  2023,  1029,   102,  4965, 14380,  2015,   102,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  5863,  5078,  2308,  1005,  1055,  6007,  2005,  2116,  2086,\n",
            "          2077, 13648,  2016,  6871,  1996,  2514,  1997,  2273,  2015,  6007,\n",
            "          1012,   102,  2129,  2052,  5863,  2514,  5728,  1029,   102,  2330,\n",
            "          1011, 13128,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101, 22712,  1998, 10555,  2020,  5962,  2000,  1037,  4613,  2445,\n",
            "          2011,  1037,  6801,  3761,  1012,   102,  2054,  2097,  4148,  2000,\n",
            "         22712,  1029,   102,  6869,  2000, 10555,  1999, 21025, 29325,  4509,\n",
            "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101, 18403,  2253,  2000,  1996,  2283,  2894,  1998,  2371,  2200,\n",
            "          5976,  2045,  2127,  4202,  5411,  2068,  1012,   102,  2054,  2097,\n",
            "          4202,  2215,  2000,  2079,  2279,  1029,   102,  2031,  4569,   102,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101, 18403,  2404, 14509,  1005,  1055,  2147,  2046,  2895,  2021,\n",
            "          2481,  1005,  1056,  3342,  2673, 14509,  2106,  1998,  2253,  2000,\n",
            "          3198,  2068,  2055,  2009,  1012,   102,  2054,  2097, 18403,  2215,\n",
            "          2000,  2079,  2279,  1029,   102,  2191,  2469,  1996,  3105,  2001,\n",
            "          2589, 11178,   102],\n",
            "        [  101, 18961,  2359,  2000,  2175,  2006,  1037,  3058,  2007,  2619,\n",
            "          2007,  4207,  5426,  1010,  2061,  6683,  2435, 18961,  5553,  1005,\n",
            "          1055,  2171,  1012,   102,  2054,  2097, 18961,  2215,  2000,  2079,\n",
            "          2279,  1029,   102,  5293,  2055,  5553,   102,     0,     0,     0,\n",
            "             0,     0,     0]]), 'B': tensor([[  101, 22712,  4055,  1996,  9850,  2046,  5020,  3033,  2000,  5676,\n",
            "          2008,  2169,  1997,  1996,  6368,  8267,  2019,  5020,  7451, 14704,\n",
            "          1012,   102,  2129,  2052,  1996,  6368,  2514,  2004,  1037,  2765,\n",
            "          1029,   102, 14337,   102,     0,     0,     0,     0,     0],\n",
            "        [  101,  7232,  2165,  3903,  2007,  2068,  2138,  2027,  2245,  2009,\n",
            "          2001,  1996,  2157,  2518,  2000,  2079,  1012,   102,  2054,  2097,\n",
            "          4148,  2000,  2500,  1029,   102,  2031,  2037,  4784,  3908,   102,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101, 15983,  2001, 11243,  2000,  8329,  2082,  1998,  2356,  3389,\n",
            "          2005,  1037, 12832,  3661,  1012,   102,  2054,  2106, 15983,  2342,\n",
            "          2000,  2079,  2077,  2023,  1029,   102,  2004,  5475,   102,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101, 22712,  2106,  2025,  2131,  2247,  2092,  2007,  2014,  2388,\n",
            "          1998,  2027,  2439,  3967,  1012,   102,  2054,  2097, 22712,  2215,\n",
            "          2000,  2079,  2279,  1029,   102,  3046,  2000,  2424,  2014,  2388,\n",
            "           102,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101, 27970,  4342,  2446,  1999,  2082,  1998,  2359,  2000,  2175,\n",
            "          2000,  4068,  1012,   102,  2054,  2515, 27970,  2342,  2000,  2079,\n",
            "          2077,  2023,  1029,   102,  2224,  2014,  2446,  4813,  1999,  1037,\n",
            "          2406,  2008,  8847,  1996,  2653,   102,     0,     0,     0],\n",
            "        [  101,  7232,  2716,  2037,  2155,  2188,  2013,  1996, 10885,  2000,\n",
            "          1996,  3509,  1999,  3516,  1012,   102,  2339,  2106,  7232,  2079,\n",
            "          2023,  1029,   102,  4895, 23947,  2010,  2477,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  5863,  2467,  2318, 13175,  2151,  2051,  2027,  2106,  2025,\n",
            "          2131,  2037,  2126,  1012,   102,  2129,  2052,  2017,  6235,  5863,\n",
            "          1029,   102, 14984,  1998,  5905,   102,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  8925,  8546,  5553,  2007,  2062,  3947,  2000,  2131,  2068,\n",
            "          2000,  2079,  2037,  2190,  1012,   102,  2054,  2097,  8925,  2215,\n",
            "          2000,  2079,  2279,  1029,   102,  2005,  2068,  2000,  2079,  2037,\n",
            "          2190,   102,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  8804,  7765,  6683,  1005,  1055,  3093,  5238,  2007,  5142,\n",
            "          1999,  2037,  2159,  1012,   102,  2054,  2515,  8804,  2342,  2000,\n",
            "          2079,  2077,  2023,  1029,   102,  2298,  2012,  6683,   102,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101, 22712,  2787,  2000,  4965,  6265,  2005,  1996,  2878,  2147,\n",
            "          3626,  1012,   102,  2054,  2515, 22712,  2342,  2000,  2079,  2077,\n",
            "          2023,  1029,   102,  2022,  2012,  2147,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101, 14509,  2318,  1037, 14380,  9648,  3233,  2648,  2010,  2160,\n",
            "          2006,  1996,  5353,  1012,   102,  2054,  2515, 14509,  2342,  2000,\n",
            "          2079,  2077,  2023,  1029,   102, 10869,  4589,  2015,   102,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  5863,  5078,  2308,  1005,  1055,  6007,  2005,  2116,  2086,\n",
            "          2077, 13648,  2016,  6871,  1996,  2514,  1997,  2273,  2015,  6007,\n",
            "          1012,   102,  2129,  2052,  5863,  2514,  5728,  1029,   102, 12320,\n",
            "           102,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101, 22712,  1998, 10555,  2020,  5962,  2000,  1037,  4613,  2445,\n",
            "          2011,  1037,  6801,  3761,  1012,   102,  2054,  2097,  4148,  2000,\n",
            "         22712,  1029,   102,  2175,  2046,  1037, 16588,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101, 18403,  2253,  2000,  1996,  2283,  2894,  1998,  2371,  2200,\n",
            "          5976,  2045,  2127,  4202,  5411,  2068,  1012,   102,  2054,  2097,\n",
            "          4202,  2215,  2000,  2079,  2279,  1029,   102,  4536,  1037, 20325,\n",
            "           102,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101, 18403,  2404, 14509,  1005,  1055,  2147,  2046,  2895,  2021,\n",
            "          2481,  1005,  1056,  3342,  2673, 14509,  2106,  1998,  2253,  2000,\n",
            "          3198,  2068,  2055,  2009,  1012,   102,  2054,  2097, 18403,  2215,\n",
            "          2000,  2079,  2279,  1029,   102,  8568, 14509,   102,     0],\n",
            "        [  101, 18961,  2359,  2000,  2175,  2006,  1037,  3058,  2007,  2619,\n",
            "          2007,  4207,  5426,  1010,  2061,  6683,  2435, 18961,  5553,  1005,\n",
            "          1055,  2171,  1012,   102,  2054,  2097, 18961,  2215,  2000,  2079,\n",
            "          2279,  1029,   102,  2131,  1037,  2907,  1997,  5553,   102]]), 'C': tensor([[  101, 22712,  4055,  1996,  9850,  2046,  5020,  3033,  2000,  5676,\n",
            "          2008,  2169,  1997,  1996,  6368,  8267,  2019,  5020,  7451, 14704,\n",
            "          1012,   102,  2129,  2052,  1996,  6368,  2514,  2004,  1037,  2765,\n",
            "          1029,   102,  7568,  2005,  1996,  9850,   102,     0,     0,     0,\n",
            "             0],\n",
            "        [  101,  7232,  2165,  3903,  2007,  2068,  2138,  2027,  2245,  2009,\n",
            "          2001,  1996,  2157,  2518,  2000,  2079,  1012,   102,  2054,  2097,\n",
            "          4148,  2000,  2500,  1029,   102,  4067,  7232,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [  101, 15983,  2001, 11243,  2000,  8329,  2082,  1998,  2356,  3389,\n",
            "          2005,  1037, 12832,  3661,  1012,   102,  2054,  2106, 15983,  2342,\n",
            "          2000,  2079,  2077,  2023,  1029,   102,  2507,  3389,  1037,  3105,\n",
            "          4357,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [  101, 22712,  2106,  2025,  2131,  2247,  2092,  2007,  2014,  2388,\n",
            "          1998,  2027,  2439,  3967,  1012,   102,  2054,  2097, 22712,  2215,\n",
            "          2000,  2079,  2279,  1029,   102,  2831,  9928,  2000,  2014,  2388,\n",
            "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [  101, 27970,  4342,  2446,  1999,  2082,  1998,  2359,  2000,  2175,\n",
            "          2000,  4068,  1012,   102,  2054,  2515, 27970,  2342,  2000,  2079,\n",
            "          2077,  2023,  1029,   102,  2734,  2000,  2215,  2000,  4553,  2446,\n",
            "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [  101,  7232,  2716,  2037,  2155,  2188,  2013,  1996, 10885,  2000,\n",
            "          1996,  3509,  1999,  3516,  1012,   102,  2339,  2106,  7232,  2079,\n",
            "          2023,  1029,   102,  2175,  2000,  1996,  4020,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [  101,  5863,  2467,  2318, 13175,  2151,  2051,  2027,  2106,  2025,\n",
            "          2131,  2037,  2126,  1012,   102,  2129,  2052,  2017,  6235,  5863,\n",
            "          1029,   102,  2019,  1999, 21031,  9048,  3468,  2711,   102,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [  101,  8925,  8546,  5553,  2007,  2062,  3947,  2000,  2131,  2068,\n",
            "          2000,  2079,  2037,  2190,  1012,   102,  2054,  2097,  8925,  2215,\n",
            "          2000,  2079,  2279,  1029,   102,  6047,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [  101,  8804,  7765,  6683,  1005,  1055,  3093,  5238,  2007,  5142,\n",
            "          1999,  2037,  2159,  1012,   102,  2054,  2515,  8804,  2342,  2000,\n",
            "          2079,  2077,  2023,  1029,   102,  3480,  6683,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [  101, 22712,  2787,  2000,  4965,  6265,  2005,  1996,  2878,  2147,\n",
            "          3626,  1012,   102,  2054,  2515, 22712,  2342,  2000,  2079,  2077,\n",
            "          2023,  1029,   102,  2131,  5045,   102,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [  101, 14509,  2318,  1037, 14380,  9648,  3233,  2648,  2010,  2160,\n",
            "          2006,  1996,  5353,  1012,   102,  2054,  2515, 14509,  2342,  2000,\n",
            "          2079,  2077,  2023,  1029,   102,  3828,  2039,  2769,  2006,  1996,\n",
            "          5353,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [  101,  5863,  5078,  2308,  1005,  1055,  6007,  2005,  2116,  2086,\n",
            "          2077, 13648,  2016,  6871,  1996,  2514,  1997,  2273,  2015,  6007,\n",
            "          1012,   102,  2129,  2052,  5863,  2514,  5728,  1029,   102,  7653,\n",
            "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [  101, 22712,  1998, 10555,  2020,  5962,  2000,  1037,  4613,  2445,\n",
            "          2011,  1037,  6801,  3761,  1012,   102,  2054,  2097,  4148,  2000,\n",
            "         22712,  1029,   102,  2131,  2046,  2019,  6685,  2007, 10555,   102,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [  101, 18403,  2253,  2000,  1996,  2283,  2894,  1998,  2371,  2200,\n",
            "          5976,  2045,  2127,  4202,  5411,  2068,  1012,   102,  2054,  2097,\n",
            "          4202,  2215,  2000,  2079,  2279,  1029,   102,  2175,  2000,  2793,\n",
            "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [  101, 18403,  2404, 14509,  1005,  1055,  2147,  2046,  2895,  2021,\n",
            "          2481,  1005,  1056,  3342,  2673, 14509,  2106,  1998,  2253,  2000,\n",
            "          3198,  2068,  2055,  2009,  1012,   102,  2054,  2097, 18403,  2215,\n",
            "          2000,  2079,  2279,  1029,   102,  2424, 14509,   102,     0,     0,\n",
            "             0],\n",
            "        [  101, 18961,  2359,  2000,  2175,  2006,  1037,  3058,  2007,  2619,\n",
            "          2007,  4207,  5426,  1010,  2061,  6683,  2435, 18961,  5553,  1005,\n",
            "          1055,  2171,  1012,   102,  2054,  2097, 18961,  2215,  2000,  2079,\n",
            "          2279,  1029,   102,  2507, 18961,  5553,  1005,  1055,  3042,  2193,\n",
            "           102]])}\n",
            "batch_attn_mask:\n",
            " {'A': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]), 'B': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'C': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "batch_labels:\n",
            " tensor([2, 2, 0, 1, 2, 1, 2, 1, 1, 1, 0, 2, 2, 0, 2, 1])\n"
          ]
        }
      ],
      "source": [
        "batch_input_ids, batch_attn_mask, batch_labels = next(iter(train_loader))\n",
        "print(f\"batch_input_ids:\\n {batch_input_ids}\")\n",
        "print(f\"batch_attn_mask:\\n {batch_attn_mask}\")\n",
        "print(f\"batch_labels:\\n {batch_labels}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "849edfc0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "849edfc0",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7f5abac97eb62041f0add8617ad42871",
          "grade": false,
          "grade_id": "cell-21c75958035402dc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## Task 2: Implementing and Training BERT-based Multiple Choice Classifier (1 hour 30 minutes)\n",
        "\n",
        "Similar to pretrained tokenizers, the transformers library also provide numerous pre-trained language models that can be fine-tuned on a wide variety of downstream tasks. We demonstrate usage of these models below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "c9322600",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "c9322600",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "292023c90e82c1cc3836a7c39718fb03",
          "grade": false,
          "grade_id": "cell-0518264e94de005b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import BertModel from the library\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create an instance of pretrained BERT\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a7f074b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "4a7f074b",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "98a82e5b7717eda4f5226c6ae269fe58",
          "grade": false,
          "grade_id": "cell-ad0d391f24e70315",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "As you can see very similar to how we created pre-trained tokenizer, we can load a pretrained BERT model by calling `BertModel.from_pretrained(bert-base-uncased)`. This can actually be considered just a Pytorch `nn.Module` like `nn.Linear` and can be similarly plugged into a network architecture. Also, notice the model contains 12 BERT layers, where each layer consists of a Self Attention layer followed by a sequence of linear layers and activation functions (MLP), as we discussed when talking about Transformer architecture in the lecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c994b139",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "c994b139",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ac922a7cea63724c27c0cf5c6f00ad83",
          "grade": false,
          "grade_id": "cell-44c7f2f43d5ba025",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2823, -0.2353, -0.3529,  ..., -0.0834,  0.2548,  0.4870],\n",
              "         [ 0.4055, -1.1768, -0.2842,  ..., -0.3740,  0.3920, -0.4480],\n",
              "         [ 0.0377, -0.7788, -0.1174,  ..., -0.4201, -0.3078,  0.1824],\n",
              "         ...,\n",
              "         [-1.1595, -1.5650, -0.2526,  ..., -0.4569, -0.5474,  0.2315],\n",
              "         [-1.0644, -0.5952, -0.3912,  ...,  0.2788, -0.0207, -0.1262],\n",
              "         [ 0.5158,  0.4573,  0.0263,  ...,  0.1445, -0.6398, -0.5258]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-7.1342e-01, -3.3350e-01, -4.4551e-01,  4.5434e-01,  5.0347e-01,\n",
              "         -8.4619e-02,  4.8077e-01,  8.4159e-02, -2.2890e-01, -9.9974e-01,\n",
              "         -2.4549e-01,  7.9480e-01,  9.8030e-01, -2.8880e-02,  9.1588e-01,\n",
              "         -2.4135e-01,  5.3666e-01, -5.0656e-01,  2.2218e-01,  3.2123e-01,\n",
              "          6.4942e-01,  9.9757e-01,  4.7173e-01,  2.0766e-01,  3.6500e-01,\n",
              "          8.8725e-01, -4.2194e-01,  9.3109e-01,  9.1743e-01,  7.2114e-01,\n",
              "         -1.4353e-02, -1.8941e-02, -9.8951e-01,  3.0722e-02, -2.8992e-01,\n",
              "         -9.8191e-01,  1.7840e-01, -5.9347e-01,  1.1879e-01,  2.5965e-01,\n",
              "         -8.6462e-01,  1.4619e-01,  9.9952e-01, -4.7741e-01,  1.0605e-01,\n",
              "         -1.2106e-01, -9.9986e-01,  2.2875e-01, -8.5771e-01,  5.0007e-01,\n",
              "          2.7700e-01,  7.1537e-01,  9.0395e-02,  3.0770e-01,  3.6540e-01,\n",
              "         -7.2003e-02, -2.7237e-01, -2.9247e-02, -2.4881e-01, -4.1984e-01,\n",
              "         -6.4784e-01,  1.4369e-01, -5.4503e-01, -7.8703e-01,  4.1153e-01,\n",
              "          2.9400e-01, -1.3110e-01, -8.3964e-02,  6.9513e-03, -1.4154e-01,\n",
              "          6.8059e-01,  8.8732e-02,  1.5826e-02, -8.0095e-01,  1.2098e-01,\n",
              "          1.5737e-01, -5.1418e-01,  1.0000e+00,  4.2996e-02, -9.8423e-01,\n",
              "          1.7809e-01,  2.3727e-01,  3.4371e-01,  4.4902e-01, -2.5424e-01,\n",
              "         -1.0000e+00,  3.7378e-01, -2.9284e-03, -9.9117e-01,  1.0483e-01,\n",
              "          4.8637e-01, -2.1140e-01, -1.3496e-01,  4.0792e-01,  5.8097e-02,\n",
              "         -1.5223e-01, -2.3065e-01, -4.2501e-01, -1.0952e-01, -1.6091e-01,\n",
              "          2.2054e-01,  7.6145e-02, -4.6461e-02, -2.1545e-01,  2.0504e-01,\n",
              "         -4.3479e-01,  2.3064e-02,  4.2032e-01, -3.0370e-01,  5.1830e-01,\n",
              "          3.7295e-01, -1.9882e-01,  2.8470e-01, -9.4566e-01,  3.9429e-01,\n",
              "         -2.5140e-01, -9.8011e-01, -4.6321e-01, -9.9085e-01,  6.2031e-01,\n",
              "          3.0363e-02, -2.4470e-01,  9.4980e-01,  4.4613e-01,  1.0056e-01,\n",
              "          1.2657e-01, -4.9239e-01, -1.0000e+00, -3.8979e-01,  1.7306e-02,\n",
              "          6.2905e-02,  7.5701e-03, -9.6858e-01, -9.6126e-01,  3.2523e-01,\n",
              "          9.4696e-01,  2.4653e-02,  9.9795e-01, -1.0900e-01,  9.2609e-01,\n",
              "          2.7076e-01, -2.3979e-01,  2.4415e-03, -4.1432e-01,  3.1651e-01,\n",
              "         -2.5690e-01, -5.1865e-02,  1.3838e-01, -1.3608e-01,  1.5089e-02,\n",
              "         -2.7091e-01,  1.0167e-02, -6.0730e-02, -9.0828e-01, -2.3963e-01,\n",
              "          9.5270e-01, -1.2421e-01, -5.1564e-01,  4.5463e-01, -9.4697e-02,\n",
              "          1.9801e-02,  6.7040e-01,  3.5773e-01,  3.0287e-01, -3.0182e-01,\n",
              "          3.2714e-01, -3.3120e-01,  4.4667e-01, -5.9530e-01,  3.9171e-01,\n",
              "          2.3601e-01, -1.9295e-01, -1.1556e-01, -9.7889e-01, -2.1943e-01,\n",
              "          1.7873e-01,  9.8326e-01,  6.1193e-01,  1.2009e-01,  4.6305e-01,\n",
              "         -2.2367e-01,  5.1362e-01, -9.4201e-01,  9.8198e-01,  2.3596e-02,\n",
              "          1.6000e-01, -1.6324e-01,  2.4670e-01, -8.0511e-01, -3.3924e-01,\n",
              "          4.8846e-01, -5.1100e-01, -7.3509e-01,  4.6975e-02, -3.3179e-01,\n",
              "         -1.8198e-01, -4.1462e-01,  1.0465e-01, -2.1432e-01, -3.4277e-01,\n",
              "          9.7267e-02,  9.3661e-01,  7.2161e-01,  4.8405e-01, -3.6871e-01,\n",
              "          3.1814e-01, -8.4417e-01, -4.6677e-01,  2.3357e-02,  8.2280e-02,\n",
              "         -3.5102e-02,  9.9018e-01, -2.8120e-01,  1.5138e-01, -8.7715e-01,\n",
              "         -9.8174e-01, -1.8074e-01, -8.3024e-01, -1.6251e-01, -4.6465e-01,\n",
              "          4.1388e-01, -5.0448e-01,  7.9140e-03,  1.6464e-01, -8.4324e-01,\n",
              "         -5.9927e-01,  3.1949e-01, -1.7097e-01,  3.2296e-01, -2.7389e-01,\n",
              "          9.0340e-01,  6.0541e-01, -4.7819e-01, -3.6939e-01,  9.1910e-01,\n",
              "         -2.9349e-01, -7.2003e-01,  3.8927e-01, -1.0532e-01,  5.2019e-01,\n",
              "         -4.1045e-01,  9.4339e-01,  6.6610e-01,  4.9983e-01, -8.7163e-01,\n",
              "          1.0599e-02, -6.2514e-01,  5.1352e-02,  7.7696e-04, -5.0177e-01,\n",
              "          2.8171e-01,  4.3400e-01,  2.9356e-01,  7.4288e-01, -4.1968e-02,\n",
              "          8.6796e-01, -9.1876e-01, -9.4036e-01, -7.8274e-01,  9.0407e-02,\n",
              "         -9.8649e-01,  1.0411e-01,  1.7797e-01, -1.0403e-01, -2.1737e-01,\n",
              "         -1.7412e-01, -9.4650e-01,  3.6575e-01, -4.9281e-02,  9.1307e-01,\n",
              "         -3.6831e-01, -6.6573e-01, -4.2411e-01, -9.2306e-01, -2.2497e-01,\n",
              "         -1.3000e-01,  6.1594e-02, -1.6496e-01, -9.4146e-01,  4.3231e-01,\n",
              "          4.3532e-01,  4.4700e-01, -1.5150e-01,  9.6835e-01,  9.9996e-01,\n",
              "          9.6499e-01,  8.9856e-01,  4.0772e-01, -9.9106e-01, -7.2477e-01,\n",
              "          9.9990e-01, -8.8650e-01, -9.9999e-01, -8.8209e-01, -3.2363e-01,\n",
              "         -1.0676e-02, -1.0000e+00, -6.8102e-02,  2.3007e-01, -8.1374e-01,\n",
              "          1.3577e-01,  9.7153e-01,  9.1199e-01, -1.0000e+00,  7.6972e-01,\n",
              "          9.3895e-01, -4.7636e-01,  7.2471e-01, -1.8839e-01,  9.6564e-01,\n",
              "          2.1396e-01,  3.6877e-01, -6.0257e-02,  2.6504e-01, -5.1107e-01,\n",
              "         -4.2091e-01,  1.9413e-02, -2.5991e-01,  9.7003e-01, -2.1713e-02,\n",
              "         -4.3161e-01, -8.6556e-01,  2.8448e-01,  5.9015e-02, -4.4330e-01,\n",
              "         -9.4856e-01, -1.1291e-01,  2.5512e-02,  3.7114e-01,  9.9046e-03,\n",
              "          5.2219e-02, -3.4138e-01, -3.1813e-02, -3.0110e-01, -5.4605e-02,\n",
              "          5.3167e-01, -9.1741e-01, -2.2246e-01, -1.0529e-02, -4.1509e-01,\n",
              "          3.0833e-01, -9.7295e-01,  9.5327e-01, -2.9202e-01,  5.2656e-01,\n",
              "          1.0000e+00, -1.5967e-02, -7.8224e-01,  2.5205e-01,  8.0371e-02,\n",
              "         -1.1654e-01,  1.0000e+00,  5.2342e-01, -9.7992e-01, -4.5646e-01,\n",
              "          4.4127e-01, -3.6341e-01, -5.4687e-01,  9.9663e-01, -1.6571e-01,\n",
              "         -2.4606e-01,  7.1765e-02,  9.8694e-01, -9.8773e-01,  9.2531e-01,\n",
              "         -7.8466e-01, -9.7499e-01,  9.5526e-01,  9.4014e-01, -3.1454e-02,\n",
              "         -3.7029e-01, -8.3567e-02,  7.2886e-02,  7.8482e-02, -8.5670e-01,\n",
              "          2.4106e-01,  1.2381e-01,  2.2441e-02,  9.0840e-01,  4.9770e-02,\n",
              "         -4.8200e-01,  1.0067e-01, -3.3606e-01,  2.7572e-01,  5.1258e-01,\n",
              "          3.4788e-01,  2.0841e-02, -4.1822e-02,  2.2078e-02, -5.3827e-01,\n",
              "         -9.6365e-01,  5.3596e-01,  1.0000e+00,  1.7988e-01,  2.2127e-01,\n",
              "          1.1171e-01,  4.2887e-02, -2.8482e-01,  2.7275e-01,  2.8186e-01,\n",
              "         -1.9890e-01, -6.9904e-01,  5.4585e-01, -8.2019e-01, -9.8801e-01,\n",
              "          3.9557e-01,  1.4115e-01, -9.0601e-02,  9.9788e-01,  5.2465e-02,\n",
              "          8.7988e-02, -6.1069e-02,  8.0411e-01, -1.0107e-01,  4.5894e-02,\n",
              "          2.8721e-01,  9.7186e-01, -5.0391e-02,  4.3895e-01,  5.3262e-01,\n",
              "         -3.7363e-01, -1.4349e-01, -5.3335e-01, -1.7289e-01, -9.3699e-01,\n",
              "          2.4583e-01, -9.5441e-01,  9.4962e-01,  6.8467e-01,  2.8843e-01,\n",
              "          2.7843e-02,  1.9399e-01,  1.0000e+00, -5.5154e-01,  2.7470e-01,\n",
              "          7.3334e-01,  2.1665e-01, -9.9376e-01, -6.3638e-01, -4.0457e-01,\n",
              "          8.1165e-02, -1.0932e-01, -1.6521e-01,  1.1090e-01, -9.6194e-01,\n",
              "          1.4183e-01,  2.6996e-01, -9.0304e-01, -9.8854e-01, -1.8411e-01,\n",
              "         -1.4560e-01,  1.0233e-01, -8.8277e-01, -4.2014e-01, -5.6760e-01,\n",
              "          2.0938e-01, -7.8962e-02, -9.2779e-01,  3.7535e-01, -3.2771e-01,\n",
              "          3.7110e-01, -1.7454e-02,  4.4611e-01,  3.7664e-01,  8.9600e-01,\n",
              "         -1.4597e-01, -4.1541e-02, -2.2711e-02, -5.6407e-01,  4.9451e-01,\n",
              "         -6.0685e-01, -5.7528e-01,  1.6080e-02,  1.0000e+00, -2.4831e-01,\n",
              "          4.7874e-01,  3.2856e-01,  4.0354e-01,  3.5562e-02,  7.1661e-02,\n",
              "          5.1131e-01,  1.7113e-01, -5.2107e-04, -2.9695e-01,  7.3751e-01,\n",
              "         -1.8529e-01,  4.4554e-01,  2.2911e-01,  2.8562e-02,  7.7319e-01,\n",
              "          5.5269e-01,  1.0272e-01,  2.6211e-01, -1.2724e-03,  9.7222e-01,\n",
              "         -2.8841e-02,  5.6738e-02, -2.8775e-01, -1.6172e-02, -1.9353e-01,\n",
              "          4.9592e-01,  1.0000e+00,  8.4378e-02, -1.5369e-01, -9.8806e-01,\n",
              "         -3.3429e-01, -7.0185e-01,  9.9986e-01,  7.6013e-01, -6.0292e-01,\n",
              "          3.8539e-01,  2.5654e-01, -1.1947e-01,  3.1904e-01, -2.9968e-02,\n",
              "         -8.2039e-02, -3.6788e-02, -4.1350e-02,  9.4251e-01, -3.8563e-01,\n",
              "         -9.6644e-01, -1.8887e-01,  3.3731e-01, -9.5321e-01,  9.9444e-01,\n",
              "         -3.1648e-01, -7.6926e-02, -2.2872e-01, -1.2125e-01, -8.2213e-01,\n",
              "         -1.8324e-01, -9.8104e-01,  3.3374e-03,  6.5977e-02,  9.6485e-01,\n",
              "          6.2123e-02, -4.1838e-01, -8.8928e-01,  4.6512e-01,  1.6997e-01,\n",
              "         -4.9288e-01, -9.0313e-01,  9.4608e-01, -9.6796e-01,  4.0968e-01,\n",
              "          9.9998e-01,  2.5584e-01, -4.0539e-01,  1.3442e-01, -1.9956e-01,\n",
              "          2.1901e-01, -7.6150e-02,  4.1793e-01, -9.3571e-01, -2.5734e-01,\n",
              "         -2.6220e-02,  1.9381e-01, -1.0859e-02,  1.0622e-02,  5.5476e-01,\n",
              "          1.3884e-01, -3.7362e-01, -5.1091e-01, -2.0794e-02,  2.3886e-01,\n",
              "          4.4453e-01, -1.9578e-01,  2.4322e-02,  1.2190e-01,  4.5709e-02,\n",
              "         -8.9249e-01, -2.3091e-01, -2.3275e-01, -9.9933e-01,  3.8215e-01,\n",
              "         -1.0000e+00,  2.4109e-01, -3.3744e-01, -9.5530e-02,  7.7688e-01,\n",
              "          6.7944e-01,  5.0124e-01, -5.4854e-01, -4.4367e-01,  7.4305e-01,\n",
              "          6.9326e-01, -1.1128e-01,  5.8066e-02, -5.1514e-01, -1.8508e-02,\n",
              "          4.7067e-02, -3.9159e-02, -1.2526e-01,  6.4872e-01, -2.8116e-01,\n",
              "          1.0000e+00,  9.4141e-02, -1.6344e-01, -8.3718e-01,  9.6639e-02,\n",
              "         -1.5380e-01,  9.9999e-01, -4.3615e-01, -9.4652e-01,  1.8444e-01,\n",
              "         -3.2900e-01, -7.3108e-01,  2.8296e-01, -1.6438e-01, -5.5741e-01,\n",
              "         -5.1496e-01,  9.4393e-01,  1.5646e-01, -4.8041e-01,  3.7670e-01,\n",
              "         -7.6502e-02, -3.4085e-01, -2.3005e-01,  4.6745e-01,  9.8561e-01,\n",
              "          1.8930e-01,  6.5902e-01, -1.4607e-01, -5.6217e-02,  9.6718e-01,\n",
              "          2.0109e-01, -4.4703e-01,  2.6055e-03,  1.0000e+00,  2.5435e-01,\n",
              "         -8.5391e-01,  1.4343e-01, -9.7004e-01,  6.7543e-02, -9.1234e-01,\n",
              "          2.3352e-01, -1.0207e-01,  9.0571e-01, -1.2382e-01,  8.9837e-01,\n",
              "         -2.5035e-01, -9.6540e-02, -1.3500e-02,  7.9872e-02,  3.4886e-01,\n",
              "         -9.0798e-01, -9.8696e-01, -9.8399e-01,  2.2660e-01, -2.6348e-01,\n",
              "         -1.7565e-03,  2.5411e-01, -4.6728e-02,  3.2300e-01,  2.3782e-01,\n",
              "         -1.0000e+00,  9.4718e-01,  2.8214e-01,  5.5079e-01,  9.5964e-01,\n",
              "          4.3230e-01,  2.9824e-01,  1.1796e-01, -9.8534e-01, -9.1499e-01,\n",
              "         -2.2060e-01, -2.2493e-01,  4.2716e-01,  4.1828e-01,  7.7361e-01,\n",
              "          2.5652e-01, -4.4088e-01, -5.4283e-01, -1.9669e-01, -9.2347e-01,\n",
              "         -9.9092e-01,  1.6564e-01, -2.1974e-02, -7.5007e-01,  9.5135e-01,\n",
              "         -4.2565e-01,  4.1159e-02,  3.4651e-01, -3.7918e-01,  5.3754e-01,\n",
              "          6.6361e-01, -1.7167e-01, -1.6186e-01,  3.5413e-01,  8.6273e-01,\n",
              "          5.4597e-01,  9.7368e-01, -3.1056e-01,  3.8867e-01, -3.5970e-01,\n",
              "          3.0381e-01,  7.2659e-01, -8.7832e-01,  4.5919e-02,  5.9937e-02,\n",
              "          1.7157e-01,  1.6686e-01, -1.6294e-01, -7.8675e-01,  4.3243e-02,\n",
              "         -2.4214e-01,  9.7457e-02, -2.9032e-01,  2.1996e-01, -2.2742e-01,\n",
              "          4.9332e-02, -3.6432e-01, -2.3407e-01,  5.0983e-01, -1.0163e-01,\n",
              "          9.1016e-01,  5.7243e-01,  4.2006e-02, -4.0053e-01, -9.6789e-03,\n",
              "         -1.7138e-01, -8.6321e-01,  6.2138e-01,  2.0165e-01,  2.1101e-01,\n",
              "          1.2278e-01, -1.7986e-01,  8.6453e-01, -5.4444e-01, -3.0941e-01,\n",
              "         -3.3675e-01, -3.4412e-01,  7.0476e-01, -5.8997e-01, -3.5156e-01,\n",
              "         -5.8885e-02,  4.5485e-01,  1.9294e-01,  9.9796e-01, -1.2719e-01,\n",
              "         -3.9385e-01, -3.1920e-01, -2.4795e-01,  2.6886e-01,  3.8582e-02,\n",
              "         -1.0000e+00,  1.7946e-01, -1.6161e-01, -9.5156e-02, -2.4447e-01,\n",
              "          3.1449e-01, -3.1112e-01, -9.0355e-01, -1.8077e-02,  5.0788e-01,\n",
              "          4.1148e-01, -4.0674e-01, -3.7335e-01,  4.8784e-01, -2.1414e-01,\n",
              "          8.2619e-01,  8.2344e-01, -1.7268e-01,  6.6992e-01,  5.1202e-01,\n",
              "         -4.5926e-01, -5.4121e-01,  9.0145e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = \"kendall was a person who kept her word exquisitely, so she got my money the other day\"\n",
        "tokenizer_output = bert_tokenizer(sentence, return_tensors=\"pt\")\n",
        "input_ids, attn_mask = tokenizer_output[\"input_ids\"], tokenizer_output[\"attention_mask\"]\n",
        "\n",
        "output = bert_model(input_ids, attention_mask = attn_mask)\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "919f7bfc",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "919f7bfc",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "473fa901be8e6117f55619d66f2ff8c3",
          "grade": false,
          "grade_id": "cell-dc933bfb33ddbf97",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "As you can see, calling `bert_model` returns a bunch of different things. Let's go through them one by one and understand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "1f337132",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "1f337132",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "af8fbb32c44fe43322dffc6d07810f4e",
          "grade": false,
          "grade_id": "cell-40ff5447737be114",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids shape: torch.Size([1, 21])\n",
            "last_hidden_state shape: torch.Size([1, 21, 768])\n"
          ]
        }
      ],
      "source": [
        "last_hidden_state = output.last_hidden_state\n",
        "print(f\"input_ids shape: {input_ids.shape}\")\n",
        "print(f\"last_hidden_state shape: {last_hidden_state.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fa9e5c2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7fa9e5c2",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "de5e5dd782c7ff39ef7ee46739bcb46e",
          "grade": false,
          "grade_id": "cell-3b5b43db00757d9a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "For an input of shape `[1,21]` which just means a single sequence of 21 tokens, last_hidden_state is a tensor of shape `[1, 21, 768]` denoting the contextual embedding of each of the 21 tokens in the sequence. These representations can be then used for solving a downstream task, by adding a linear layer or MLP layer on top. These can be useful for sequence labelling type of tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "c9c00632",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "c9c00632",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "00a748ad98adedcbc525dda6496de309",
          "grade": false,
          "grade_id": "cell-2b52de7298f3eda7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids shape: torch.Size([1, 21])\n",
            "pooler_output shape: torch.Size([1, 768])\n"
          ]
        }
      ],
      "source": [
        "pooler_output = output.pooler_output\n",
        "print(f\"input_ids shape: {input_ids.shape}\")\n",
        "print(f\"pooler_output shape: {pooler_output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88dae74d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "88dae74d",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a6a16d47faf0f2b84efa11ea9416a8c6",
          "grade": false,
          "grade_id": "cell-720761c11ee8fa23",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "`pooler_output` is an aggregate representation of the entire sentence and can be thought of as a sentence embedding. It is obtained by passing the representation of the \\[CLS\\] token through a linear layer. This can be useful for sentence-level tasks like sentiment analysis as well as multiple choice classification tasks etc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5785c1f7",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5785c1f7",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "94333c842666bbf724ab301c505a21a9",
          "grade": false,
          "grade_id": "cell-4b59e5d86dbd6e41",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Apart from these two we can also obtain other values by providing additional arguments. Like if we want to obtain attention maps which can be useful for interpretating the model's behavior, we can just specify `output_attentions=True` while calling the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "50ae6ee7",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "50ae6ee7",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "dac70c849fa65e8f8d505d88c9e88019",
          "grade": false,
          "grade_id": "cell-08b6ef4cef17c16e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data type of attentions output: <class 'tuple'>\n",
            "Number of elements: 12\n",
            "Shape of individual element: torch.Size([1, 12, 21, 21])\n",
            "Example attention map: tensor([[0.0365, 0.0188, 0.0239, 0.0918, 0.0375, 0.0409, 0.0390, 0.0327, 0.0256,\n",
            "         0.0260, 0.0488, 0.0387, 0.0532, 0.0188, 0.0251, 0.0408, 0.0321, 0.0840,\n",
            "         0.0793, 0.0262, 0.1801],\n",
            "        [0.0178, 0.0460, 0.0240, 0.0306, 0.0482, 0.0194, 0.0510, 0.0919, 0.0260,\n",
            "         0.0818, 0.0425, 0.0482, 0.0159, 0.0576, 0.0703, 0.1526, 0.0337, 0.0405,\n",
            "         0.0194, 0.0371, 0.0454],\n",
            "        [0.0527, 0.0466, 0.0899, 0.0241, 0.0464, 0.0245, 0.0645, 0.0587, 0.0256,\n",
            "         0.0401, 0.0298, 0.0606, 0.0302, 0.0735, 0.1222, 0.0573, 0.0297, 0.0204,\n",
            "         0.0224, 0.0539, 0.0268],\n",
            "        [0.0425, 0.0528, 0.0454, 0.0338, 0.0402, 0.0353, 0.0515, 0.0666, 0.0397,\n",
            "         0.0464, 0.0336, 0.0518, 0.0492, 0.0609, 0.0529, 0.0731, 0.0509, 0.0580,\n",
            "         0.0381, 0.0492, 0.0280],\n",
            "        [0.0492, 0.1514, 0.0892, 0.0078, 0.0237, 0.0282, 0.0478, 0.0639, 0.0288,\n",
            "         0.0346, 0.0376, 0.0427, 0.0386, 0.0865, 0.0633, 0.0612, 0.0589, 0.0099,\n",
            "         0.0217, 0.0326, 0.0225],\n",
            "        [0.0328, 0.0851, 0.0669, 0.0210, 0.0342, 0.0278, 0.0618, 0.0965, 0.0244,\n",
            "         0.0388, 0.0382, 0.0554, 0.0281, 0.0809, 0.0671, 0.0634, 0.0372, 0.0276,\n",
            "         0.0302, 0.0289, 0.0536],\n",
            "        [0.0390, 0.0402, 0.1059, 0.0350, 0.0359, 0.0453, 0.0443, 0.0639, 0.0261,\n",
            "         0.0437, 0.0524, 0.0837, 0.0440, 0.0611, 0.0661, 0.0560, 0.0265, 0.0284,\n",
            "         0.0247, 0.0368, 0.0410],\n",
            "        [0.0637, 0.0480, 0.0736, 0.0488, 0.0387, 0.0301, 0.0386, 0.1177, 0.0174,\n",
            "         0.0286, 0.0322, 0.0448, 0.0351, 0.0715, 0.0595, 0.0795, 0.0202, 0.0495,\n",
            "         0.0239, 0.0297, 0.0488],\n",
            "        [0.0379, 0.0930, 0.0471, 0.0373, 0.0634, 0.0240, 0.0501, 0.0724, 0.0077,\n",
            "         0.0576, 0.1025, 0.0672, 0.0309, 0.0510, 0.0276, 0.0513, 0.0243, 0.0229,\n",
            "         0.0249, 0.0429, 0.0641],\n",
            "        [0.0287, 0.1002, 0.0683, 0.0142, 0.0493, 0.0313, 0.0819, 0.0516, 0.0350,\n",
            "         0.0476, 0.0602, 0.0453, 0.0288, 0.0470, 0.1209, 0.0661, 0.0310, 0.0140,\n",
            "         0.0197, 0.0225, 0.0367],\n",
            "        [0.0285, 0.0755, 0.0461, 0.0240, 0.0530, 0.0329, 0.0516, 0.0549, 0.0636,\n",
            "         0.0900, 0.0263, 0.0202, 0.0290, 0.0448, 0.0628, 0.0496, 0.0781, 0.0263,\n",
            "         0.0399, 0.0470, 0.0559],\n",
            "        [0.0478, 0.0464, 0.0582, 0.0576, 0.0513, 0.0424, 0.0580, 0.0508, 0.0362,\n",
            "         0.0565, 0.0378, 0.0387, 0.0565, 0.0367, 0.0458, 0.0434, 0.0477, 0.0464,\n",
            "         0.0433, 0.0513, 0.0475],\n",
            "        [0.0320, 0.0923, 0.0587, 0.0300, 0.0370, 0.0399, 0.0736, 0.1011, 0.0183,\n",
            "         0.0490, 0.0473, 0.0639, 0.0212, 0.0637, 0.0596, 0.0799, 0.0234, 0.0308,\n",
            "         0.0306, 0.0149, 0.0330],\n",
            "        [0.0335, 0.0893, 0.1025, 0.0227, 0.0361, 0.0223, 0.0555, 0.1325, 0.0104,\n",
            "         0.0403, 0.0382, 0.0476, 0.0227, 0.0636, 0.0781, 0.0949, 0.0216, 0.0269,\n",
            "         0.0159, 0.0252, 0.0202],\n",
            "        [0.0544, 0.1423, 0.0956, 0.0254, 0.0281, 0.0320, 0.0471, 0.0546, 0.0136,\n",
            "         0.0673, 0.0418, 0.0395, 0.0531, 0.0422, 0.0400, 0.1038, 0.0275, 0.0331,\n",
            "         0.0201, 0.0151, 0.0234],\n",
            "        [0.0403, 0.0612, 0.0584, 0.0240, 0.0489, 0.0368, 0.0703, 0.0585, 0.0444,\n",
            "         0.0276, 0.0329, 0.0323, 0.0453, 0.0484, 0.1187, 0.0948, 0.0410, 0.0257,\n",
            "         0.0229, 0.0184, 0.0490],\n",
            "        [0.0249, 0.0599, 0.0252, 0.0498, 0.0809, 0.0517, 0.0329, 0.0799, 0.0372,\n",
            "         0.0461, 0.0336, 0.0263, 0.0299, 0.0659, 0.0677, 0.0610, 0.0249, 0.0287,\n",
            "         0.0350, 0.0519, 0.0864],\n",
            "        [0.0298, 0.0571, 0.0516, 0.0481, 0.0414, 0.0276, 0.0582, 0.0824, 0.0294,\n",
            "         0.0469, 0.0406, 0.0386, 0.0444, 0.0660, 0.0562, 0.0832, 0.0321, 0.0636,\n",
            "         0.0414, 0.0343, 0.0273],\n",
            "        [0.0209, 0.0511, 0.0464, 0.0348, 0.0476, 0.0474, 0.0860, 0.0561, 0.0389,\n",
            "         0.0365, 0.0628, 0.0366, 0.0508, 0.0486, 0.0764, 0.0663, 0.0431, 0.0291,\n",
            "         0.0349, 0.0329, 0.0528],\n",
            "        [0.0228, 0.1058, 0.1149, 0.0203, 0.0668, 0.0299, 0.0469, 0.0448, 0.0492,\n",
            "         0.0439, 0.0376, 0.0540, 0.0458, 0.0640, 0.0903, 0.0373, 0.0353, 0.0134,\n",
            "         0.0297, 0.0104, 0.0368],\n",
            "        [0.0342, 0.0267, 0.0292, 0.0788, 0.0409, 0.0381, 0.0354, 0.0651, 0.0243,\n",
            "         0.0478, 0.0643, 0.0683, 0.0461, 0.0384, 0.0317, 0.0849, 0.0342, 0.0737,\n",
            "         0.0565, 0.0304, 0.0511]], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "output = bert_model(input_ids, attention_mask = attn_mask, output_attentions=True)\n",
        "attentions = output.attentions\n",
        "print(f\"Data type of attentions output: {type(attentions)}\")\n",
        "print(f\"Number of elements: {len(attentions)}\")\n",
        "print(f\"Shape of individual element: {attentions[0].shape}\")\n",
        "print(f\"Example attention map: {attentions[0][0,0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3a4519b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "a3a4519b",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6b9a58816d3b1ec81fb7081f80acb009",
          "grade": false,
          "grade_id": "cell-34a578f52897b7d7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "As you can see `attentions` is a tuple containing 12 elements which corresponds to the attention maps of each of the 12 layers in the network. Further each layer's attention maps also contains 12 attention maps corresponding to 12 heads in each layer. A single attention map as you can see is a 18x18 matrix representing the attention pattern for all the tokens in the sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a79f03f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8a79f03f",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "345bea080f4368b2a6d87453a22a1edc",
          "grade": false,
          "grade_id": "cell-c224cab96a9915ec",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### Task 2.1: Implementing BERT-based Classifier for Multiple Choice Classification\n",
        "\n",
        "In this task you will implement a bert-based classifier in Pytorch very similar to how we created bag of word classifiers in the previous assignments. The architecture of the model is as follows:\n",
        "\n",
        "![architecture](https://i.ibb.co/hVmS9Qx/siqa-bert-arch-excalli.png)\n",
        "\n",
        "Essentially, what we have here is a model that takes a context and question, and scores a particular answer (denoted as a score(a)). At the backbone we have the BERT model, using which we obtain the contextualized representation of the [context, question, answer] sequence. We then use the \\[CLS\\] token's embedding as the sequence representation and feed it to a 2 layer MLP (Linear(768, 768) -> ReLU -> Linear(768, 1)) that scores the answer. To predict the correct answer, score each of the three answers, obtain their scores and normalize them by applying softmax, that gives us the probability of each option being the correct answer.\n",
        "\n",
        "\n",
        "![forward pass](https://i.ibb.co/r3SrLHY/siqa-bert-forward-excalli.png)\n",
        "\n",
        "Implement the architecture and forward pass in `BertMultiChoiceClassifierModel` class below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "1807722c",
      "metadata": {},
      "outputs": [],
      "source": [
        "bert_out = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "0f626042",
      "metadata": {
        "deletable": false,
        "id": "0f626042",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "34a2a6ddf2f67acbfe7fd30857f6de91",
          "grade": false,
          "grade_id": "cell-7930c03ec3b56775",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class BertMultiChoiceClassifierModel(nn.Module):\n",
        "\n",
        "    def __init__(self, d_hidden = 768, bert_variant = \"bert-base-uncased\"):\n",
        "        \"\"\"\n",
        "        Define the architecture of Bert-Based mulit-choice classifier.\n",
        "        You will mainly need to define 3 components, first a BERT layer\n",
        "        using `BertModel` from transformers library,\n",
        "        a two layer MLP layer to map the representation from Bert to the output i.e. (Linear(d_hidden, d_hidden) -> ReLU -> Linear(d_hidden, 1)),\n",
        "        and a log sftmax layer to map the scores to a probabilities\n",
        "\n",
        "        Inputs:\n",
        "            - d_hidden (int): Size of the hidden representations of bert\n",
        "            - bert_variant (str): BERT variant to use\n",
        "        \"\"\"\n",
        "        super(BertMultiChoiceClassifierModel, self).__init__()\n",
        "        self.bert_layer = BertModel.from_pretrained(bert_variant)\n",
        "        self.mlp_layer = nn.Sequential(nn.Linear(d_hidden, d_hidden), nn.ReLU(), nn.Linear(d_hidden, 1))\n",
        "        self.log_softmax_layer = nn.LogSoftmax()\n",
        "\n",
        "    def forward(self, input_ids_dict, attn_mask_dict):\n",
        "        \"\"\"\n",
        "        Forward Passes the inputs through the network and obtains the prediction\n",
        "\n",
        "        Inputs:\n",
        "            - input_ids_dict (dict(str,torch.tensor)): A dictionary containing input_ids corresponding to each answer choice. Keys are A, B and C and value is a torch tensor of shape [batch_size, seq_len]\n",
        "                                        representing the sequence of token ids\n",
        "            - attn_mask_dict (dict(str,torch.tensor)): A dictionary containing attention mask corresponding to each answer choice. Keys are A, B and C and value is a torch tensor of shape [batch_size, seq_len]\n",
        "\n",
        "        Returns:\n",
        "          - output (torch.tensor): A torch tensor of shape [batch_size,] obtained after passing the input to the network\n",
        "\n",
        "\n",
        "        Hints:\n",
        "            1. Recall which of the outputs from BertModel is appropriate for the sentence classification task and how to access it.\n",
        "            2. `torch.cat` might come in handy before performing softmax\n",
        "        \"\"\"\n",
        "        out = []\n",
        "        for i in \"ABC\":\n",
        "            input_ids = input_ids_dict[i]\n",
        "            attn_mask = attn_mask_dict[i]\n",
        "            bert_out = self.bert_layer(input_ids, attention_mask=attn_mask).pooler_output\n",
        "            mlp_out = self.mlp_layer(bert_out)\n",
        "            out.append(mlp_out)\n",
        "\n",
        "        return self.log_softmax_layer(torch.cat(out, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "1c65d7c7",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "1c65d7c7",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "35f231082355ea3357feed7ad243b08a",
          "grade": true,
          "grade_id": "cell-cf9b5db5de53eeac",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Sample Test Cases!\n",
            "Sample Test Case 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/rajatjacob/.pyenv/versions/3.11.4/envs/nlp/lib/python3.11/site-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Output: [[-1.1189675 -1.0885007 -1.0886751]\n",
            " [-1.1045517 -1.0834143 -1.1080489]\n",
            " [-1.1027125 -1.0822924 -1.1110512]\n",
            " [-1.1008492 -1.0936637 -1.1013427]\n",
            " [-1.0921422 -1.0974908 -1.1062545]\n",
            " [-1.0798945 -1.1088551 -1.1073538]\n",
            " [-1.1030428 -1.0939087 -1.0989064]\n",
            " [-1.0971034 -1.097092  -1.1016483]\n",
            " [-1.1319208 -1.082568  -1.0821619]\n",
            " [-1.096135  -1.1014838 -1.0982256]\n",
            " [-1.0979306 -1.0836827 -1.1144608]\n",
            " [-1.1034716 -1.0959275 -1.0964555]\n",
            " [-1.1019449 -1.0958116 -1.0980898]\n",
            " [-1.1050866 -1.0986388 -1.0921534]\n",
            " [-1.1013198 -1.0821339 -1.1126211]\n",
            " [-1.102798  -1.0906712 -1.1024152]]\n",
            "Expected Output: [[-1.1189675 -1.0885007 -1.0886753]\n",
            " [-1.1045516 -1.0834142 -1.108049 ]\n",
            " [-1.1027125 -1.0822924 -1.1110513]\n",
            " [-1.1008494 -1.0936636 -1.1013424]\n",
            " [-1.0921422 -1.0974907 -1.1062546]\n",
            " [-1.0798943 -1.1088552 -1.1073538]\n",
            " [-1.1030427 -1.0939085 -1.0989065]\n",
            " [-1.0971034 -1.097092  -1.1016482]\n",
            " [-1.131921  -1.0825679 -1.0821619]\n",
            " [-1.0961349 -1.1014836 -1.0982255]\n",
            " [-1.0979307 -1.0836827 -1.1144608]\n",
            " [-1.1034715 -1.0959275 -1.0964555]\n",
            " [-1.1019452 -1.0958116 -1.0980899]\n",
            " [-1.1050864 -1.0986389 -1.0921533]\n",
            " [-1.1013198 -1.0821339 -1.112621 ]\n",
            " [-1.1027979 -1.0906712 -1.1024152]]\n",
            "Test Case Passed! :)\n",
            "******************************\n",
            "\n",
            "Sample Test Case 2\n",
            "Model Output: [[-1.1005359 -1.1009302 -1.094384 ]\n",
            " [-1.073251  -1.117882  -1.1052345]\n",
            " [-1.1025076 -1.0943629 -1.098983 ]\n",
            " [-1.1236265 -1.1056153 -1.0674216]\n",
            " [-1.0999552 -1.1014045 -1.0944904]\n",
            " [-1.0953275 -1.0959653 -1.1045706]\n",
            " [-1.10844   -1.0971687 -1.0903118]\n",
            " [-1.099349  -1.1130908 -1.0836148]\n",
            " [-1.103172  -1.0897291 -1.1029954]\n",
            " [-1.0929242 -1.1077557 -1.0952207]\n",
            " [-1.0995092 -1.0998485 -1.0964825]\n",
            " [-1.1419644 -1.1081929 -1.0479565]\n",
            " [-1.1052557 -1.0851234 -1.1055952]\n",
            " [-1.0840431 -1.1084775 -1.1034836]\n",
            " [-1.0872698 -1.1025085 -1.1061594]\n",
            " [-1.1060572 -1.0939908 -1.095831 ]]\n",
            "Expected Output: [[-1.1005359 -1.1009303 -1.094384 ]\n",
            " [-1.073251  -1.1178819 -1.1052346]\n",
            " [-1.1025076 -1.094363  -1.098983 ]\n",
            " [-1.1236262 -1.1056151 -1.0674216]\n",
            " [-1.0999551 -1.1014045 -1.0944905]\n",
            " [-1.0953273 -1.0959654 -1.1045709]\n",
            " [-1.1084402 -1.0971687 -1.0903118]\n",
            " [-1.099349  -1.1130908 -1.0836148]\n",
            " [-1.1031718 -1.0897288 -1.1029954]\n",
            " [-1.0929244 -1.1077557 -1.0952206]\n",
            " [-1.0995092 -1.0998485 -1.0964826]\n",
            " [-1.1419646 -1.1081928 -1.0479565]\n",
            " [-1.1052557 -1.0851235 -1.1055952]\n",
            " [-1.0840428 -1.1084775 -1.1034834]\n",
            " [-1.0872697 -1.1025085 -1.1061592]\n",
            " [-1.1060572 -1.0939908 -1.095831 ]]\n",
            "Test Case Passed! :)\n",
            "******************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Running Sample Test Cases!\")\n",
        "torch.manual_seed(42)\n",
        "model = BertMultiChoiceClassifierModel()\n",
        "\n",
        "print(\"Sample Test Case 1\")\n",
        "batch_input_ids, batch_attn_mask, batch_labels = next(iter(train_loader))\n",
        "bert_out = model(batch_input_ids, batch_attn_mask).detach().numpy()\n",
        "expected_bert_out = np.array([[-1.1189675, -1.0885007, -1.0886753],\n",
        "                            [-1.1045516, -1.0834142, -1.108049 ],\n",
        "                            [-1.1027125, -1.0822924, -1.1110513],\n",
        "                            [-1.1008494, -1.0936636, -1.1013424],\n",
        "                            [-1.0921422, -1.0974907, -1.1062546],\n",
        "                            [-1.0798943, -1.1088552, -1.1073538],\n",
        "                            [-1.1030427, -1.0939085, -1.0989065],\n",
        "                            [-1.0971034, -1.097092 , -1.1016482],\n",
        "                            [-1.131921 , -1.0825679, -1.0821619],\n",
        "                            [-1.0961349, -1.1014836, -1.0982255],\n",
        "                            [-1.0979307, -1.0836827, -1.1144608],\n",
        "                            [-1.1034715, -1.0959275, -1.0964555],\n",
        "                            [-1.1019452, -1.0958116, -1.0980899],\n",
        "                            [-1.1050864, -1.0986389, -1.0921533],\n",
        "                            [-1.1013198, -1.0821339, -1.112621 ],\n",
        "                            [-1.1027979, -1.0906712, -1.1024152]],)\n",
        "print(f\"Model Output: {bert_out}\")\n",
        "print(f\"Expected Output: {expected_bert_out}\")\n",
        "\n",
        "assert bert_out.shape == expected_bert_out.shape\n",
        "assert np.allclose(bert_out, expected_bert_out, 1e-4)\n",
        "print(\"Test Case Passed! :)\")\n",
        "print(\"******************************\\n\")\n",
        "\n",
        "print(\"Sample Test Case 2\")\n",
        "batch_input_ids, batch_attn_mask, batch_labels = next(iter(dev_loader))\n",
        "bert_out = model(batch_input_ids, batch_attn_mask).detach().numpy()\n",
        "expected_bert_out = np.array([[-1.1005359, -1.1009303, -1.094384 ],\n",
        "                            [-1.073251 , -1.1178819, -1.1052346],\n",
        "                            [-1.1025076, -1.094363 , -1.098983 ],\n",
        "                            [-1.1236262, -1.1056151, -1.0674216],\n",
        "                            [-1.0999551, -1.1014045, -1.0944905],\n",
        "                            [-1.0953273, -1.0959654, -1.1045709],\n",
        "                            [-1.1084402, -1.0971687, -1.0903118],\n",
        "                            [-1.099349 , -1.1130908, -1.0836148],\n",
        "                            [-1.1031718, -1.0897288, -1.1029954],\n",
        "                            [-1.0929244, -1.1077557, -1.0952206],\n",
        "                            [-1.0995092, -1.0998485, -1.0964826],\n",
        "                            [-1.1419646, -1.1081928, -1.0479565],\n",
        "                            [-1.1052557, -1.0851235, -1.1055952],\n",
        "                            [-1.0840428, -1.1084775, -1.1034834],\n",
        "                            [-1.0872697, -1.1025085, -1.1061592],\n",
        "                            [-1.1060572, -1.0939908, -1.095831 ]])\n",
        "print(f\"Model Output: {bert_out}\")\n",
        "print(f\"Expected Output: {expected_bert_out}\")\n",
        "\n",
        "assert bert_out.shape == expected_bert_out.shape\n",
        "assert np.allclose(bert_out, expected_bert_out, 1e-4)\n",
        "print(\"Test Case Passed! :)\")\n",
        "print(\"******************************\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f38f523b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "f38f523b",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "b96506abcb89deafbdc245e7e22a0509",
          "grade": false,
          "grade_id": "cell-d26d1e16847283ff",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### Task 2.2: Training and Evaluating the Model\n",
        "\n",
        "Now that we have implemented the custom Dataset and a BERT based classifier model, we can start training and evaluating the model. This time we will modify the training loop slightly. At the end of each training epoch we will now evaluate on the validation data and check the accuracy. Based on this we will select the best model across the epochs that obtains highest validation accuracy. You will need to implement the `train` and `evaluate` functions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "af1e43b1",
      "metadata": {
        "deletable": false,
        "id": "af1e43b1",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fce28e32786d4f34b17a1826f752e3a2",
          "grade": true,
          "grade_id": "cell-37de7065d6cb7392",
          "locked": false,
          "points": 3,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_dataloader, device = \"cpu\"):\n",
        "    \"\"\"\n",
        "    Evaluates `model` on test dataset\n",
        "\n",
        "    Inputs:\n",
        "        - model (BertMultiChoiceClassifierModel): BERT based multiple choice classifier model to be evaluated\n",
        "        - test_dataloader (torch.utils.DataLoader): A dataloader defined over the test dataset\n",
        "\n",
        "    Returns:\n",
        "        - accuracy (float): Average accuracy over the test dataset\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    accuracy = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for test_batch in test_dataloader:\n",
        "\n",
        "            # Read the batch from dataloader\n",
        "            input_ids_dict, attn_mask_dict, labels = test_batch\n",
        "\n",
        "            # Send all values of dicts to device\n",
        "            for key in input_ids_dict.keys():\n",
        "                input_ids_dict[key] = input_ids_dict[key].to(device)\n",
        "                attn_mask_dict[key] = attn_mask_dict[key].to(device)\n",
        "            labels = labels.float()\n",
        "\n",
        "            # Step 1: Compute model's prediction on the test batch (Note here you need to get the final prediction from the model's output)\n",
        "            preds = model(input_ids_dict, attn_mask_dict).detach().numpy()\n",
        "\n",
        "            # Step 2: then compute accuracy and store it in batch_accuracy\n",
        "            batch_accuracy = (preds == labels).sum().item() / len(labels)\n",
        "\n",
        "            accuracy += batch_accuracy\n",
        "\n",
        "    accuracy = accuracy / len(test_dataloader)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader,\n",
        "          lr = 1e-5, num_epochs = 3,\n",
        "          device = \"cpu\"):\n",
        "    \"\"\"\n",
        "    Runs the training loop. Define the loss function as BCELoss like the last tine\n",
        "    and optimizer as Adam and traine for `num_epochs` epochs.\n",
        "\n",
        "    Inputs:\n",
        "        - model (BertMultiChoiceClassifierModel): BERT based classifer model to be trained\n",
        "        - train_dataloader (torch.utils.DataLoader): A dataloader defined over the training dataset\n",
        "        - val_dataloader (torch.utils.DataLoader): A dataloader defined over the validation dataset\n",
        "        - lr (float): The learning rate for the optimizer\n",
        "        - num_epochs (int): Number of epochs to train the model for.\n",
        "        - device (str): Device to train the model on. Can be either 'cuda' (for using gpu) or 'cpu'\n",
        "\n",
        "    Returns:\n",
        "        - best_model (BertMultiChoiceClassifierModel): model corresponding to the highest validation accuracy (checked at the end of each epoch)\n",
        "        - best_val_accuracy (float): Validation accuracy corresponding to the best epoch\n",
        "    \"\"\"\n",
        "    epoch_loss = 0\n",
        "    model = model.to(device)\n",
        "\n",
        "    best_val_accuracy = float(\"-inf\")\n",
        "    best_model = None\n",
        "\n",
        "    # 1. Define Loss function and optimizer\n",
        "    loss_fn = nn.NLLLoss()\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Iterate over `num_epochs`\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0 # We can use this to keep track of how the loss value changes as we train the model.\n",
        "        # Iterate over each batch using the `train_dataloader`\n",
        "        for train_batch in tqdm(train_dataloader):\n",
        "\n",
        "            # Zero out any gradients stored in the previous steps\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Read the batch from dataloader\n",
        "            input_ids_dict, attn_mask_dict, labels = train_batch\n",
        "\n",
        "            # Send all values of dicts to device\n",
        "            for key in input_ids_dict.keys():\n",
        "                input_ids_dict[key] = input_ids_dict[key].to(device)\n",
        "                attn_mask_dict[key] = attn_mask_dict[key].to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Step 3: Feed the input features to the model to get outputs log-probabilities\n",
        "            model_outs = model(input_ids_dict, attn_mask_dict).to(device)\n",
        "\n",
        "            # Step 4: Compute the loss and perform backward pass\n",
        "            loss = loss_fn(model_outs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Step 5: Take optimizer step\n",
        "            optimizer.step()\n",
        "\n",
        "            # Store loss value for tracking\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss = epoch_loss / len(train_dataloader)\n",
        "        # Step 6. Evaluate on validation data by calling `evaluate` and store the validation accuracy in `val_accurracy`\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for val_batch in val_dataloader:\n",
        "            input_ids_dict, attn_mask_dict, labels = val_batch\n",
        "            pred = torch.argmax(model(input_ids_dict, attn_mask_dict), axis=1)\n",
        "            correct += (pred==labels).sum().item()\n",
        "            total += len(labels)\n",
        "        val_accuracy = total / correct\n",
        "\n",
        "        # Model selection\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            best_model = copy.deepcopy(model) # Create a copy of model\n",
        "\n",
        "        print(f\"Epoch {epoch} completed | Average Training Loss: {epoch_loss} | Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "    return best_model, best_val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "b408695c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b408695c",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6e8f472b79d52b3b88d3ea6a7b0b6e74",
          "grade": true,
          "grade_id": "cell-f17fa8b3f55ab382",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on 100 data points for sanity check\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "132de9cd96b44f96b41fd536d11cf202",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 completed | Average Training Loss: 1.099404821395874 | Validation Accuracy: 1.2820512820512822\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1874d1d58734be68d51c96c9a9b407f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 completed | Average Training Loss: 1.004215521812439 | Validation Accuracy: 1.1235955056179776\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2822f47f65b04c1291f157667a79909b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 completed | Average Training Loss: 0.5962693852186203 | Validation Accuracy: 1.1235955056179776\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8996b88b8d5460692716af6b16a1e93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 completed | Average Training Loss: 0.5150470608472824 | Validation Accuracy: 1.0204081632653061\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19f68682c30145a7ad3d1d9a72eea6c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 completed | Average Training Loss: 0.2365075484663248 | Validation Accuracy: 1.0\n",
            "Best Validation Accuracy: 1.2820512820512822\n",
            "Expected Best Validation Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "print(\"Training on 100 data points for sanity check\")\n",
        "sample_data = train_data[:100]\n",
        "sample_labels = train_labels[:100]\n",
        "sample_dataset = SIQABertDataset(sample_data, sample_labels)\n",
        "sample_dataloader = DataLoader(sample_dataset, batch_size=4, collate_fn=partial(collate_fn, sample_dataset.tokenizer))\n",
        "\n",
        "model = BertMultiChoiceClassifierModel()\n",
        "best_model, best_val_acc = train(model, sample_dataloader, sample_dataloader, num_epochs = 5)\n",
        "print(f\"Best Validation Accuracy: {best_val_acc}\")\n",
        "print(f\"Expected Best Validation Accuracy: {1.0}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e958b3c7",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "e958b3c7",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "dbd0f3b1ded747e9f489e15a0fd7dd8e",
          "grade": false,
          "grade_id": "cell-e98f8c2ba73c862e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        " You can expect the validation accuracy of 1.0 by the end of training. This is so high because we trained on just 100 examples and just use those for validation for a sanity check. This is often done to debug the model and training loop. Let's now train on the entire dataset. This can take some time approximately 50 minutes per epoch, since we are fine-tuning all the 12 layers of BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "298b6cf0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "298b6cf0",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "19c69cac6d134da18698445835172a21",
          "grade": false,
          "grade_id": "cell-33834a90b1557f37",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4df5a83ec9c646959e083b7b28a8a5a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2089 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 completed | Average Training Loss: 0.7001695415230674 | Validation Accuracy: 1.6378876781223806\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c25a3d5434e945a79783667a73c43c2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2089 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 completed | Average Training Loss: 0.4223009190861405 | Validation Accuracy: 1.6447811447811447\n"
          ]
        }
      ],
      "source": [
        "model = BertMultiChoiceClassifierModel()\n",
        "best_model, best_val_acc = train(model, train_loader, dev_loader, num_epochs = 2, device = \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a397015f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "a397015f",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "48068cd407a033e3d3a83d6f5ffc416f",
          "grade": false,
          "grade_id": "cell-eb742cd2053baea4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "You should expect about ~61% validation accuracy (random classifier will have an accuracy of 33%), which is around what's reported in the SocialIQA paper. Note that this is a much more complex task than the news classification that we had in the last lab. You can further improve the performance by using bigger models like bert-base-large or roberta-large."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95b18060",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "95b18060",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "fb7eeb52eab4c3dee032a34d8cab16a8",
          "grade": false,
          "grade_id": "cell-92df0b4e94279d7b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Now that we have a model ready for the task, we can save it on disk, so we can use it later (This will come handy for Assignment2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "c3168433",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "c3168433",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3840681b544c16526676ad44bfbbf4d7",
          "grade": false,
          "grade_id": "cell-58bf127a74f43114",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Save the best model\n",
        "save_dir = \"models/siqa_bert-base-uncased/\"\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "torch.save(best_model.state_dict(), f\"{save_dir}/model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82f7ea51",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "82f7ea51",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3a3028590f28ef4abb1bba217f555232",
          "grade": false,
          "grade_id": "cell-28db2cd28da09990",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### Task 2.3: Making Predictions from scratch\n",
        "\n",
        "Similar to assignment 1, implement the function `predict_siqa` that takes as input the context, question and answers and runs them through the BERT classifier model to obtain the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "639651fe",
      "metadata": {
        "deletable": false,
        "id": "639651fe",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f7d2ad4a45affa789ed2eab66e144df8",
          "grade": false,
          "grade_id": "cell-8a815364723b18a7",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def predict_text(siqa_instance, model, tokenizer,device = \"cpu\"):\n",
        "    \"\"\"\n",
        "    Predicts the correct answer for a piece of a Social IQA instance using the BERT classifier model\n",
        "\n",
        "    Inputs:\n",
        "        - siqa_instance (dict(str, str)): An SIQA instance containing the context, question and the three answer choices.\n",
        "        - model (BertMultiChoiceClassifierModel): Fine-tuned BERT based classifer model\n",
        "        - tokenizer (BertTokenizer): Pre-trained BERT tokenizer\n",
        "    Returns:\n",
        "        - pred_label (float): Predicted answer for `siqa_instance`\n",
        "    \"\"\"\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    pred_label = None\n",
        "\n",
        "    input_ids_dict = None\n",
        "    attn_mask_dict = None\n",
        "    # Step 1: Tokenize the [sentence, question, answer] triplet using the tokenizer and create input_ids_dict and attn_mask_dict, as done in the Dataset class\n",
        "    # (Don't forget to convert the lists to tensors, torch.Tensor() can come handy or just use return_tensors = \"pt\" while calling the tokenizer)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "\n",
        "    # Send the input_ids_dict and attn_mask_dict to device\n",
        "    for key in input_ids_dict.keys():\n",
        "        input_ids_dict[key] = input_ids_dict[key].to(device)\n",
        "        attn_mask_dict[key] = attn_mask_dict[key].to(device)\n",
        "\n",
        "    # Step 2: Feed the input_ids_dict and attn_mask_dict to the model and get the final predictions\n",
        "    # (Don't forget torch.no_grad())\n",
        "    pred_label = None\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "\n",
        "    # Step 3: Make the predicted human readable i.e. convert 0 to A, 1 to B and 2 to C\n",
        "    pred_label_hr = None\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "    return pred_label_hr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b79990de",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b79990de",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4e2ea8703c05b6e64a2fd3574ac83692",
          "grade": false,
          "grade_id": "cell-2a404905599a658d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "print(\"Running Sample Test Case. If the implementation is correct, we should get the same accuracy as best_val_acc above, by predicting on each example of the dev data\")\n",
        "preds = [\n",
        "    predict_text(siqa_instance, best_model, bert_tokenizer, device = \"cuda\")\n",
        "    for siqa_instance in tqdm(dev_data)\n",
        "]\n",
        "test_case_accuracy = (np.array(preds) == np.array(dev_labels)).mean()\n",
        "print(f\"Accuracy by calling `predict_text`: {test_case_accuracy}\")\n",
        "print(f\"Expected Accuracy: {best_val_acc}\")\n",
        "\n",
        "assert np.allclose(test_case_accuracy, best_val_acc, 1e-2)\n",
        "print(\"Test Case Passed! :)\")\n",
        "print(\"******************************\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c969000",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7c969000",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5236adbcc6dc50b20fd6f56c133a3ee0",
          "grade": true,
          "grade_id": "cell-00c5243a8d59f404",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "idx = 0\n",
        "sample_data= dev_data[idx]\n",
        "predicted_label = predict_text(sample_data, best_model, bert_tokenizer)\n",
        "expected_label = \"C\"\n",
        "pprint(sample_data, sort_dicts=False, indent = 4)\n",
        "print(f\"Predicted Label: {predicted_label}\")\n",
        "print(f\"Gold Label: {dev_labels[idx]}\")\n",
        "print(\"**********************************\\n\")\n",
        "\n",
        "idx = 100\n",
        "sample_data= dev_data[idx]\n",
        "predicted_label = predict_text(sample_data, best_model, bert_tokenizer)\n",
        "expected_label = \"C\"\n",
        "pprint(sample_data, sort_dicts=False, indent = 4)\n",
        "print(f\"Predicted Label: {predicted_label}\")\n",
        "print(f\"Gold Label: {dev_labels[idx]}\")\n",
        "print(\"**********************************\\n\")\n",
        "\n",
        "\n",
        "idx = 200\n",
        "sample_data= dev_data[idx]\n",
        "predicted_label = predict_text(sample_data, best_model, bert_tokenizer)\n",
        "expected_label = \"C\"\n",
        "pprint(sample_data, sort_dicts=False, indent = 4)\n",
        "print(f\"Predicted Label: {predicted_label}\")\n",
        "print(f\"Gold Label: {dev_labels[idx]}\")\n",
        "\n",
        "print(\"**********************************\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef516c5",
      "metadata": {
        "id": "5ef516c5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
